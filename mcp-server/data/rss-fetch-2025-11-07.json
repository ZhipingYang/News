{
  "ai_programming": [
    {
      "title": "GitHub Copilot CLI 101: How to use GitHub Copilot from the command line",
      "link": "https://github.blog/ai-and-ml/github-copilot-cli-101-how-to-use-github-copilot-from-the-command-line/",
      "description": "Curious about using GitHub Copilot in your terminal? Here's our guide to GitHub Copilot CLI, including a starter kit with the best prompts for a wide range of use cases.\nThe post GitHub Copilot CLI 101: How to use GitHub Copilot from the command line appeared first on The GitHub Blog.",
      "pubDate": "Thu, 06 Nov 2025 20:30:00 +0000",
      "source": "GitHub Blog",
      "sourceUrl": "https://github.blog/feed/",
      "credibility": 0.95,
      "category": "company_official"
    },
    {
      "title": "TypeScriptâ€™s rise in the AI era: Insights from Lead Architect, Anders Hejlsberg",
      "link": "https://github.blog/developer-skills/programming-languages-and-frameworks/typescripts-rise-in-the-ai-era-insights-from-lead-architect-anders-hejlsberg/",
      "description": "TypeScript just became the most-used language on GitHub. Hereâ€™s why, according to its creator.\nThe post TypeScriptâ€™s rise in the AI era: Insights from Lead Architect, Anders Hejlsberg appeared first on The GitHub Blog.",
      "pubDate": "Thu, 06 Nov 2025 17:00:00 +0000",
      "source": "GitHub Blog",
      "sourceUrl": "https://github.blog/feed/",
      "credibility": 0.95,
      "category": "company_official"
    },
    {
      "title": "GitHub Copilot tutorial: How to build, test, review, and ship code faster (with real prompts)",
      "link": "https://github.blog/ai-and-ml/github-copilot/a-developers-guide-to-writing-debugging-reviewing-and-shipping-code-faster-with-github-copilot/",
      "description": "How GitHub Copilot works todayâ€”including mission controlâ€”and how to get the most out of it. Hereâ€™s what you need to know.\nThe post GitHub Copilot tutorial: How to build, test, review, and ship code faster (with real prompts) appeared first on The GitHub Blog.",
      "pubDate": "Wed, 05 Nov 2025 17:00:00 +0000",
      "source": "GitHub Blog",
      "sourceUrl": "https://github.blog/feed/",
      "credibility": 0.95,
      "category": "company_official"
    },
    {
      "title": "The AI ick",
      "link": "https://stackoverflow.blog/2025/11/05/the-ai-ick/",
      "description": "How we feel about AI-generated content, what AI detectors tell us, and why human creativity matters. Also, what is art?",
      "pubDate": "Wed, 05 Nov 2025 17:00:00 GMT",
      "source": "Stack Overflow Blog",
      "sourceUrl": "https://stackoverflow.blog/feed/",
      "credibility": 0.85,
      "category": "tech_news"
    },
    {
      "title": "The Rider 2025.3 Release Candidate Is Now Available",
      "link": "https://blog.jetbrains.com/dotnet/2025/11/05/the-rider-2025-3-release-candidate/",
      "description": "The next big release for Rider is just around the corner! If youâ€™re eager to get a sneak peek, you can download the Release Candidate version of Rider 2025.3 from our website right now.Â  Here are the feature highlights of the Rider 2025.3 RC build: If you encounter any issues when using the Rider 2025.3 [â€¦]",
      "pubDate": "Wed, 05 Nov 2025 16:30:53 +0000",
      "source": "JetBrains Blog",
      "sourceUrl": "https://blog.jetbrains.com/feed/",
      "credibility": 0.9,
      "category": "company_blog"
    },
    {
      "title": "The ReSharper, .NET Tools, and ReSharper C++ 2025.3 Release Candidates Are Now Available",
      "link": "https://blog.jetbrains.com/dotnet/2025/11/05/the-resharper-dotnet-tools-2025-3-release-candidate/",
      "description": "Get a preview of all the latest features and improvements set to be shipped with the next major ReSharper by downloading the Release Candidate builds that have just landed. The ReSharper 2025.3 Release Candidate For the full list of changes included in this build, please refer to the issue tracker.Â  dotTrace, dotMemory, dotCover and dotPeek [â€¦]",
      "pubDate": "Wed, 05 Nov 2025 16:30:45 +0000",
      "source": "JetBrains Blog",
      "sourceUrl": "https://blog.jetbrains.com/feed/",
      "credibility": 0.9,
      "category": "company_blog"
    },
    {
      "title": "dotInsights | November 2025",
      "link": "https://blog.jetbrains.com/dotnet/2025/11/05/dotinsights-november-2025/",
      "description": "Did you know? C# includes a feature called â€œexpression-bodied membersâ€œ, which allows you to define one-line methods, properties, constructors, and destructors in a very concise way. Welcome to dotInsights by JetBrains! This newsletter is the home for recent .NET and software development related information. ðŸ”— Links Hereâ€™s the latest from the developer community. ðŸ”¦ From [â€¦]",
      "pubDate": "Wed, 05 Nov 2025 13:54:51 +0000",
      "source": "JetBrains Blog",
      "sourceUrl": "https://blog.jetbrains.com/feed/",
      "credibility": 0.9,
      "category": "company_blog"
    },
    {
      "title": "Java Annotated Monthly â€“ November 2025",
      "link": "https://blog.jetbrains.com/idea/2025/11/java-annotated-monthly-november-2025/",
      "description": "This edition is packed with insightful, practical, and curiosity-fueling reads. From hands-on advice to thought-provoking pieces, weâ€™ve gathered a nice selection of stories that will help you stay sharp and inspired in the tech world. Weâ€™re also happy to have Josh Long as our featured content author this month! Expect some top-notch insights and a [â€¦]",
      "pubDate": "Wed, 05 Nov 2025 12:10:48 +0000",
      "source": "JetBrains Blog",
      "sourceUrl": "https://blog.jetbrains.com/feed/",
      "credibility": 0.9,
      "category": "company_blog"
    },
    {
      "title": "Growing Kotlin Adoption in Your Company",
      "link": "https://blog.jetbrains.com/kotlin/2025/11/growing-kotlin-adoption-in-your-company-2/",
      "description": "Guest post by Urs Peter, Senior Software Engineer and JetBrains-certified Kotlin Trainer. For readers whoâ€™d like a more structured way to build Kotlin skills, Urs also leads theÂ Kotlin Upskill Program at Xebia Academy. This is the third post in The Ultimate Guide to Successfully Adopting Kotlin in a Java-Dominated Environment, a series that follows how [â€¦]",
      "pubDate": "Wed, 05 Nov 2025 11:15:53 +0000",
      "source": "JetBrains Blog",
      "sourceUrl": "https://blog.jetbrains.com/feed/",
      "credibility": 0.9,
      "category": "company_blog"
    },
    {
      "title": "10 Smart Performance Hacks For Faster Python Code",
      "link": "https://blog.jetbrains.com/pycharm/2025/11/10-smart-performance-hacks-for-faster-python-code/",
      "description": "This is a guest post from Dido Grigorov, a deep learning engineer and Python programmer with 17 years of experience in the field. In the rapidly evolving domain of software development, Python has established itself as a premier language, renowned for its simplicity, readability, and versatility. It underpins a vast range of applications, from web [â€¦]",
      "pubDate": "Wed, 05 Nov 2025 10:50:20 +0000",
      "source": "JetBrains Blog",
      "sourceUrl": "https://blog.jetbrains.com/feed/",
      "credibility": 0.9,
      "category": "company_blog"
    }
  ],
  "generative_ai": [
    {
      "title": "Introducing the File Search Tool in Gemini API",
      "link": "https://blog.google/technology/developers/file-search-gemini-api/",
      "description": "File Search is a fully managed Retrieval Augmented Generation (RAG) system built directly into the Gemini API.",
      "pubDate": "Thu, 06 Nov 2025 18:00:00 +0000",
      "source": "Google AI Blog",
      "sourceUrl": "https://blog.google/technology/ai/rss/",
      "credibility": 0.95,
      "category": "company_official"
    },
    {
      "title": "Google Finance adds AI features for research, earnings and more",
      "link": "https://blog.google/products/search/new-google-finance-ai-deep-search/",
      "description": "Learn more about the new Google Finance, including new features like Deep Search and prediction markets data.",
      "pubDate": "Thu, 06 Nov 2025 17:00:00 +0000",
      "source": "Google AI Blog",
      "sourceUrl": "https://blog.google/technology/ai/rss/",
      "credibility": 0.95,
      "category": "company_official"
    },
    {
      "title": "AI and learning: A new chapter for students and educators",
      "link": "https://blog.google/outreach-initiatives/education/ai-and-learning/",
      "description": "How Google approaches AI and education, from our tools to our commitment to responsibility.",
      "pubDate": "Thu, 06 Nov 2025 17:00:00 +0000",
      "source": "Google AI Blog",
      "sourceUrl": "https://blog.google/technology/ai/rss/",
      "credibility": 0.95,
      "category": "company_official"
    },
    {
      "title": "Evaluating Control Protocols for Untrusted AI Agents",
      "link": "https://arxiv.org/abs/2511.02997",
      "description": "arXiv:2511.02997v1 Announce Type: new \nAbstract: As AI systems become more capable and widely deployed as agents, ensuring their safe operation becomes critical. AI control offers one approach to mitigating the risk from untrusted AI agents by monitoring their actions and intervening or auditing when necessary. Evaluating the safety of these protocols requires understanding both their effectiveness against current attacks and their robustness to adaptive adversaries. In this work, we systematically evaluate a range of control protocols in SHADE-Arena, a dataset of diverse agentic environments. First, we evaluate blue team protocols, including deferral to trusted models, resampling, and deferring on critical actions, against a default attack policy. We find that resampling for incrimination and deferring on critical actions perform best, increasing safety from 50% to 96%. We then iterate on red team strategies against these protocols and find that attack policies with additional affordances, such as knowledge of when resampling occurs or the ability to simulate monitors, can substantially improve attack success rates against our resampling strategy, decreasing safety to 17%. However, deferring on critical actions is highly robust to even our strongest red team strategies, demonstrating the importance of denying attack policies access to protocol internals.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "PublicAgent: Multi-Agent Design Principles From an LLM-Based Open Data Analysis Framework",
      "link": "https://arxiv.org/abs/2511.03023",
      "description": "arXiv:2511.03023v1 Announce Type: new \nAbstract: Open data repositories hold potential for evidence-based decision-making, yet are inaccessible to non-experts lacking expertise in dataset discovery, schema mapping, and statistical analysis. Large language models show promise for individual tasks, but end-to-end analytical workflows expose fundamental limitations: attention dilutes across growing contexts, specialized reasoning patterns interfere, and errors propagate undetected. We present PublicAgent, a multi-agent framework that addresses these limitations through decomposition into specialized agents for intent clarification, dataset discovery, analysis, and reporting. This architecture maintains focused attention within agent contexts and enables validation at each stage. Evaluation across five models and 50 queries derives five design principles for multi-agent LLM systems. First, specialization provides value independent of model strength--even the strongest model shows 97.5% agent win rates, with benefits orthogonal to model scale. Second, agents divide into universal (discovery, analysis) and conditional (report, intent) categories. Universal agents show consistent effectiveness (std dev 12.4%) while conditional agents vary by model (std dev 20.5%). Third, agents mitigate distinct failure modes--removing discovery or analysis causes catastrophic failures (243-280 instances), while removing report or intent causes quality degradation. Fourth, architectural benefits persist across task complexity with stable win rates (86-92% analysis, 84-94% discovery), indicating workflow management value rather than reasoning enhancement. Fifth, wide variance in agent effectiveness across models (42-96% for analysis) requires model-aware architecture design. These principles guide when and why specialization is necessary for complex analytical workflows while enabling broader access to public data through natural language interfaces.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "No-Human in the Loop: Agentic Evaluation at Scale for Recommendation",
      "link": "https://arxiv.org/abs/2511.03051",
      "description": "arXiv:2511.03051v1 Announce Type: new \nAbstract: Evaluating large language models (LLMs) as judges is increasingly critical for building scalable and trustworthy evaluation pipelines. We present ScalingEval, a large-scale benchmarking study that systematically compares 36 LLMs, including GPT, Gemini, Claude, and Llama, across multiple product categories using a consensus-driven evaluation protocol. Our multi-agent framework aggregates pattern audits and issue codes into ground-truth labels via scalable majority voting, enabling reproducible comparison of LLM evaluators without human annotation. Applied to large-scale complementary-item recommendation, the benchmark reports four key findings: (i) Anthropic Claude 3.5 Sonnet achieves the highest decision confidence; (ii) Gemini 1.5 Pro offers the best overall performance across categories; (iii) GPT-4o provides the most favorable latency-accuracy-cost tradeoff; and (iv) GPT-OSS 20B leads among open-source models. Category-level analysis shows strong consensus in structured domains (Electronics, Sports) but persistent disagreement in lifestyle categories (Clothing, Food). These results establish ScalingEval as a reproducible benchmark and evaluation protocol for LLMs as judges, with actionable guidance on scaling, reliability, and model family tradeoffs.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Epidemiology of Large Language Models: A Benchmark for Observational Distribution Knowledge",
      "link": "https://arxiv.org/abs/2511.03070",
      "description": "arXiv:2511.03070v1 Announce Type: new \nAbstract: Artificial intelligence (AI) systems hold great promise for advancing various scientific disciplines, and are increasingly used in real-world applications. Despite their remarkable progress, further capabilities are expected in order to achieve more general types of intelligence. A critical distinction in this context is between factual knowledge, which can be evaluated against true or false answers (e.g., \"what is the capital of England?\"), and probabilistic knowledge, reflecting probabilistic properties of the real world (e.g., \"what is the sex of a computer science graduate in the US?\"). In this paper, our goal is to build a benchmark for understanding the capabilities of LLMs in terms of knowledge of probability distributions describing the real world. Given that LLMs are trained on vast amounts of text, it may be plausible that they internalize aspects of these distributions. Indeed, LLMs are touted as powerful universal approximators of real-world distributions. At the same time, classical results in statistics, known as curse of dimensionality, highlight fundamental challenges in learning distributions in high dimensions, challenging the notion of universal distributional learning. In this work, we develop the first benchmark to directly test this hypothesis, evaluating whether LLMs have access to empirical distributions describing real-world populations across domains such as economics, health, education, and social behavior. Our results demonstrate that LLMs perform poorly overall, and do not seem to internalize real-world statistics naturally. When interpreted in the context of Pearl's Causal Hierarchy (PCH), our benchmark demonstrates that language models do not contain knowledge on observational distributions (Layer 1 of PCH), and thus the Causal Hierarchy Theorem implies that interventional (Layer 2) and counterfactual (Layer 3) knowledge of these models is also limited.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "SnapStream: Efficient Long Sequence Decoding on Dataflow Accelerators",
      "link": "https://arxiv.org/abs/2511.03092",
      "description": "arXiv:2511.03092v1 Announce Type: new \nAbstract: The proliferation of 100B+ parameter Large Language Models (LLMs) with 100k+ context length support have resulted in increasing demands for on-chip memory to support large KV caches. Techniques such as StreamingLLM and SnapKV demonstrate how to control KV cache size while maintaining model accuracy. Yet, these techniques are not commonly used within industrial deployments using frameworks like vLLM or SGLang. The reason is twofold: on one hand, the static graphs and continuous batching methodology employed by these frameworks make it difficult to admit modifications to the standard multi-head attention algorithm, while on the other hand, the accuracy implications of such techniques on modern instruction-following and reasoning models are not well understood, obfuscating the need for implementing these techniques. In this paper, we explore these accuracy implications on Llama-3.1-8B-Instruct and DeepSeek-R1, and develop SnapStream, a KV cache compression method that can be deployed at scale. We demonstrate the efficacy of SnapStream in a 16-way tensor-parallel deployment of DeepSeek-671B on SambaNova SN40L accelerators running at 128k context length and up to 1832 tokens per second in a real production setting. SnapStream enables $4\\times$ improved on-chip memory usage and introduces minimal accuracy degradation on LongBench-v2, AIME24 and LiveCodeBench. To the best of our knowledge, this is the first implementation of sparse KV attention techniques deployed in a production inference system with static graphs and continuous batching.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Large language models require a new form of oversight: capability-based monitoring",
      "link": "https://arxiv.org/abs/2511.03106",
      "description": "arXiv:2511.03106v1 Announce Type: new \nAbstract: The rapid adoption of large language models (LLMs) in healthcare has been accompanied by scrutiny of their oversight. Existing monitoring approaches, inherited from traditional machine learning (ML), are task-based and founded on assumed performance degradation arising from dataset drift. In contrast, with LLMs, inevitable model degradation due to changes in populations compared to the training dataset cannot be assumed, because LLMs were not trained for any specific task in any given population. We therefore propose a new organizing principle guiding generalist LLM monitoring that is scalable and grounded in how these models are developed and used in practice: capability-based monitoring. Capability-based monitoring is motivated by the fact that LLMs are generalist systems whose overlapping internal capabilities are reused across numerous downstream tasks. Instead of evaluating each downstream task independently, this approach organizes monitoring around shared model capabilities, such as summarization, reasoning, translation, or safety guardrails, in order to enable cross-task detection of systemic weaknesses, long-tail errors, and emergent behaviors that task-based monitoring may miss. We describe considerations for developers, organizational leaders, and professional societies for implementing a capability-based monitoring approach. Ultimately, capability-based monitoring will provide a scalable foundation for safe, adaptive, and collaborative monitoring of LLMs and future generalist artificial intelligence models in healthcare.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "miniF2F-Lean Revisited: Reviewing Limitations and Charting a Path Forward",
      "link": "https://arxiv.org/abs/2511.03108",
      "description": "arXiv:2511.03108v1 Announce Type: new \nAbstract: We perform a thorough analysis of the formal and informal statements in the miniF2F benchmark from the perspective of an AI system that is tasked to participate in a math Olympiad consisting of the problems in miniF2F. In such setting, the model has to read and comprehend the problems in natural language, formalize them in Lean language, then proceed with proving the problems, and it will get credit for each problem if the formal proof corresponds to the original informal statement presented to the model. Our evaluation results reveal that the best accuracy of such pipeline can be about 36% using the SoTA models in the literature, considerably lower than the individual SoTA accuracies, 97% and 69% reported in the autoformalization and theorem proving literature. Analyzing the failure modes, we trace back a considerable portion of this drop to discrepancies between the formal and informal statements for more than half of the problems in miniF2F. We proceed with correcting all the errors, discrepancies and simplifications in formal and informal statements, and present the miniF2F-v2 with fully verified formal and informal statements and proofs. Evaluating the full theorem proving pipeline on miniF2F-v2 leads to the best accuracy of 70%, a significant improvement from the 40% on the original miniF2F, yet indicating considerable misalignment between the autoformalization models and theorem provers. Our deep analysis suggests that a higher quality benchmark can help the community better evaluate progress in the field of formal reasoning and also better diagnose the failure and success modes of autoformalization and theorem proving models. Our dataset is available at https://github.com/roozbeh-yz/miniF2F_v2.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Using Multi-modal Large Language Model to Boost Fireworks Algorithm's Ability in Settling Challenging Optimization Tasks",
      "link": "https://arxiv.org/abs/2511.03137",
      "description": "arXiv:2511.03137v1 Announce Type: new \nAbstract: As optimization problems grow increasingly complex and diverse, advancements in optimization techniques and paradigm innovations hold significant importance. The challenges posed by optimization problems are primarily manifested in their non-convexity, high-dimensionality, black-box nature, and other unfavorable characteristics. Traditional zero-order or first-order methods, which are often characterized by low efficiency, inaccurate gradient information, and insufficient utilization of optimization information, are ill-equipped to address these challenges effectively. In recent years, the rapid development of large language models (LLM) has led to substantial improvements in their language understanding and code generation capabilities. Consequently, the design of optimization algorithms leveraging large language models has garnered increasing attention from researchers. In this study, we choose the fireworks algorithm(FWA) as the basic optimizer and propose a novel approach to assist the design of the FWA by incorporating multi-modal large language model(MLLM). To put it simply, we propose the concept of Critical Part(CP), which extends FWA to complex high-dimensional tasks, and further utilizes the information in the optimization process with the help of the multi-modal characteristics of large language models. We focus on two specific tasks: the \\textit{traveling salesman problem }(TSP) and \\textit{electronic design automation problem} (EDA). The experimental results show that FWAs generated under our new framework have achieved or surpassed SOTA results on many problem instances.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "A Proprietary Model-Based Safety Response Framework for AI Agents",
      "link": "https://arxiv.org/abs/2511.03138",
      "description": "arXiv:2511.03138v1 Announce Type: new \nAbstract: With the widespread application of Large Language Models (LLMs), their associated security issues have become increasingly prominent, severely constraining their trustworthy deployment in critical domains. This paper proposes a novel safety response framework designed to systematically safeguard LLMs at both the input and output levels. At the input level, the framework employs a supervised fine-tuning-based safety classification model. Through a fine-grained four-tier taxonomy (Safe, Unsafe, Conditionally Safe, Focused Attention), it performs precise risk identification and differentiated handling of user queries, significantly enhancing risk coverage and business scenario adaptability, and achieving a risk recall rate of 99.3%. At the output level, the framework integrates Retrieval-Augmented Generation (RAG) with a specifically fine-tuned interpretation model, ensuring all responses are grounded in a real-time, trustworthy knowledge base. This approach eliminates information fabrication and enables result traceability. Experimental results demonstrate that our proposed safety control model achieves a significantly higher safety score on public safety evaluation benchmarks compared to the baseline model, TinyR1-Safety-8B. Furthermore, on our proprietary high-risk test set, the framework's components attained a perfect 100% safety score, validating their exceptional protective capabilities in complex risk scenarios. This research provides an effective engineering pathway for building high-security, high-trust LLM applications.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Uncovering Bugs in Formal Explainers: A Case Study with PyXAI",
      "link": "https://arxiv.org/abs/2511.03169",
      "description": "arXiv:2511.03169v1 Announce Type: new \nAbstract: Formal explainable artificial intelligence (XAI) offers unique theoretical guarantees of rigor when compared to other non-formal methods of explainability. However, little attention has been given to the validation of practical implementations of formal explainers. This paper develops a novel methodology for validating formal explainers and reports on the assessment of the publicly available formal explainer PyXAI. The paper documents the existence of incorrect explanations computed by PyXAI on most of the datasets analyzed in the experiments, thereby confirming the importance of the proposed novel methodology for the validation of formal explainers.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Toward Autonomous Engineering Design: A Knowledge-Guided Multi-Agent Framework",
      "link": "https://arxiv.org/abs/2511.03179",
      "description": "arXiv:2511.03179v1 Announce Type: new \nAbstract: The engineering design process often demands expertise from multiple domains, leading to complex collaborations and iterative refinements. Traditional methods can be resource-intensive and prone to inefficiencies. To address this, we formalize the engineering design process through a multi-agent AI framework that integrates structured design and review loops. The framework introduces specialized knowledge-driven agents that collaborate to generate and refine design candidates. As an exemplar, we demonstrate its application to the aerodynamic optimization of 4-digit NACA airfoils. The framework consists of three key AI agents: a Graph Ontologist, a Design Engineer, and a Systems Engineer. The Graph Ontologist employs a Large Language Model (LLM) to construct two domain-specific knowledge graphs from airfoil design literature. The Systems Engineer, informed by a human manager, formulates technical requirements that guide design generation and evaluation. The Design Engineer leverages the design knowledge graph and computational tools to propose candidate airfoils meeting these requirements. The Systems Engineer reviews and provides feedback both qualitative and quantitative using its own knowledge graph, forming an iterative feedback loop until a design is validated by the manager. The final design is then optimized to maximize performance metrics such as the lift-to-drag ratio. Overall, this work demonstrates how collaborative AI agents equipped with structured knowledge representations can enhance efficiency, consistency, and quality in the engineering design process.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Adobe Summit Concierge Evaluation with Human in the Loop",
      "link": "https://arxiv.org/abs/2511.03186",
      "description": "arXiv:2511.03186v1 Announce Type: new \nAbstract: Generative AI assistants offer significant potential to enhance productivity, streamline information access, and improve user experience in enterprise contexts. In this work, we present Summit Concierge, a domain-specific AI assistant developed for Adobe Summit. The assistant handles a wide range of event-related queries and operates under real-world constraints such as data sparsity, quality assurance, and rapid deployment. To address these challenges, we adopt a human-in-the-loop development workflow that combines prompt engineering, retrieval grounding, and lightweight human validation. We describe the system architecture, development process, and real-world deployment outcomes. Our experience shows that agile, feedback-driven development enables scalable and reliable AI assistants, even in cold-start scenarios.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "From Five Dimensions to Many: Large Language Models as Precise and Interpretable Psychological Profilers",
      "link": "https://arxiv.org/abs/2511.03235",
      "description": "arXiv:2511.03235v1 Announce Type: new \nAbstract: Psychological constructs within individuals are widely believed to be interconnected. We investigated whether and how Large Language Models (LLMs) can model the correlational structure of human psychological traits from minimal quantitative inputs. We prompted various LLMs with Big Five Personality Scale responses from 816 human individuals to role-play their responses on nine other psychological scales. LLMs demonstrated remarkable accuracy in capturing human psychological structure, with the inter-scale correlation patterns from LLM-generated responses strongly aligning with those from human data $(R^2 > 0.89)$. This zero-shot performance substantially exceeded predictions based on semantic similarity and approached the accuracy of machine learning algorithms trained directly on the dataset. Analysis of reasoning traces revealed that LLMs use a systematic two-stage process: First, they transform raw Big Five responses into natural language personality summaries through information selection and compression, analogous to generating sufficient statistics. Second, they generate target scale responses based on reasoning from these summaries. For information selection, LLMs identify the same key personality factors as trained algorithms, though they fail to differentiate item importance within factors. The resulting compressed summaries are not merely redundant representations but capture synergistic information--adding them to original scores enhances prediction alignment, suggesting they encode emergent, second-order patterns of trait interplay. Our findings demonstrate that LLMs can precisely predict individual participants' psychological traits from minimal data through a process of abstraction and reasoning, offering both a powerful tool for psychological simulation and valuable insights into their emergent reasoning capabilities.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Towards Scalable Web Accessibility Audit with MLLMs as Copilots",
      "link": "https://arxiv.org/abs/2511.03471",
      "description": "arXiv:2511.03471v1 Announce Type: new \nAbstract: Ensuring web accessibility is crucial for advancing social welfare, justice, and equality in digital spaces, yet the vast majority of website user interfaces remain non-compliant, due in part to the resource-intensive and unscalable nature of current auditing practices. While WCAG-EM offers a structured methodology for site-wise conformance evaluation, it involves great human efforts and lacks practical support for execution at scale. In this work, we present an auditing framework, AAA, which operationalizes WCAG-EM through a human-AI partnership model. AAA is anchored by two key innovations: GRASP, a graph-based multimodal sampling method that ensures representative page coverage via learned embeddings of visual, textual, and relational cues; and MaC, a multimodal large language model-based copilot that supports auditors through cross-modal reasoning and intelligent assistance in high-effort tasks. Together, these components enable scalable, end-to-end web accessibility auditing, empowering human auditors with AI-enhanced assistance for real-world impact. We further contribute four novel datasets designed for benchmarking core stages of the audit pipeline. Extensive experiments demonstrate the effectiveness of our methods, providing insights that small-scale language models can serve as capable experts when fine-tuned.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Explaining Decisions in ML Models: a Parameterized Complexity Analysis (Part I)",
      "link": "https://arxiv.org/abs/2511.03545",
      "description": "arXiv:2511.03545v1 Announce Type: new \nAbstract: This paper presents a comprehensive theoretical investigation into the parameterized complexity of explanation problems in various machine learning (ML) models. Contrary to the prevalent black-box perception, our study focuses on models with transparent internal mechanisms. We address two principal types of explanation problems: abductive and contrastive, both in their local and global variants. Our analysis encompasses diverse ML models, including Decision Trees, Decision Sets, Decision Lists, Boolean Circuits, and ensembles thereof, each offering unique explanatory challenges. This research fills a significant gap in explainable AI (XAI) by providing a foundational understanding of the complexities of generating explanations for these models. This work provides insights vital for further research in the domain of XAI, contributing to the broader discourse on the necessity of transparency and accountability in AI systems.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Outbidding and Outbluffing Elite Humans: Mastering Liar's Poker via Self-Play and Reinforcement Learning",
      "link": "https://arxiv.org/abs/2511.03724",
      "description": "arXiv:2511.03724v1 Announce Type: new \nAbstract: AI researchers have long focused on poker-like games as a testbed for environments characterized by multi-player dynamics, imperfect information, and reasoning under uncertainty. While recent breakthroughs have matched elite human play at no-limit Texas hold'em, the multi-player dynamics are subdued: most hands converge quickly with only two players engaged through multiple rounds of bidding. In this paper, we present Solly, the first AI agent to achieve elite human play in reduced-format Liar's Poker, a game characterized by extensive multi-player engagement. We trained Solly using self-play with a model-free, actor-critic, deep reinforcement learning algorithm. Solly played at an elite human level as measured by win rate (won over 50% of hands) and equity (money won) in heads-up and multi-player Liar's Poker. Solly also outperformed large language models (LLMs), including those with reasoning abilities, on the same metrics. Solly developed novel bidding strategies, randomized play effectively, and was not easily exploitable by world-class human players.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "An extended reality-based framework for user risk training in urban built environment",
      "link": "https://arxiv.org/abs/2511.02837",
      "description": "arXiv:2511.02837v1 Announce Type: cross \nAbstract: In the context of increasing urban risks, particularly from climate change-induced flooding, this paper presents an extended Reality (XR)-based framework to improve user risk training within urban built environments. The framework is designed to improve risk awareness and preparedness among various stakeholders, including citizens, local authorities, and emergency responders. Using immersive XR technologies, the training experience simulates real-world emergency scenarios, contributing to active participation and a deeper understanding of potential hazards and especially for floods. The framework highlights the importance of stakeholder participation in its development, ensuring that training modules are customized to address the specific needs of different user groups. The iterative approach of the framework supports ongoing refinement through user feedback and performance data, thus improving the overall effectiveness of risk training initiatives. This work outlines the methodological phases involved in the framework's implementation, including i) user flow mapping, ii) scenario selection, and iii) performance evaluation, with a focus on the pilot application in Senigallia, Italy. The findings underscore the potential of XR technologies to transform urban risk training, promoting a culture of preparedness and resilience against urban hazards.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Evaluating Generative AI as an Educational Tool for Radiology Resident Report Drafting",
      "link": "https://arxiv.org/abs/2511.02839",
      "description": "arXiv:2511.02839v1 Announce Type: cross \nAbstract: Objective: Radiology residents require timely, personalized feedback to develop accurate image analysis and reporting skills. Increasing clinical workload often limits attendings' ability to provide guidance. This study evaluates a HIPAA-compliant GPT-4o system that delivers automated feedback on breast imaging reports drafted by residents in real clinical settings.\n  Methods: We analyzed 5,000 resident-attending report pairs from routine practice at a multi-site U.S. health system. GPT-4o was prompted with clinical instructions to identify common errors and provide feedback. A reader study using 100 report pairs was conducted. Four attending radiologists and four residents independently reviewed each pair, determined whether predefined error types were present, and rated GPT-4o's feedback as helpful or not. Agreement between GPT and readers was assessed using percent match. Inter-reader reliability was measured with Krippendorff's alpha. Educational value was measured as the proportion of cases rated helpful.\n  Results: Three common error types were identified: (1) omission or addition of key findings, (2) incorrect use or omission of technical descriptors, and (3) final assessment inconsistent with findings. GPT-4o showed strong agreement with attending consensus: 90.5%, 78.3%, and 90.4% across error types. Inter-reader reliability showed moderate variability ({\\alpha} = 0.767, 0.595, 0.567), and replacing a human reader with GPT-4o did not significantly affect agreement ({\\Delta} = -0.004 to 0.002). GPT's feedback was rated helpful in most cases: 89.8%, 83.0%, and 92.0%.\n  Discussion: ChatGPT-4o can reliably identify key educational errors. It may serve as a scalable tool to support radiology education.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Digital Transformation Chatbot (DTchatbot): Integrating Large Language Model-based Chatbot in Acquiring Digital Transformation Needs",
      "link": "https://arxiv.org/abs/2511.02842",
      "description": "arXiv:2511.02842v1 Announce Type: cross \nAbstract: Many organisations pursue digital transformation to enhance operational efficiency, reduce manual efforts, and optimise processes by automation and digital tools. To achieve this, a comprehensive understanding of their unique needs is required. However, traditional methods, such as expert interviews, while effective, face several challenges, including scheduling conflicts, resource constraints, inconsistency, etc. To tackle these issues, we investigate the use of a Large Language Model (LLM)-powered chatbot to acquire organisations' digital transformation needs. Specifically, the chatbot integrates workflow-based instruction with LLM's planning and reasoning capabilities, enabling it to function as a virtual expert and conduct interviews. We detail the chatbot's features and its implementation. Our preliminary evaluation indicates that the chatbot performs as designed, effectively following predefined workflows and supporting user interactions with areas for improvement. We conclude by discussing the implications of employing chatbots to elicit user information, emphasizing their potential and limitations.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "AI-Enhanced Wi-Fi Sensing Through Single Transceiver Pair",
      "link": "https://arxiv.org/abs/2511.02845",
      "description": "arXiv:2511.02845v1 Announce Type: cross \nAbstract: The advancement of next-generation Wi-Fi technology heavily relies on sensing capabilities, which play a pivotal role in enabling sophisticated applications. In response to the growing demand for large-scale deployments, contemporary Wi-Fi sensing systems strive to achieve high-precision perception while maintaining minimal bandwidth consumption and antenna count requirements. Remarkably, various AI-driven perception technologies have demonstrated the ability to surpass the traditional resolution limitations imposed by radar theory. However, the theoretical underpinnings of this phenomenon have not been thoroughly investigated in existing research. In this study, we found that under hardware-constrained conditions, the performance gains brought by AI to Wi-Fi sensing systems primarily originate from two aspects: prior information and temporal correlation. Prior information enables the AI to generate plausible details based on vague input, while temporal correlation helps reduce the upper bound of sensing error. We developed an AI-based Wi-Fi sensing system using a single transceiver pair and designed experiments focusing on human pose estimation and indoor localization to validate the theoretical claims. The results confirm the performance gains contributed by temporal correlation and prior information.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Spatio-Temporal Attention Network for Epileptic Seizure Prediction",
      "link": "https://arxiv.org/abs/2511.02846",
      "description": "arXiv:2511.02846v1 Announce Type: cross \nAbstract: In this study, we present a deep learning framework that learns complex spatio-temporal correlation structures of EEG signals through a Spatio-Temporal Attention Network (STAN) for accurate predictions of onset of seizures for Epilepsy patients. Unlike existing methods, which rely on feature engineering and/or assume fixed preictal durations, our approach simultaneously models spatio-temporal correlations through STAN and employs an adversarial discriminator to distinguish preictal from interictal attention patterns, enabling patient-specific learning. Evaluation on CHB-MIT and MSSM datasets demonstrates 96.6\\% sensitivity with 0.011/h false detection rate on CHB-MIT, and 94.2% sensitivity with 0.063/h FDR on MSSM, significantly outperforming state-of-the-art methods. The framework reliably detects preictal states at least 15 minutes before an onset, with patient-specific windows extending to 45 minutes, providing sufficient intervention time for clinical applications.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "EEGReXferNet: A Lightweight Gen-AI Framework for EEG Subspace Reconstruction via Cross-Subject Transfer Learning and Channel-Aware Embedding",
      "link": "https://arxiv.org/abs/2511.02848",
      "description": "arXiv:2511.02848v1 Announce Type: cross \nAbstract: Electroencephalography (EEG) is a widely used non-invasive technique for monitoring brain activity, but low signal-to-noise ratios (SNR) due to various artifacts often compromise its utility. Conventional artifact removal methods require manual intervention or risk suppressing critical neural features during filtering/reconstruction. Recent advances in generative models, including Variational Autoencoders (VAEs) and Generative Adversarial Networks (GANs), have shown promise for EEG reconstruction; however, these approaches often lack integrated temporal-spectral-spatial sensitivity and are computationally intensive, limiting their suitability for real-time applications like brain-computer interfaces (BCIs). To overcome these challenges, we introduce EEGReXferNet, a lightweight Gen-AI framework for EEG subspace reconstruction via cross-subject transfer learning - developed using Keras TensorFlow (v2.15.1). EEGReXferNet employs a modular architecture that leverages volume conduction across neighboring channels, band-specific convolution encoding, and dynamic latent feature extraction through sliding windows. By integrating reference-based scaling, the framework ensures continuity across successive windows and generalizes effectively across subjects. This design improves spatial-temporal-spectral resolution (mean PSD correlation >= 0.95; mean spectrogram RV-Coefficient >= 0.85), reduces total weights by ~45% to mitigate overfitting, and maintains computational efficiency for robust, real-time EEG preprocessing in neurophysiological and BCI applications.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Approaching Low-Cost Cardiac Intelligence with Semi-Supervised Knowledge Distillation",
      "link": "https://arxiv.org/abs/2511.02851",
      "description": "arXiv:2511.02851v1 Announce Type: cross \nAbstract: Deploying advanced cardiac artificial intelligence for daily cardiac monitoring is hindered by its reliance on extensive medical data and high computational resources. Low-cost cardiac intelligence (LCCI) offers a promising alternative by using wearable device data, such as 1-lead electrocardiogram (ECG), but it suffers from a significant diagnostic performance gap compared to high-cost cardiac intelligence (HCCI). To bridge this gap, we propose LiteHeart, a semi-supervised knowledge distillation framework. LiteHeart introduces a region-aware distillation module to mimic how cardiologists focus on diagnostically relevant ECG regions and a cross-layer mutual information module to align the decision processes of LCCI and HCCI systems. Using a semi-supervised training strategy, LiteHeart further improves model robustness under limited supervision. Evaluated on five datasets covering over 38 cardiovascular diseases, LiteHeart substantially reduces the performance gap between LCCI and HCCI, outperforming existing methods by 4.27% to 7.10% in macro F1 score. These results demonstrate that LiteHeart significantly enhances the diagnostic capabilities of low-cost cardiac intelligence systems, paving the way for scalable, affordable, and accurate daily cardiac healthcare using wearable technologies.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Consciousness-ECG Transformer for Conscious State Estimation System with Real-Time Monitoring",
      "link": "https://arxiv.org/abs/2511.02853",
      "description": "arXiv:2511.02853v1 Announce Type: cross \nAbstract: Conscious state estimation is important in various medical settings, including sleep staging and anesthesia management, to ensure patient safety and optimize health outcomes. Traditional methods predominantly utilize electroencephalography (EEG), which faces challenges such as high sensitivity to noise and the requirement for controlled environments. In this study, we propose the consciousness-ECG transformer that leverages electrocardiography (ECG) signals for non-invasive and reliable conscious state estimation. Our approach employs a transformer with decoupled query attention to effectively capture heart rate variability features that distinguish between conscious and unconscious states. We implemented the conscious state estimation system with real-time monitoring and validated our system on datasets involving sleep staging and anesthesia level monitoring during surgeries. Experimental results demonstrate that our model outperforms baseline models, achieving accuracies of 0.877 on sleep staging and 0.880 on anesthesia level monitoring. Moreover, our model achieves the highest area under curve values of 0.786 and 0.895 on sleep staging and anesthesia level monitoring, respectively. The proposed system offers a practical and robust alternative to EEG-based methods, particularly suited for dynamic clinical environments. Our results highlight the potential of ECG-based consciousness monitoring to enhance patient safety and advance our understanding of conscious states.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "SELF-REDRAFT: Eliciting Intrinsic Exploration-Exploitation Balance in Test-Time Scaling for Code Generation",
      "link": "https://arxiv.org/abs/2511.02854",
      "description": "arXiv:2511.02854v1 Announce Type: cross \nAbstract: Test-time scaling without interpreter feedback is essential for real-world code generation scenarios where test cases are not readily available. While existing paradigms often rely on either greedy exploitation (i.e., iterative refinement) or stochastic exploration (i.e., relying on sample-based voting or reranking mechanisms), the balance between these two dimensions remains underexplored. To investigate the LLM's intrinsic ability to balance exploitation and exploration, we introduce SELF-REDRAFT, a framework built upon Self-Refine that encourages the model to propose new drafts for solutions that are fundamentally flawed. Our results show that SELF-REDRAFT consistently achieves better performance than Self-Refine when converged under the same maximum number of iterations. Still, we observe that significant room for improvement remains, largely due to two core aspects of current self-redraft capabilities: constrained capacity for generating instructive feedback and fragile discriminative judgment. We also find that balancing strategies vary notably across different LLMs, reflecting distinct, model-specific behaviors. Overall, our study establishes a baseline for intrinsic exploration-exploitation balancing in test-time scaling and identifies feedback and discrimination as key areas with potential for future advances.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Digitizing Spermatogenesis Lineage at Nanoscale Resolution In Tissue-Level Electron Microscopy",
      "link": "https://arxiv.org/abs/2511.02860",
      "description": "arXiv:2511.02860v1 Announce Type: cross \nAbstract: Recent advances in 2D large-scale and 3D volume electron microscopy have stimulated the rapid development of nanoscale functional analysis at the tissue and organ levels. Digitizing the cell by mapping the intricate organellar networks into its physiological and pathological textures will revolutionarize the contents of cell atlases. To meet the requirements of characterizing intracellular organelles and their interactions within defined cellular cohorts at tissue level, we have developed DeepOrganelle. It adopts a lightweighted Mask2Former frameworks as a universal segmentor and is capable of segmenting and extracting organelles within different cell types, performing statistical quantitative analysis, as well as visualizing and quantifying the spatial distribution of organelle morphologies and interactions across different cell types at tissue scales. Using DeepOrganelle, we systemically perform cross-scale quantification of membrane contact sites(MCSs) dynamics across the progression of the seminiferous epithelial cycle, covering 12 distinct developmental stages and 24 statuses of germ cells. DeepOrganelle uncovers the spatiotemporal gradient of the germ cell differentiation atlas according to different types of organelles and their interactions. Noticeably, it discovers a waved pattern of mitochondria(Mito)-endoplasmic reticulum(ER) contact with a significant increase specifically at Stage X pachytene preceding the transition to diplotene, which aligns well with a newly reported experiment that mitochondrial metabolic proteins like PDHA2 are essential for this transition by maintaining ATP supply for double-strand break(DSB) repair. DeepOrganelle also observes a dynamic restructuring of the blood-testis barrier and stage-specific reorganization of organelle topography in Sertoli cells from preleptotene to leptotene phases of prophase I.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Mathematical exploration and discovery at scale",
      "link": "https://arxiv.org/abs/2511.02864",
      "description": "arXiv:2511.02864v1 Announce Type: cross \nAbstract: AlphaEvolve is a generic evolutionary coding agent that combines the generative capabilities of LLMs with automated evaluation in an iterative evolutionary framework that proposes, tests, and refines algorithmic solutions to challenging scientific and practical problems. In this paper we showcase AlphaEvolve as a tool for autonomously discovering novel mathematical constructions and advancing our understanding of long-standing open problems.\n  To demonstrate its breadth, we considered a list of 67 problems spanning mathematical analysis, combinatorics, geometry, and number theory. The system rediscovered the best known solutions in most of the cases and discovered improved solutions in several. In some instances, AlphaEvolve is also able to generalize results for a finite number of input values into a formula valid for all input values. Furthermore, we are able to combine this methodology with Deep Think and AlphaProof in a broader framework where the additional proof-assistants and reasoning systems provide automated proof generation and further mathematical insights.\n  These results demonstrate that large language model-guided evolutionary search can autonomously discover mathematical constructions that complement human intuition, at times matching or even improving the best known results, highlighting the potential for significant new ways of interaction between mathematicians and AI systems. We present AlphaEvolve as a powerful new tool for mathematical discovery, capable of exploring vast search spaces to solve complex optimization problems at scale, often with significantly reduced requirements on preparation and computation time.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "LM-Fix: Lightweight Bit-Flip Detection and Rapid Recovery Framework for Language Models",
      "link": "https://arxiv.org/abs/2511.02866",
      "description": "arXiv:2511.02866v1 Announce Type: cross \nAbstract: This paper presents LM-Fix, a lightweight detection and rapid recovery framework for faults in large language models (LLMs). Existing integrity approaches are often heavy or slow for modern LLMs. LM-Fix runs a short test-vector pass and uses hash-guided checks to detect bit-flip faults, then repairs them locally without a full reload. Across multiple models, it detects over 94% of single-bit flips at TVL=200 and nearly 100% of multi-bit flips with approximately 1% to 7.7% runtime overhead; recovery is more than 100x faster than reloading. These results show a practical, low-overhead solution to keep LLMs reliable in production",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Proof-of-Spiking-Neurons(PoSN): Neuromorphic Consensus for Next-Generation Blockchains",
      "link": "https://arxiv.org/abs/2511.02868",
      "description": "arXiv:2511.02868v1 Announce Type: cross \nAbstract: Blockchain systems face persistent challenges of scalability, latency, and energy inefficiency. Existing consensus protocols such as Proof-of-Work (PoW) and Proof-of-Stake (PoS) either consume excessive resources or risk centralization. This paper proposes \\textit{Proof-of-Spiking-Neurons (PoSN)}, a neuromorphic consensus protocol inspired by spiking neural networks. PoSN encodes transactions as spike trains, elects leaders through competitive firing dynamics, and finalizes blocks via neural synchronization, enabling parallel and event-driven consensus with minimal energy overhead. A hybrid system architecture is implemented on neuromorphic platforms, supported by simulation frameworks such as Nengo and PyNN. Experimental results show significant gains in energy efficiency, throughput, and convergence compared to PoB and PoR. PoSN establishes a foundation for sustainable, adaptive blockchains suitable for IoT, edge, and large-scale distributed systems.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Analysis of AdvFusion: Adapter-based Multilingual Learning for Code Large Language Models",
      "link": "https://arxiv.org/abs/2511.02869",
      "description": "arXiv:2511.02869v1 Announce Type: cross \nAbstract: Programming languages can benefit from one another by utilizing a language model for software engineering tasks. Full fine-tuning and Parameter Efficient Fine-Tuning (PEFT) of Code Language Models (Code-LMs) has been explored for multilingual knowledge transfer. AdapterFusion is a PEFT architecture that aims to enhance task performance by leveraging information from multiple programming languages, but primarily focuses on the target programming language.\n  In our previous work, we proposed AdvFusion, a novel PEFT-based approach that effectively learns from other programming languages before adapting to the target task. Though previous experiments showed that AdvFusion outperformed AdapterFusion and LoRA, it was applied on pre-trained Code-LMs and was limited to only two tasks, code summarization and method name prediction. In this study, we expanded our work and investigated AdvFusion on Code Large Language Models (Code-LLMs), considering three new tasks: code generation, code translation, and commit message generation. We observed that different Code-LLMs/tasks exhibit different characteristics. In code generation, AdvFusion outperformed AdapterFusion but not other PEFT methods (LoRA, Compacter, and TaskAdapter). In commit message generation, AdapterFusion performed better than AdvFusion, and contrary to code generation, we found that the other PEFT methods do not have better performance. In code translation, AdvFusion performed worse than AdapterFusion overall, with the performance gap marginally widening as the model size increases. However, consistent with code generation, other PEFT methods showed better performance.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "FATE: A Formal Benchmark Series for Frontier Algebra of Multiple Difficulty Levels",
      "link": "https://arxiv.org/abs/2511.02872",
      "description": "arXiv:2511.02872v1 Announce Type: cross \nAbstract: Recent advances in large language models (LLMs) have demonstrated impressive capabilities in formal theorem proving, particularly on contest-based mathematical benchmarks like the IMO. However, these contests do not reflect the depth, breadth, and abstraction of modern mathematical research. To bridge this gap, we introduce FATE (Formal Algebra Theorem Evaluation), a new benchmark series in formal algebra designed to chart a course toward advanced mathematical reasoning. We present two new components, FATE-H and FATE-X, each with 100 problems in abstract and commutative algebra. The FATE series spans a difficulty spectrum from undergraduate exercises to problems exceeding PhD qualifying exams. Notably, FATE-X is the first formal benchmark to surpass both PhD-level exam difficulty and the coverage of the Mathlib library. Our evaluations of state-of-the-art LLM provers on this new benchmark reveal a stark performance gap compared to contest math: the best model achieves only 3% (pass@64) accuracy on FATE-H and 0% on FATE-X. Our two-stage evaluation reveals that models' natural-language reasoning is notably more accurate than their ability to formalize this reasoning. We systematically classify the common errors that arise during this formalization process. Furthermore, a comparative study shows that a specialized prover can exhibit less effective reflection than general-purpose models, reducing its accuracy at the natural-language stage. We believe FATE provides a robust and challenging benchmark that establishes essential checkpoints on the path toward research-level formal mathematical reasoning.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Academics and Generative AI: Empirical and Epistemic Indicators of Policy-Practice Voids",
      "link": "https://arxiv.org/abs/2511.02875",
      "description": "arXiv:2511.02875v1 Announce Type: cross \nAbstract: As generative AI diffuses through academia, policy-practice divergence becomes consequential, creating demand for auditable indicators of alignment. This study prototypes a ten-item, indirect-elicitation instrument embedded in a structured interpretive framework to surface voids between institutional rules and practitioner AI use. The framework extracts empirical and epistemic signals from academics, yielding three filtered indicators of such voids: (1) AI-integrated assessment capacity (proxy) - within a three-signal screen (AI skill, perceived teaching benefit, detection confidence), the share who would fully allow AI in exams; (2) sector-level necessity (proxy) - among high output control users who still credit AI with high contribution, the proportion who judge AI capable of challenging established disciplines; and (3) ontological stance - among respondents who judge AI different in kind from prior tools, report practice change, and pass a metacognition gate, the split between material and immaterial views as an ontological map aligning procurement claims with evidence classes.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "A Novel Reservoir Computing Framework for Chaotic Time Series Prediction Using Time Delay Embedding and Random Fourier Features",
      "link": "https://arxiv.org/abs/2511.02877",
      "description": "arXiv:2511.02877v1 Announce Type: cross \nAbstract: Forecasting chaotic time series requires models that can capture the intrinsic geometry of the underlying attractor while remaining computationally efficient. We introduce a novel reservoir computing (RC) framework that integrates time-delay embedding with Random Fourier Feature (RFF) mappings to construct a dynamical reservoir without the need for traditional recurrent architectures. Unlike standard RC, which relies on high-dimensional recurrent connectivity, the proposed RFF-RC explicitly approximates nonlinear kernel transformations that uncover latent dynamical relations in the reconstructed phase space. This hybrid formulation offers two key advantages: (i) it provides a principled way to approximate complex nonlinear interactions among delayed coordinates, thereby enriching the effective dynamical representation of the reservoir, and (ii) it reduces reliance on manual reservoir hyperparameters such as spectral radius and leaking rate. We evaluate the framework on canonical chaotic systems-the Mackey-Glass equation, the Lorenz system, and the Kuramoto-Sivashinsky equation. This novel formulation demonstrates that RFF-RC not only achieves superior prediction accuracy but also yields robust attractor reconstructions and long-horizon forecasts. These results show that the combination of delay embedding and RFF-based reservoirs reveals new dynamical structure by embedding the system in an enriched feature space, providing a computationally efficient and interpretable approach to modeling chaotic dynamics.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Stochastic Deep Graph Clustering for Practical Group Formation",
      "link": "https://arxiv.org/abs/2511.02879",
      "description": "arXiv:2511.02879v1 Announce Type: cross \nAbstract: While prior work on group recommender systems (GRSs) has primarily focused on improving recommendation accuracy, most approaches assume static or predefined groups, making them unsuitable for dynamic, real-world scenarios. We reframe group formation as a core challenge in GRSs and propose DeepForm (Stochastic Deep Graph Clustering for Practical Group Formation), a framework designed to meet three key operational requirements: (1) the incorporation of high-order user information, (2) real-time group formation, and (3) dynamic adjustment of the number of groups. DeepForm employs a lightweight GCN architecture that effectively captures high-order structural signals. Stochastic cluster learning enables adaptive group reconfiguration without retraining, while contrastive learning refines groups under dynamic conditions. Experiments on multiple datasets demonstrate that DeepForm achieves superior group formation quality, efficiency, and recommendation accuracy compared with various baselines.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "NEF-NET+: Adapting Electrocardio panorama in the wild",
      "link": "https://arxiv.org/abs/2511.02880",
      "description": "arXiv:2511.02880v1 Announce Type: cross \nAbstract: Conventional multi-lead electrocardiogram (ECG) systems capture cardiac signals from a fixed set of anatomical viewpoints defined by lead placement. However, certain cardiac conditions (e.g., Brugada syndrome) require additional, non-standard viewpoints to reveal diagnostically critical patterns that may be absent in standard leads. To systematically overcome this limitation, Nef-Net was recently introduced to reconstruct a continuous electrocardiac field, enabling virtual observation of ECG signals from arbitrary views (termed Electrocardio Panorama). Despite its promise, Nef-Net operates under idealized assumptions and faces in-the-wild challenges, such as long-duration ECG modeling, robustness to device-specific signal artifacts, and suboptimal lead placement calibration. This paper presents NEF-NET+, an enhanced framework for realistic panoramic ECG synthesis that supports arbitrary-length signal synthesis from any desired view, generalizes across ECG devices, and com- pensates for operator-induced deviations in electrode placement. These capabilities are enabled by a newly designed model architecture that performs direct view transformation, incorporating a workflow comprising offline pretraining, device calibration tuning steps as well as an on-the-fly calibration step for patient-specific adaptation. To rigorously evaluate panoramic ECG synthesis, we construct a new Electrocardio Panorama benchmark, called Panobench, comprising 5367 recordings with 48-view per subject, capturing the full spatial variability of cardiac electrical activity. Experimental results show that NEF-NET+ delivers substantial improvements over Nef-Net, yielding an increase of around 6 dB in PSNR in real-world setting. The code and Panobench will be released in a subsequent publication.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "AgentSLA : Towards a Service Level Agreement for AI Agents",
      "link": "https://arxiv.org/abs/2511.02885",
      "description": "arXiv:2511.02885v1 Announce Type: cross \nAbstract: AI components are increasingly becoming a key element of all types of software systems to enhance their functionality. These AI components are often implemented as AI Agents, offering more autonomy than a plain integration of Large Language Models (LLMs), moving from a Model-as-a-Service paradigm to an Agent-as-a-Service one, bringing new challenges to the development of smart software systems. Indeed, while support for the design, implementation, and deployment of those agents exist, the specification of Quality of Service (QoS) and definition of Service Level Agreements (SLAs) aspects for those agents, important to ensure the quality of the resulting systems, remains an open challenge. Part of this is due to the difficulty to clearly define quality in the context of AI components, resulting in a lack of consensus on how to best approach Quality Assurance (QA) for these types of systems. To address this challenge, this paper proposes both a quality model for AI agents based on the ISO/IEC 25010 standard, and a domain specific language to support the definition of SLAs for the services provided by these AI agents.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Test-time Adaptation of Tiny Recursive Models",
      "link": "https://arxiv.org/abs/2511.02886",
      "description": "arXiv:2511.02886v1 Announce Type: cross \nAbstract: Prior to the close of the 2025 ARC Prize competition, the leading open source approach - known as TRM, or Tiny Recursive Models - involved training a 7M parameter recursive neural network on augmented variants of ARC tasks. That approach scored approximately 7.8% on the public ARC AGI II evaluation set, but required a level of compute far in excess of what is allowed during the competition. This paper shows that, by starting from a tiny recursive model that has been pre-trained on public ARC tasks, one can efficiently fine-tune on competition tasks within the allowed compute limits. Specifically, a model was pre-trained on 1,280 public tasks for 700k+ optimizer steps over 48 hours on 4xH100 SXM GPUs to obtain a ~10% score on the public evaluation set. That model was then post-trained in just 12,500 gradient steps during the competition to reach a score of 6.67% on semi-private evaluation tasks. Notably, such post-training performance is achieved by full-fine tuning of the tiny model, not LoRA fine-tuning or fine-tuning of task embeddings alone.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Predicting Weekly Fishing Concentration Zones through Deep Learning Integration of Heterogeneous Environmental Spatial Datasets",
      "link": "https://arxiv.org/abs/2511.02887",
      "description": "arXiv:2511.02887v1 Announce Type: cross \nAbstract: The North Indian Ocean, including the Arabian Sea and the Bay of Bengal, represents a vital source of livelihood for coastal communities, yet fishermen often face uncertainty in locating productive fishing grounds. To address this challenge, we present an AI-assisted framework for predicting Potential Fishing Zones (PFZs) using oceanographic parameters such as sea surface temperature and chlorophyll concentration. The approach is designed to enhance the accuracy of PFZ identification and provide region-specific insights for sustainable fishing practices. Preliminary results indicate that the framework can support fishermen by reducing search time, lowering fuel consumption, and promoting efficient resource utilization.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "NABench: Large-Scale Benchmarks of Nucleotide Foundation Models for Fitness Prediction",
      "link": "https://arxiv.org/abs/2511.02888",
      "description": "arXiv:2511.02888v1 Announce Type: cross \nAbstract: Nucleotide sequence variation can induce significant shifts in functional fitness. Recent nucleotide foundation models promise to predict such fitness effects directly from sequence, yet heterogeneous datasets and inconsistent preprocessing make it difficult to compare methods fairly across DNA and RNA families. Here we introduce NABench, a large-scale, systematic benchmark for nucleic acid fitness prediction. NABench aggregates 162 high-throughput assays and curates 2.6 million mutated sequences spanning diverse DNA and RNA families, with standardized splits and rich metadata. We show that NABench surpasses prior nucleotide fitness benchmarks in scale, diversity, and data quality. Under a unified evaluation suite, we rigorously assess 29 representative foundation models across zero-shot, few-shot prediction, transfer learning, and supervised settings. The results quantify performance heterogeneity across tasks and nucleic-acid types, demonstrating clear strengths and failure modes for different modeling choices and establishing strong, reproducible baselines. We release NABench to advance nucleic acid modeling, supporting downstream applications in RNA/DNA design, synthetic biology, and biochemistry. Our code is available at https://github.com/mrzzmrzz/NABench.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "A Criminology of Machines",
      "link": "https://arxiv.org/abs/2511.02895",
      "description": "arXiv:2511.02895v1 Announce Type: cross \nAbstract: While the possibility of reaching human-like Artificial Intelligence (AI) remains controversial, the likelihood that the future will be characterized by a society with a growing presence of autonomous machines is high. Autonomous AI agents are already deployed and active across several industries and digital environments and alongside human-human and human-machine interactions, machine-machine interactions are poised to become increasingly prevalent. Given these developments, I argue that criminology must begin to address the implications of this transition for crime and social control. Drawing on Actor-Network Theory and Woolgar's decades-old call for a sociology of machines -- frameworks that acquire renewed relevance with the rise of generative AI agents -- I contend that criminologists should move beyond conceiving AI solely as a tool. Instead, AI agents should be recognized as entities with agency encompassing computational, social, and legal dimensions. Building on the literature on AI safety, I thus examine the risks associated with the rise of multi-agent AI systems, proposing a dual taxonomy to characterize the channels through which interactions among AI agents may generate deviant, unlawful, or criminal outcomes. I then advance and discuss four key questions that warrant theoretical and empirical attention: (1) Can we assume that machines will simply mimic humans? (2) Will crime theories developed for humans suffice to explain deviant or criminal behaviors emerging from interactions between autonomous AI agents? (3) What types of criminal behaviors will be affected first? (4) How might this unprecedented societal shift impact policing? These questions underscore the urgent need for criminologists to theoretically and empirically engage with the implications of multi-agent AI systems for the study of crime and play a more active role in debates on AI safety and governance.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Performance Evaluation of Bitstring Representations in a Linear Genetic Programming Framework",
      "link": "https://arxiv.org/abs/2511.02897",
      "description": "arXiv:2511.02897v1 Announce Type: cross \nAbstract: Different bitstring representations can yield varying computational performance. This work compares three bitstring implementations in C++: std::bitset, boost::dynamic_bitset, and a custom direct implementation. Their performance is benchmarked in the context of concatenation within a Linear Genetic Programming system. Benchmarks were conducted on three platforms (macOS, Linux, and Windows MSYS2) to assess platform specific performance variations. The results show that the custom direct implementation delivers the fastest performance on Linux and Windows, while std::bitset performs best on macOS. Although consistently slower, boost::dynamic_bitset remains a viable and flexible option. These findings highlight the influence of compiler optimisations and system architecture on performance, providing practical guidance for selecting the optimal method based on platform and application requirements.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Generative Hints",
      "link": "https://arxiv.org/abs/2511.02933",
      "description": "arXiv:2511.02933v1 Announce Type: cross \nAbstract: Data augmentation is widely used in vision to introduce variation and mitigate overfitting, through enabling models to learn invariant properties, such as spatial invariance. However, these properties are not fully captured by data augmentation alone, since it attempts to learn the property on transformations of the training data only. We propose generative hints, a training methodology that directly enforces known invariances in the entire input space. Our approach leverages a generative model trained on the training set to approximate the input distribution and generate unlabeled images, which we refer to as virtual examples. These virtual examples are used to enforce functional properties known as hints. In generative hints, although the training dataset is fully labeled, the model is trained in a semi-supervised manner on both the classification and hint objectives, using the unlabeled virtual examples to guide the model in learning the desired hint. Across datasets, architectures, and loss functions, generative hints consistently outperform standard data augmentation when learning the same property. On popular fine-grained visual classification benchmarks, we achieved up to 1.78% top-1 accuracy improvement (0.63% on average) over fine-tuned models with data augmentation and an average performance boost of 1.286% on the CheXpert X-ray dataset.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Zero-shot data citation function classification using transformer-based large language models (LLMs)",
      "link": "https://arxiv.org/abs/2511.02936",
      "description": "arXiv:2511.02936v1 Announce Type: cross \nAbstract: Efforts have increased in recent years to identify associations between specific datasets and the scientific literature that incorporates them. Knowing that a given publication cites a given dataset, the next logical step is to explore how or why that data was used. Advances in recent years with pretrained, transformer-based large language models (LLMs) offer potential means for scaling the description of data use cases in the published literature. This avoids expensive manual labeling and the development of training datasets for classical machine-learning (ML) systems. In this work we apply an open-source LLM, Llama 3.1-405B, to generate structured data use case labels for publications known to incorporate specific genomic datasets. We also introduce a novel evaluation framework for determining the efficacy of our methods. Our results demonstrate that the stock model can achieve an F1 score of .674 on a zero-shot data citation classification task with no previously defined categories. While promising, our results are qualified by barriers related to data availability, prompt overfitting, computational infrastructure, and the expense required to conduct responsible performance evaluation.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "From Narrow to Wide: Autoencoding Transformers for Ultrasound Bandwidth Recovery",
      "link": "https://arxiv.org/abs/2511.02938",
      "description": "arXiv:2511.02938v1 Announce Type: cross \nAbstract: Conventional pulse-echo ultrasound suffers when low-cost probes deliver only narrow fractional bandwidths, elongating pulses and erasing high-frequency detail. We address this limitation by learning a data-driven mapping from band-limited to broadband spectrogram of radio-frequency (RF) lines. To this end, a variation of Tiny Vision Transform (ViT) auto-encoder is trained on simulation data using a curriculum-weighted loss. On heterogeneous speckle-cyst phantoms, the network reduces image-domain MSE by 90 percent, boosts PSNR by 6.7 dB, and raises SSIM to 0.965 compared with the narrow-band input. It also sharpens point-target rows in a completely unseen resolution phantom, demonstrating strong out-of-distribution generalisation without sacrificing frame rate or phase information. These results indicate that a purely software upgrade can endow installed narrow-band probes with broadband-like performance, potentially widening access to high-resolution ultrasound in resource-constrained settings.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Power Constrained Nonstationary Bandits with Habituation and Recovery Dynamics",
      "link": "https://arxiv.org/abs/2511.02944",
      "description": "arXiv:2511.02944v1 Announce Type: cross \nAbstract: A common challenge for decision makers is selecting actions whose rewards are unknown and evolve over time based on prior policies. For instance, repeated use may reduce an action's effectiveness (habituation), while inactivity may restore it (recovery). These nonstationarities are captured by the Reducing or Gaining Unknown Efficacy (ROGUE) bandit framework, which models real-world settings such as behavioral health interventions. While existing algorithms can compute sublinear regret policies to optimize these settings, they may not provide sufficient exploration due to overemphasis on exploitation, limiting the ability to estimate population-level effects. This is a challenge of particular interest in micro-randomized trials (MRTs) that aid researchers in developing just-in-time adaptive interventions that have population-level effects while still providing personalized recommendations to individuals. In this paper, we first develop ROGUE-TS, a Thompson Sampling algorithm tailored to the ROGUE framework, and provide theoretical guarantees of sublinear regret. We then introduce a probability clipping procedure to balance personalization and population-level learning, with quantified trade-off that balances regret and minimum exploration probability. Validation on two MRT datasets concerning physical activity promotion and bipolar disorder treatment shows that our methods both achieve lower regret than existing approaches and maintain high statistical power through the clipping procedure without significantly increasing regret. This enables reliable detection of treatment effects while accounting for individual behavioral dynamics. For researchers designing MRTs, our framework offers practical guidance on balancing personalization with statistical validity.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "EvtSlowTV - A Large and Diverse Dataset for Event-Based Depth Estimation",
      "link": "https://arxiv.org/abs/2511.02953",
      "description": "arXiv:2511.02953v1 Announce Type: cross \nAbstract: Event cameras, with their high dynamic range (HDR) and low latency, offer a promising alternative for robust depth estimation in challenging environments. However, many event-based depth estimation approaches are constrained by small-scale annotated datasets, limiting their generalizability to real-world scenarios. To bridge this gap, we introduce EvtSlowTV, a large-scale event camera dataset curated from publicly available YouTube footage, which contains more than 13B events across various environmental conditions and motions, including seasonal hiking, flying, scenic driving, and underwater exploration. EvtSlowTV is an order of magnitude larger than existing event datasets, providing an unconstrained, naturalistic setting for event-based depth learning. This work shows the suitability of EvtSlowTV for a self-supervised learning framework to capitalise on the HDR potential of raw event streams. We further demonstrate that training with EvtSlowTV enhances the model's ability to generalise to complex scenes and motions. Our approach removes the need for frame-based annotations and preserves the asynchronous nature of event data.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Value of Information-Enhanced Exploration in Bootstrapped DQN",
      "link": "https://arxiv.org/abs/2511.02969",
      "description": "arXiv:2511.02969v1 Announce Type: cross \nAbstract: Efficient exploration in deep reinforcement learning remains a fundamental challenge, especially in environments characterized by high-dimensional states and sparse rewards. Traditional exploration strategies that rely on random local policy noise, such as $\\epsilon$-greedy and Boltzmann exploration methods, often struggle to efficiently balance exploration and exploitation. In this paper, we integrate the notion of (expected) value of information (EVOI) within the well-known Bootstrapped DQN algorithmic framework, to enhance the algorithm's deep exploration ability. Specifically, we develop two novel algorithms that incorporate the expected gain from learning the value of information into Bootstrapped DQN. Our methods use value of information estimates to measure the discrepancies of opinions among distinct network heads, and drive exploration towards areas with the most potential. We evaluate our algorithms with respect to performance and their ability to exploit inherent uncertainty arising from random network initialization. Our experiments in complex, sparse-reward Atari games demonstrate increased performance, all the while making better use of uncertainty, and, importantly, without introducing extra hyperparameters.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Systematizing LLM Persona Design: A Four-Quadrant Technical Taxonomy for AI Companion Applications",
      "link": "https://arxiv.org/abs/2511.02979",
      "description": "arXiv:2511.02979v1 Announce Type: cross \nAbstract: The design and application of LLM-based personas in AI companionship is a rapidly expanding but fragmented field, spanning from virtual emotional compan- ions and game NPCs to embodied functional robots. This diversity in objectives, modality, and technical stacks creates an urgent need for a unified framework. To address this gap, this paper systematizes the field by proposing a Four-Quadrant Technical Taxonomy for AI companion applications. The framework is structured along two critical axes: Virtual vs. Embodied and Emotional Companionship vs. Functional Augmentation. Quadrant I (Virtual Companionship) explores virtual idols, romantic companions, and story characters, introducing a four-layer technical framework to analyze their challenges in maintaining long-term emotional consistency. Quadrant II (Functional Virtual Assistants) analyzes AI applica- tions in work, gaming, and mental health, highlighting the shift from \"feeling\" to \"thinking and acting\" and pinpointing key technologies like enterprise RAG and on-device inference. Quadrants III & IV (Embodied Intelligence) shift from the virtual to the physical world, analyzing home robots and vertical-domain assistants, revealing core challenges in symbol grounding, data privacy, and ethical liability. This taxonomy provides not only a systematic map for researchers and developers to navigate the complex persona design space but also a basis for policymakers to identify and address the unique risks inherent in different application scenarios.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "SLIP: Structural-aware Language-Image Pretraining for Vision-Language Alignment",
      "link": "https://arxiv.org/abs/2511.03019",
      "description": "arXiv:2511.03019v1 Announce Type: cross \nAbstract: Vision-Language Pretraining (VLP) has achieved remarkable success across various downstream tasks, but such gains are largely driven by scaling up on training data. Yet, literature methods treat image-text pairs as isolated training examples; this neglects the rich relational structure naturally present in many domains, such as e-commerce product co-purchase graphs and social recommendation networks. Inspired by neuroscientific evidence that human encodes knowledge as relationship cognitive maps, we introduce Structure-aware Language-Image Pretraining (SLIP). SLIP integrates a structural contrastive loss to align modalities while also modeling relationships between neighboring entities in a structured graph. To support this paradigm, we construct a large-scale Amazon Product Co-purchase Multimodal Graph Dataset, enabling structured cross-modality supervision at scale. Experiment results show that SLIP consistently outperforms CLIP on cross-modal retrieval and classification tasks in both zero-shot and few-shot settings, showing the value of relational supervision for cross-modal alignment.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Adaptive-Sensorless Monitoring of Shipping Containers",
      "link": "https://arxiv.org/abs/2511.03022",
      "description": "arXiv:2511.03022v1 Announce Type: cross \nAbstract: Monitoring the internal temperature and humidity of shipping containers is essential to preventing quality degradation during cargo transportation. Sensorless monitoring -- machine learning models that predict the internal conditions of the containers using exogenous factors -- shows promise as an alternative to monitoring using sensors. However, it does not incorporate telemetry information and correct for systematic errors, causing the predictions to differ significantly from the live data and confusing the users. In this paper, we introduce the residual correction method, a general framework for correcting for systematic biases in sensorless models after observing live telemetry data. We call this class of models ``adaptive-sensorless'' monitoring. We train and evaluate adaptive-sensorless models on the 3.48 million data points -- the largest dataset of container sensor readings ever used in academic research -- and show that they produce consistent improvements over the baseline sensorless models. When evaluated on the holdout set of the simulated data, they achieve average mean absolute errors (MAEs) of 2.24 $\\sim$ 2.31$^\\circ$C (vs 2.43$^\\circ$C by sensorless) for temperature and 5.72 $\\sim$ 7.09% for relative humidity (vs 7.99% by sensorless) and average root mean-squared errors (RMSEs) of 3.19 $\\sim$ 3.26$^\\circ$C for temperature (vs 3.38$^\\circ$C by sensorless) and 7.70 $\\sim$ 9.12% for relative humidity (vs 10.0% by sensorless). Adaptive-sensorless models enable more accurate cargo monitoring, early risk detection, and less dependence on full connectivity in global shipping.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Reading Between the Lines: The One-Sided Conversation Problem",
      "link": "https://arxiv.org/abs/2511.03056",
      "description": "arXiv:2511.03056v1 Announce Type: cross \nAbstract: Conversational AI is constrained in many real-world settings where only one side of a dialogue can be recorded, such as telemedicine, call centers, and smart glasses. We formalize this as the one-sided conversation problem (1SC): inferring and learning from one side of a conversation. We study two tasks: (1) reconstructing the missing speaker's turns for real-time use cases, and (2) generating summaries from one-sided transcripts. Evaluating prompting and finetuned models on MultiWOZ, DailyDialog, and Candor with both human A/B testing and LLM-as-a-judge metrics, we find that access to one future turn and information about utterance length improves reconstruction, placeholder prompting helps to mitigate hallucination, and while large models generate promising reconstructions with prompting, smaller models require finetuning. Further, high-quality summaries can be generated without reconstructing missing turns. We present 1SC as a novel challenge and report promising results that mark a step toward privacy-aware conversational AI.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Sparse, self-organizing ensembles of local kernels detect rare statistical anomalies",
      "link": "https://arxiv.org/abs/2511.03095",
      "description": "arXiv:2511.03095v1 Announce Type: cross \nAbstract: Modern artificial intelligence has revolutionized our ability to extract rich and versatile data representations across scientific disciplines. Yet, the statistical properties of these representations remain poorly controlled, causing misspecified anomaly detection (AD) methods to falter. Weak or rare signals can remain hidden within the apparent regularity of normal data, creating a gap in our ability to detect and interpret anomalies. We examine this gap and identify a set of structural desiderata for detection methods operating under minimal prior information: sparsity, to enforce parsimony; locality, to preserve geometric sensitivity; and competition, to promote efficient allocation of model capacity. These principles define a class of self-organizing local kernels that adaptively partition the representation space around regions of statistical imbalance. As an instantiation of these principles, we introduce SparKer, a sparse ensemble of Gaussian kernels trained within a semi-supervised Neyman--Pearson framework to locally model the likelihood ratio between a sample that may contain anomalies and a nominal, anomaly-free reference. We provide theoretical insights into the mechanisms that drive detection and self-organization in the proposed model, and demonstrate the effectiveness of this approach on realistic high-dimensional problems of scientific discovery, open-world novelty detection, intrusion detection, and generative-model validation. Our applications span both the natural- and computer-science domains. We demonstrate that ensembles containing only a handful of kernels can identify statistically significant anomalous locations within representation spaces of thousands of dimensions, underscoring both the interpretability, efficiency and scalability of the proposed approach.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Scaling Multi-Agent Environment Co-Design with Diffusion Models",
      "link": "https://arxiv.org/abs/2511.03100",
      "description": "arXiv:2511.03100v1 Announce Type: cross \nAbstract: The agent-environment co-design paradigm jointly optimises agent policies and environment configurations in search of improved system performance. With application domains ranging from warehouse logistics to windfarm management, co-design promises to fundamentally change how we deploy multi-agent systems. However, current co-design methods struggle to scale. They collapse under high-dimensional environment design spaces and suffer from sample inefficiency when addressing moving targets inherent to joint optimisation. We address these challenges by developing Diffusion Co-Design (DiCoDe), a scalable and sample-efficient co-design framework pushing co-design towards practically relevant settings. DiCoDe incorporates two core innovations. First, we introduce Projected Universal Guidance (PUG), a sampling technique that enables DiCoDe to explore a distribution of reward-maximising environments while satisfying hard constraints such as spatial separation between obstacles. Second, we devise a critic distillation mechanism to share knowledge from the reinforcement learning critic, ensuring that the guided diffusion model adapts to evolving agent policies using a dense and up-to-date learning signal. Together, these improvements lead to superior environment-policy pairs when validated on challenging multi-agent environment co-design benchmarks including warehouse automation, multi-agent pathfinding and wind farm optimisation. Our method consistently exceeds the state-of-the-art, achieving, for example, 39% higher rewards in the warehouse setting with 66% fewer simulation samples. This sets a new standard in agent-environment co-design, and is a stepping stone towards reaping the rewards of co-design in real world domains.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "CARMA: Comprehensive Automatically-annotated Reddit Mental Health Dataset for Arabic",
      "link": "https://arxiv.org/abs/2511.03102",
      "description": "arXiv:2511.03102v1 Announce Type: cross \nAbstract: Mental health disorders affect millions worldwide, yet early detection remains a major challenge, particularly for Arabic-speaking populations where resources are limited and mental health discourse is often discouraged due to cultural stigma. While substantial research has focused on English-language mental health detection, Arabic remains significantly underexplored, partly due to the scarcity of annotated datasets. We present CARMA, the first automatically annotated large-scale dataset of Arabic Reddit posts. The dataset encompasses six mental health conditions, such as Anxiety, Autism, and Depression, and a control group. CARMA surpasses existing resources in both scale and diversity. We conduct qualitative and quantitative analyses of lexical and semantic differences between users, providing insights into the linguistic markers of specific mental health conditions. To demonstrate the dataset's potential for further mental health analysis, we perform classification experiments using a range of models, from shallow classifiers to large language models. Our results highlight the promise of advancing mental health detection in underrepresented languages such as Arabic.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Adaptive Detection of Software Aging under Workload Shift",
      "link": "https://arxiv.org/abs/2511.03103",
      "description": "arXiv:2511.03103v1 Announce Type: cross \nAbstract: Software aging is a phenomenon that affects long-running systems, leading to progressive performance degradation and increasing the risk of failures. To mitigate this problem, this work proposes an adaptive approach based on machine learning for software aging detection in environments subject to dynamic workload conditions. We evaluate and compare a static model with adaptive models that incorporate adaptive detectors, specifically the Drift Detection Method (DDM) and Adaptive Windowing (ADWIN), originally developed for concept drift scenarios and applied in this work to handle workload shifts. Experiments with simulated sudden, gradual, and recurring workload transitions show that static models suffer a notable performance drop when applied to unseen workload profiles, whereas the adaptive model with ADWIN maintains high accuracy, achieving an F1-Score above 0.93 in all analyzed scenarios.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "FP-AbDiff: Improving Score-based Antibody Design by Capturing Nonequilibrium Dynamics through the Underlying Fokker-Planck Equation",
      "link": "https://arxiv.org/abs/2511.03113",
      "description": "arXiv:2511.03113v1 Announce Type: cross \nAbstract: Computational antibody design holds immense promise for therapeutic discovery, yet existing generative models are fundamentally limited by two core challenges: (i) a lack of dynamical consistency, which yields physically implausible structures, and (ii) poor generalization due to data scarcity and structural bias. We introduce FP-AbDiff, the first antibody generator to enforce Fokker-Planck Equation (FPE) physics along the entire generative trajectory. Our method minimizes a novel FPE residual loss over the mixed manifold of CDR geometries (R^3 x SO(3)), compelling locally-learned denoising scores to assemble into a globally coherent probability flow. This physics-informed regularizer is synergistically integrated with deep biological priors within a state-of-the-art SE(3)-equivariant diffusion framework. Rigorous evaluation on the RAbD benchmark confirms that FP-AbDiff establishes a new state-of-the-art. In de novo CDR-H3 design, it achieves a mean Root Mean Square Deviation of 0.99 {\\AA} when superposing on the variable region, a 25% improvement over the previous state-of-the-art model, AbX, and the highest reported Contact Amino Acid Recovery of 39.91%. This superiority is underscored in the more challenging six-CDR co-design task, where our model delivers consistently superior geometric precision, cutting the average full-chain Root Mean Square Deviation by ~15%, and crucially, achieves the highest full-chain Amino Acid Recovery on the functionally dominant CDR-H3 loop (45.67%). By aligning generative dynamics with physical laws, FP-AbDiff enhances robustness and generalizability, establishing a principled approach for physically faithful and functionally viable antibody design.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "An Augmentation Overlap Theory of Contrastive Learning",
      "link": "https://arxiv.org/abs/2511.03114",
      "description": "arXiv:2511.03114v1 Announce Type: cross \nAbstract: Recently, self-supervised contrastive learning has achieved great success on various tasks. However, its underlying working mechanism is yet unclear. In this paper, we first provide the tightest bounds based on the widely adopted assumption of conditional independence. Further, we relax the conditional independence assumption to a more practical assumption of augmentation overlap and derive the asymptotically closed bounds for the downstream performance. Our proposed augmentation overlap theory hinges on the insight that the support of different intra-class samples will become more overlapped under aggressive data augmentations, thus simply aligning the positive samples (augmented views of the same sample) could make contrastive learning cluster intra-class samples together. Moreover, from the newly derived augmentation overlap perspective, we develop an unsupervised metric for the representation evaluation of contrastive learning, which aligns well with the downstream performance almost without relying on additional modules. Code is available at https://github.com/PKU-ML/GARC.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Image-Intrinsic Priors for Integrated Circuit Defect Detection and Novel Class Discovery via Self-Supervised Learning",
      "link": "https://arxiv.org/abs/2511.03120",
      "description": "arXiv:2511.03120v1 Announce Type: cross \nAbstract: Integrated circuit manufacturing is highly complex, comprising hundreds of process steps. Defects can arise at any stage, causing yield loss and ultimately degrading product reliability. Supervised methods require extensive human annotation and struggle with emergent categories and rare, data scarce defects. Clustering-based unsupervised methods often exhibit unstable performance due to missing priors. We propose IC DefectNCD, a support set free framework that leverages Image Intrinsic Priors in IC SEM images for defect detection and novel class discovery. We first develop Self Normal Information Guided IC Defect Detection, aggregating representative normal features via a learnable normal information extractor and using reconstruction residuals to coarsely localize defect regions. To handle saliency variations across defects, we introduce an adaptive binarization strategy that produces stable subimages focused on core defective areas. Finally, we design Self Defect Information Guided IC Defect Classification, which incorporates a soft mask guided attention mechanism to inject spatial defect priors into the teacher student model. This enhances sensitivity to defective regions, suppresses background interference, and enables recognition and classification of unseen defects. We validate the approach on a real world dataset spanning three key fabrication stages and covering 15 defect types. Experiments demonstrate robust performance on both defect detection and unseen defect classification.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Control Barrier Function for Aligning Large Language Models",
      "link": "https://arxiv.org/abs/2511.03121",
      "description": "arXiv:2511.03121v1 Announce Type: cross \nAbstract: This paper proposes a control-based framework for aligning large language models (LLMs) by leveraging a control barrier function (CBF) to ensure user-desirable text generation. The presented framework applies the CBF safety filter to the predicted token generated from the baseline LLM, to intervene in the generated text. The safety filter includes two significant advantages: this safety filter is an add-on type, allowing it to be used for alignment purposes without fine-tuning the baseline LLM, and if there is an evaluation model regarding the desired alignment, it can be directly applied to the filter design. The overall text-generation system is implemented with open-source language models, aiming to generate positive text.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "EGMOF: Efficient Generation of Metal-Organic Frameworks Using a Hybrid Diffusion-Transformer Architecture",
      "link": "https://arxiv.org/abs/2511.03122",
      "description": "arXiv:2511.03122v1 Announce Type: cross \nAbstract: Designing materials with targeted properties remains challenging due to the vastness of chemical space and the scarcity of property-labeled data. While recent advances in generative models offer a promising way for inverse design, most approaches require large datasets and must be retrained for every new target property. Here, we introduce the EGMOF (Efficient Generation of MOFs), a hybrid diffusion-transformer framework that overcomes these limitations through a modular, descriptor-mediated workflow. EGMOF decomposes inverse design into two steps: (1) a one-dimensional diffusion model (Prop2Desc) that maps desired properties to chemically meaningful descriptors followed by (2) a transformer model (Desc2MOF) that generates structures from these descriptors. This modular hybrid design enables minimal retraining and maintains high accuracy even under small-data conditions. On a hydrogen uptake dataset, EGMOF achieved over 95% validity and 84% hit rate, representing significant improvements of up to 57% in validity and 14% in hit rate compared to existing methods, while remaining effective with only 1,000 training samples. Moreover, our model successfully performed conditional generation across 29 diverse property datasets, including CoREMOF, QMOF, and text-mined experimental datasets, whereas previous models have not. This work presents a data-efficient, generalizable approach to the inverse design of diverse MOFs and highlights the potential of modular inverse design workflows for broader materials discovery.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Optimal Boundary Control of Diffusion on Graphs via Linear Programming",
      "link": "https://arxiv.org/abs/2511.03129",
      "description": "arXiv:2511.03129v1 Announce Type: cross \nAbstract: We propose a linear programming (LP) framework for steady-state diffusion and flux optimization on geometric networks. The state variable satisfies a discrete diffusion law on a weighted, oriented graph, where conductances are scaled by edge lengths to preserve geometric fidelity. Boundary potentials act as controls that drive interior fluxes according to a linear network Laplacian. The optimization problem enforces physically meaningful sign and flux-cap constraints at all boundary edges, derived directly from a gradient bound. This yields a finite-dimensional LP whose feasible set is polyhedral, and whose boundedness and solvability follow from simple geometric or algebraic conditions on the network data.\n  We prove that under the absence of negative recession directions--automatically satisfied in the presence of finite box bounds, flux caps, or sign restrictions--the LP admits a global minimizer. Several sufficient conditions guaranteeing boundedness of the feasible region are identified, covering both full-rank and rank-deficient flux maps. The analysis connects classical results such as the Minkowski--Weyl decomposition, Hoffman's bound, and the fundamental theorem of linear programming with modern network-based diffusion modeling.\n  Two large-scale examples illustrate the framework: (i) A typical large stadium in a major modern city, which forms a single connected component with relatively uniform corridor widths, and a (ii) A complex street network emanating from a large, historical city center, which forms a multi-component system.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Deploying Rapid Damage Assessments from sUAS Imagery for Disaster Response",
      "link": "https://arxiv.org/abs/2511.03132",
      "description": "arXiv:2511.03132v1 Announce Type: cross \nAbstract: This paper presents the first AI/ML system for automating building damage assessment in uncrewed aerial systems (sUAS) imagery to be deployed operationally during federally declared disasters (Hurricanes Debby and Helene). In response to major disasters, sUAS teams are dispatched to collect imagery of the affected areas to assess damage; however, at recent disasters, teams collectively delivered between 47GB and 369GB of imagery per day, representing more imagery than can reasonably be transmitted or interpreted by subject matter experts in the disaster scene, thus delaying response efforts. To alleviate this data avalanche encountered in practice, computer vision and machine learning techniques are necessary. While prior work has been deployed to automatically assess damage in satellite imagery, there is no current state of practice for sUAS-based damage assessment systems, as all known work has been confined to academic settings. This work establishes the state of practice via the development and deployment of models for building damage assessment with sUAS imagery. The model development involved training on the largest known dataset of post-disaster sUAS aerial imagery, containing 21,716 building damage labels, and the operational training of 91 disaster practitioners. The best performing model was deployed during the responses to Hurricanes Debby and Helene, where it assessed a combined 415 buildings in approximately 18 minutes. This work contributes documentation of the actual use of AI/ML for damage assessment during a disaster and lessons learned to the benefit of the AI/ML research and user communities.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "From Measurement to Expertise: Empathetic Expert Adapters for Context-Based Empathy in Conversational AI Agents",
      "link": "https://arxiv.org/abs/2511.03143",
      "description": "arXiv:2511.03143v1 Announce Type: cross \nAbstract: Empathy is a critical factor in fostering positive user experiences in conversational AI. While models can display empathy, it is often generic rather than tailored to specific tasks and contexts. In this work, we introduce a novel framework for developing and evaluating context-specific empathetic large language models (LLMs). We first analyze a real-world conversational dataset consisting of 672 multi-turn conversations across 8 tasks, revealing significant differences in terms of expected and experienced empathy before and after the conversations, respectively. To help minimize this gap, we develop a synthetic multi-turn conversational generation pipeline and steer responses toward our defined empathy patterns based on the context that more closely matches users' expectations. We then train empathetic expert adapters for context-specific empathy that specialize in varying empathy levels based on the recognized task. Our empirical results demonstrate a significant gap reduction of 72.66% between perceived and desired empathy with scores increasing by an average factor of 2.43 as measured by our metrics and reward models. Additionally, our trained empathetic expert adapters demonstrate superior effectiveness in preserving empathy patterns throughout conversation turns, outperforming system prompts, which tend to dramatically diminish in impact as conversations lengthen.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Forecast2Anomaly (F2A): Adapting Multivariate Time Series Foundation Models for Anomaly Prediction",
      "link": "https://arxiv.org/abs/2511.03149",
      "description": "arXiv:2511.03149v1 Announce Type: cross \nAbstract: Forecasting anomalies (anomaly prediction) in multivariate time series from different real-world, dynamic, and complex systems is vital for preempting critical failures, leading to a substantial minimization in operational costs and human labor. Yet, existing methods are limited to specific systems while failing to generalize to evolving anomaly patterns over time. In contrast, pretrained Time Series Foundation Models (TSFMs) have recently demonstrated strong generalization and zero-shot forecasting capabilities. However, their potential remains untapped for anomaly prediction, a task fundamentally different from forecasting normal behavior. Thus, we present Forecast2Anomaly (F2A), a novel framework that empowers TSFMs with anomaly prediction abilities through two key innovations. First, we propose a joint forecast-anomaly loss that fine-tunes TSFMs to accurately forecast future signals even at anomalous time points. Second, we introduce a Retrieval-Augmented Generation (RAG) module that retrieves historically relevant horizons and conditions predictions on them. This component dynamically adapts to distributional shifts at inference time, enabling F2A to track evolving anomalies without requiring model updates. By combining targeted fine-tuning with dynamic retrieval, F2A bridges the gap between robust TSFM zero-shot forecasting and zero-shot anomaly prediction. Extensive experiments across 16 diverse datasets and multiple TSFM backbones show that F2A consistently outperforms state-of-the-art methods, offering a scalable, zero-shot anomaly prediction solution for real-world applications.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Who Sees the Risk? Stakeholder Conflicts and Explanatory Policies in LLM-based Risk Assessment",
      "link": "https://arxiv.org/abs/2511.03152",
      "description": "arXiv:2511.03152v1 Announce Type: cross \nAbstract: Understanding how different stakeholders perceive risks in AI systems is essential for their responsible deployment. This paper presents a framework for stakeholder-grounded risk assessment by using LLMs, acting as judges to predict and explain risks. Using the Risk Atlas Nexus and GloVE explanation method, our framework generates stakeholder-specific, interpretable policies that shows how different stakeholders agree or disagree about the same risks. We demonstrate our method using three real-world AI use cases of medical AI, autonomous vehicles, and fraud detection domain. We further propose an interactive visualization that reveals how and why conflicts emerge across stakeholder perspectives, enhancing transparency in conflict reasoning. Our results show that stakeholder perspectives significantly influence risk perception and conflict patterns. Our work emphasizes the importance of these stakeholder-aware explanations needed to make LLM-based evaluations more transparent, interpretable, and aligned with human-centered AI governance goals.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "RefAgent: A Multi-agent LLM-based Framework for Automatic Software Refactoring",
      "link": "https://arxiv.org/abs/2511.03153",
      "description": "arXiv:2511.03153v1 Announce Type: cross \nAbstract: Large Language Models (LLMs) have substantially influenced various software engineering tasks. Indeed, in the case of software refactoring, traditional LLMs have shown the ability to reduce development time and enhance code quality. However, these LLMs often rely on static, detailed instructions for specific tasks. In contrast, LLM-based agents can dynamically adapt to evolving contexts and autonomously make decisions by interacting with software tools and executing workflows. In this paper, we explore the potential of LLM-based agents in supporting refactoring activities. Specifically, we introduce RefAgent, a multi-agent LLM-based framework for end-to-end software refactoring. RefAgent consists of specialized agents responsible for planning, executing, testing, and iteratively refining refactorings using self-reflection and tool-calling capabilities. We evaluate RefAgent on eight open-source Java projects, comparing its effectiveness against a single-agent approach, a search-based refactoring tool, and historical developer refactorings. Our assessment focuses on: (1) the impact of generated refactorings on software quality, (2) the ability to identify refactoring opportunities, and (3) the contribution of each LLM agent through an ablation study. Our results show that RefAgent achieves a median unit test pass rate of 90%, reduces code smells by a median of 52.5%, and improves key quality attributes (e.g., reusability) by a median of 8.6%. Additionally, it closely aligns with developer refactorings and the search-based tool in identifying refactoring opportunities, attaining a median F1-score of 79.15% and 72.7%, respectively. Compared to single-agent approaches, RefAgent improves the median unit test pass rate by 64.7% and the median compilation success rate by 40.1%. These findings highlight the promise of multi-agent architectures in advancing automated software refactoring.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "GraphCliff: Short-Long Range Gating for Subtle Differences but Critical Changes",
      "link": "https://arxiv.org/abs/2511.03170",
      "description": "arXiv:2511.03170v1 Announce Type: cross \nAbstract: Quantitative structure-activity relationship assumes a smooth relationship between molecular structure and biological activity. However, activity cliffs defined as pairs of structurally similar compounds with large potency differences break this continuity. Recent benchmarks targeting activity cliffs have revealed that classical machine learning models with extended connectivity fingerprints outperform graph neural networks. Our analysis shows that graph embeddings fail to adequately separate structurally similar molecules in the embedding space, making it difficult to distinguish between structurally similar but functionally different molecules. Despite this limitation, molecular graph structures are inherently expressive and attractive, as they preserve molecular topology. To preserve the structural representation of molecules as graphs, we propose a new model, GraphCliff, which integrates short- and long-range information through a gating mechanism. Experimental results demonstrate that GraphCliff consistently improves performance on both non-cliff and cliff compounds. Furthermore, layer-wise node embedding analyses reveal reduced over-smoothing and enhanced discriminative power relative to strong baseline graph models.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Optimizing Earth-Moon Transfer and Cislunar Navigation: Integrating Low-Energy Trajectories, AI Techniques and GNSS-R Technologies",
      "link": "https://arxiv.org/abs/2511.03173",
      "description": "arXiv:2511.03173v1 Announce Type: cross \nAbstract: The rapid growth of cislunar activities, including lunar landings, the Lunar Gateway, and in-space refueling stations, requires advances in cost-efficient trajectory design and reliable integration of navigation and remote sensing. Traditional Earth-Moon transfers suffer from rigid launch windows and high propellant demands, while Earth-based GNSS systems provide little to no coverage beyond geostationary orbit. This limits autonomy and environmental awareness in cislunar space. This review compares four major transfer strategies by evaluating velocity requirements, flight durations, and fuel efficiency, and by identifying their suitability for both crewed and robotic missions. The emerging role of artificial intelligence and machine learning is highlighted: convolutional neural networks support automated crater recognition and digital terrain model generation, while deep reinforcement learning enables adaptive trajectory refinement during descent and landing to reduce risk and decision latency. The study also examines how GNSS-Reflectometry and advanced Positioning, Navigation, and Timing architectures can extend navigation capabilities beyond current limits. GNSS-R can act as a bistatic radar for mapping lunar ice, soil properties, and surface topography, while PNT systems support autonomous rendezvous, Lagrange point station-keeping, and coordinated satellite swarm operations. Combining these developments establishes a scalable framework for sustainable cislunar exploration and long-term human and robotic presence.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Efficient Linear Attention for Multivariate Time Series Modeling via Entropy Equality",
      "link": "https://arxiv.org/abs/2511.03190",
      "description": "arXiv:2511.03190v1 Announce Type: cross \nAbstract: Attention mechanisms have been extensively employed in various applications, including time series modeling, owing to their capacity to capture intricate dependencies; however, their utility is often constrained by quadratic computational complexity, which impedes scalability for long sequences. In this work, we propose a novel linear attention mechanism designed to overcome these limitations. Our approach is grounded in a theoretical demonstration that entropy, as a strictly concave function on the probability simplex, implies that distributions with aligned probability rankings and similar entropy values exhibit structural resemblance. Building on this insight, we develop an efficient approximation algorithm that computes the entropy of dot-product-derived distributions with only linear complexity, enabling the implementation of a linear attention mechanism based on entropy equality. Through rigorous analysis, we reveal that the effectiveness of attention in spatio-temporal time series modeling may not primarily stem from the non-linearity of softmax but rather from the attainment of a moderate and well-balanced weight distribution. Extensive experiments on four spatio-temporal datasets validate our method, demonstrating competitive or superior forecasting performance while achieving substantial reductions in both memory usage and computational time.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "A Quantized VAE-MLP Botnet Detection Model: A Systematic Evaluation of Quantization-Aware Training and Post-Training Quantization Strategies",
      "link": "https://arxiv.org/abs/2511.03201",
      "description": "arXiv:2511.03201v1 Announce Type: cross \nAbstract: In an effort to counter the increasing IoT botnet-based attacks, state-of-the-art deep learning methods have been proposed and have achieved impressive detection accuracy. However, their computational intensity restricts deployment on resource-constrained IoT devices, creating a critical need for lightweight detection models. A common solution to this challenge is model compression via quantization. This study proposes a VAE-MLP model framework where an MLP-based classifier is trained on 8-dimensional latent vectors derived from the high-dimensional train data using the encoder component of a pretrained variational autoencoder (VAE). Two widely used quantization strategies--Quantization-Aware Training (QAT) and Post-Training Quantization (PTQ)--are then systematically evaluated in terms of their impact on detection performance, storage efficiency, and inference latency using two benchmark IoT botnet datasets--N-BaIoT and CICIoT2022. The results revealed that, with respect to detection accuracy, the QAT strategy experienced a more noticeable decline,whereas PTQ incurred only a marginal reduction compared to the original unquantized model. Furthermore, PTQ yielded a 6x speedup and 21x reduction in size, while QAT achieved a 3x speedup and 24x compression, demonstrating the practicality of quantization for device-level IoT botnet detection.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "QG-CoC: Question-Guided Chain-of-Captions for Large Multimodal Models",
      "link": "https://arxiv.org/abs/2511.03206",
      "description": "arXiv:2511.03206v1 Announce Type: cross \nAbstract: Recently, Multimodal Large Language Models (MLLMs) encounter two key issues in multi-image contexts: (1) a lack of fine-grained perception across disparate images, and (2) a diminished capability to effectively reason over and synthesize information from multiple visual inputs. However, while various prompting methods aim to describe visual content, many existing studies focus primarily on single-image settings or specific, constrained scenarios. This leaves a critical gap in understanding and addressing how MLLMs tackle more general and complex multi-image reasoning tasks. Thus, we first extensively investigate how current prompting methods perceive fine-grained visual details and process visual information when dealing with multiple images. Our findings reveal that existing prompting methods fall short in attending to needed clues and seamlessly integrating perception and reasoning. Inspired by the findings, we propose a new zero-shot prompting method, Question-Guided Chain-of-Captions (QG-CoC), a generalized prompting approach that effectively handles problems with an arbitrary number of images. We evaluate our method on various open-source and closed-source MLLMs for multi-image and single-image benchmarks. Experimental results indicate that QG-CoC demonstrates competitive performance across tasks and exhibits robust improvements in the challenging scenarios where existing prompting methods fail.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Retrofitters, pragmatists and activists: Public interest litigation for accountable automated decision-making",
      "link": "https://arxiv.org/abs/2511.03211",
      "description": "arXiv:2511.03211v1 Announce Type: cross \nAbstract: This paper examines the role of public interest litigation in promoting accountability for AI and automated decision-making (ADM) in Australia. Since ADM regulatio faces geopolitical headwinds, effective governance will have to rely at least in part on the enforcement of existing laws. Drawing on interviews with Australian public interest litigators, technology policy activists, and technology law scholars, the paper positions public interest litigation as part of a larger ecosystem for transparency, accountability and justice with respect to ADM. It builds on one participants's characterisation of litigation about ADM as an exercise in legal retrofitting: adapting old laws to new circumstances. The paper's primary contribution is to aggregate, organise and present original insights on pragmatic strategies and tactics for effective public interest litigation about ADM. Naturally, it also contends with the limits of these strategies, and of the legal system. Where limits are, however, capable of being overcome, the paper presents findings on urgent needs: the enabling institutional arrangements without which effective litigation and accountability will falter. The paper is relevant to law and technology scholars; individuals and groups harmed by ADM; public interest litigators and technology lawyers; civil society and advocacy organisations; and policymakers.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "LGM: Enhancing Large Language Models with Conceptual Meta-Relations and Iterative Retrieval",
      "link": "https://arxiv.org/abs/2511.03214",
      "description": "arXiv:2511.03214v1 Announce Type: cross \nAbstract: Large language models (LLMs) exhibit strong semantic understanding, yet struggle when user instructions involve ambiguous or conceptually misaligned terms. We propose the Language Graph Model (LGM) to enhance conceptual clarity by extracting meta-relations-inheritance, alias, and composition-from natural language. The model further employs a reflection mechanism to validate these meta-relations. Leveraging a Concept Iterative Retrieval Algorithm, these relations and related descriptions are dynamically supplied to the LLM, improving its ability to interpret concepts and generate accurate responses. Unlike conventional Retrieval-Augmented Generation (RAG) approaches that rely on extended context windows, our method enables large language models to process texts of any length without the need for truncation. Experiments on standard benchmarks demonstrate that the LGM consistently outperforms existing RAG baselines.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Hybrid Fact-Checking that Integrates Knowledge Graphs, Large Language Models, and Search-Based Retrieval Agents Improves Interpretable Claim Verification",
      "link": "https://arxiv.org/abs/2511.03217",
      "description": "arXiv:2511.03217v1 Announce Type: cross \nAbstract: Large language models (LLMs) excel in generating fluent utterances but can lack reliable grounding in verified information. At the same time, knowledge-graph-based fact-checkers deliver precise and interpretable evidence, yet suffer from limited coverage or latency. By integrating LLMs with knowledge graphs and real-time search agents, we introduce a hybrid fact-checking approach that leverages the individual strengths of each component. Our system comprises three autonomous steps: 1) a Knowledge Graph (KG) Retrieval for rapid one - hop lookups in DBpedia, 2) an LM-based classification guided by a task-specific labeling prompt, producing outputs with internal rule-based logic, and 3) a Web Search Agent invoked only when KG coverage is insufficient. Our pipeline achieves an F1 score of 0.93 on the FEVER benchmark on the Supported/Refuted split without task- specific fine - tuning. To address Not enough information cases, we conduct a targeted reannotation study showing that our approach frequently uncovers valid evidence for claims originally labeled as Not Enough Information (NEI), as confirmed by both expert annotators and LLM reviewers. With this paper, we present a modular, opensource fact-checking pipeline with fallback strategies and generalization across datasets.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Node-Based Editing for Multimodal Generation of Text, Audio, Image, and Vide",
      "link": "https://arxiv.org/abs/2511.03227",
      "description": "arXiv:2511.03227v1 Announce Type: cross \nAbstract: We present a node-based storytelling system for multimodal content generation. The system represents stories as graphs of nodes that can be expanded, edited, and iteratively refined through direct user edits and natural-language prompts. Each node can integrate text, images, audio, and video, allowing creators to compose multimodal narratives. A task selection agent routes between specialized generative tasks that handle story generation, node structure reasoning, node diagram formatting, and context generation. The interface supports targeted editing of individual nodes, automatic branching for parallel storylines, and node-based iterative refinement. Our results demonstrate that node-based editing supports control over narrative structure and iterative generation of text, images, audio, and video. We report quantitative outcomes on automatic story outline generation and qualitative observations of editing workflows. Finally, we discuss current limitations such as scalability to longer narratives and consistency across multiple nodes, and outline future work toward human-in-the-loop and user-centered creative AI tools.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "GMoPE:A Prompt-Expert Mixture Framework for Graph Foundation Models",
      "link": "https://arxiv.org/abs/2511.03251",
      "description": "arXiv:2511.03251v1 Announce Type: cross \nAbstract: Graph Neural Networks (GNNs) have demonstrated impressive performance on task-specific benchmarks, yet their ability to generalize across diverse domains and tasks remains limited. Existing approaches often struggle with negative transfer, scalability issues, and high adaptation costs. To address these challenges, we propose GMoPE (Graph Mixture of Prompt-Experts), a novel framework that seamlessly integrates the Mixture-of-Experts (MoE) architecture with prompt-based learning for graphs. GMoPE leverages expert-specific prompt vectors and structure-aware MoE routing to enable each expert to specialize in distinct subdomains and dynamically contribute to predictions. To promote diversity and prevent expert collapse, we introduce a soft orthogonality constraint across prompt vectors, encouraging expert specialization and facilitating a more balanced expert utilization. Additionally, we adopt a prompt-only fine-tuning strategy that significantly reduces spatiotemporal complexity during transfer. We validate GMoPE through extensive experiments under various pretraining strategies and multiple downstream tasks. Results show that GMoPE consistently outperforms state-of-the-art baselines and achieves performance comparable to full parameter fine-tuning-while requiring only a fraction of the adaptation overhead. Our work provides a principled and scalable framework for advancing generalizable and efficient graph foundation models.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Generative deep learning for foundational video translation in ultrasound",
      "link": "https://arxiv.org/abs/2511.03255",
      "description": "arXiv:2511.03255v1 Announce Type: cross \nAbstract: Deep learning (DL) has the potential to revolutionize image acquisition and interpretation across medicine, however, attention to data imbalance and missingness is required. Ultrasound data presents a particular challenge because in addition to different views and structures, it includes several sub-modalities-such as greyscale and color flow doppler (CFD)-that are often imbalanced in clinical studies. Image translation can help balance datasets but is challenging for ultrasound sub-modalities to date. Here, we present a generative method for ultrasound CFD-greyscale video translation, trained on 54,975 videos and tested on 8,368. The method developed leveraged pixel-wise, adversarial, and perceptual loses and utilized two networks: one for reconstructing anatomic structures and one for denoising to achieve realistic ultrasound imaging. Average pairwise SSIM between synthetic videos and ground truth was 0.91+/-0.04. Synthetic videos performed indistinguishably from real ones in DL classification and segmentation tasks and when evaluated by blinded clinical experts: F1 score was 0.9 for real and 0.89 for synthetic videos; Dice score between real and synthetic segmentation was 0.97. Overall clinician accuracy in distinguishing real vs synthetic videos was 54+/-6% (42-61%), indicating realistic synthetic videos. Although trained only on heart videos, the model worked well on ultrasound spanning several clinical domains (average SSIM 0.91+/-0.05), demonstrating foundational abilities. Together, these data expand the utility of retrospectively collected imaging and augment the dataset design toolbox for medical imaging.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Comparing the Performance of LLMs in RAG-based Question-Answering: A Case Study in Computer Science Literature",
      "link": "https://arxiv.org/abs/2511.03261",
      "description": "arXiv:2511.03261v1 Announce Type: cross \nAbstract: Retrieval Augmented Generation (RAG) is emerging as a powerful technique to enhance the capabilities of Generative AI models by reducing hallucination. Thus, the increasing prominence of RAG alongside Large Language Models (LLMs) has sparked interest in comparing the performance of different LLMs in question-answering (QA) in diverse domains. This study compares the performance of four open-source LLMs, Mistral-7b-instruct, LLaMa2-7b-chat, Falcon-7b-instruct and Orca-mini-v3-7b, and OpenAI's trending GPT-3.5 over QA tasks within the computer science literature leveraging RAG support. Evaluation metrics employed in the study include accuracy and precision for binary questions and ranking by a human expert, ranking by Google's AI model Gemini, alongside cosine similarity for long-answer questions. GPT-3.5, when paired with RAG, effectively answers binary and long-answer questions, reaffirming its status as an advanced LLM. Regarding open-source LLMs, Mistral AI's Mistral-7b-instruct paired with RAG surpasses the rest in answering both binary and long-answer questions. However, among the open-source LLMs, Orca-mini-v3-7b reports the shortest average latency in generating responses, whereas LLaMa2-7b-chat by Meta reports the highest average latency. This research underscores the fact that open-source LLMs, too, can go hand in hand with proprietary models like GPT-3.5 with better infrastructure.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "When Generative Artificial Intelligence meets Extended Reality: A Systematic Review",
      "link": "https://arxiv.org/abs/2511.03282",
      "description": "arXiv:2511.03282v1 Announce Type: cross \nAbstract: With the continuous advancement of technology, the application of generative artificial intelligence (AI) in various fields is gradually demonstrating great potential, particularly when combined with Extended Reality (XR), creating unprecedented possibilities. This survey article systematically reviews the applications of generative AI in XR, covering as much relevant literature as possible from 2023 to 2025. The application areas of generative AI in XR and its key technology implementations are summarised through PRISMA screening and analysis of the final 26 articles. The survey highlights existing articles from the last three years related to how XR utilises generative AI, providing insights into current trends and research gaps. We also explore potential opportunities for future research to further empower XR through generative AI, providing guidance and information for future generative XR research.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "How to Evaluate Speech Translation with Source-Aware Neural MT Metrics",
      "link": "https://arxiv.org/abs/2511.03295",
      "description": "arXiv:2511.03295v1 Announce Type: cross \nAbstract: Automatic evaluation of speech-to-text translation (ST) systems is typically performed by comparing translation hypotheses with one or more reference translations. While effective to some extent, this approach inherits the limitation of reference-based evaluation that ignores valuable information from the source input. In machine translation (MT), recent progress has shown that neural metrics incorporating the source text achieve stronger correlation with human judgments. Extending this idea to ST, however, is not trivial because the source is audio rather than text, and reliable transcripts or alignments between source and references are often unavailable. In this work, we conduct the first systematic study of source-aware metrics for ST, with a particular focus on real-world operating conditions where source transcripts are not available. We explore two complementary strategies for generating textual proxies of the input audio, automatic speech recognition (ASR) transcripts, and back-translations of the reference translation, and introduce a novel two-step cross-lingual re-segmentation algorithm to address the alignment mismatch between synthetic sources and reference translations. Our experiments, carried out on two ST benchmarks covering 79 language pairs and six ST systems with diverse architectures and performance levels, show that ASR transcripts constitute a more reliable synthetic source than back-translations when word error rate is below 20%, while back-translations always represent a computationally cheaper but still effective alternative. Furthermore, our cross-lingual re-segmentation algorithm enables robust use of source-aware MT metrics in ST evaluation, paving the way toward more accurate and principled evaluation methodologies for speech translation.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Extending Fair Null-Space Projections for Continuous Attributes to Kernel Methods",
      "link": "https://arxiv.org/abs/2511.03304",
      "description": "arXiv:2511.03304v1 Announce Type: cross \nAbstract: With the on-going integration of machine learning systems into the everyday social life of millions the notion of fairness becomes an ever increasing priority in their development. Fairness notions commonly rely on protected attributes to assess potential biases. Here, the majority of literature focuses on discrete setups regarding both target and protected attributes. The literature on continuous attributes especially in conjunction with regression -- we refer to this as \\emph{continuous fairness} -- is scarce. A common strategy is iterative null-space projection which as of now has only been explored for linear models or embeddings such as obtained by a non-linear encoder. We improve on this by generalizing to kernel methods, significantly extending the scope. This yields a model and fairness-score agnostic method for kernel embeddings applicable to continuous protected attributes. We demonstrate that our novel approach in conjunction with Support Vector Regression (SVR) provides competitive or improved performance across multiple datasets in comparisons to other contemporary methods.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Benchmarking the Thinking Mode of Multimodal Large Language Models in Clinical Tasks",
      "link": "https://arxiv.org/abs/2511.03328",
      "description": "arXiv:2511.03328v1 Announce Type: cross \nAbstract: A recent advancement in Multimodal Large Language Models (MLLMs) research is the emergence of \"reasoning MLLMs\" that offer explicit control over their internal thinking processes (normally referred as the \"thinking mode\") alongside the standard \"non-thinking mode\". This capability allows these models to engage in a step-by-step process of internal deliberation before generating a final response. With the rapid transition to and adoption of these \"dual-state\" MLLMs, this work rigorously evaluated how the enhanced reasoning processes of these MLLMs impact model performance and reliability in clinical tasks. This paper evaluates the active \"thinking mode\" capabilities of two leading MLLMs, Seed1.5-VL and Gemini-2.5-Flash, for medical applications. We assessed their performance on four visual medical tasks using VQA-RAD and ROCOv2 datasets. Our findings reveal that the improvement from activating the thinking mode remains marginal compared to the standard non-thinking mode for the majority of the tasks. Their performance on complex medical tasks such as open-ended VQA and medical image interpretation remains suboptimal, highlighting the need for domain-specific medical data and more advanced methods for medical knowledge integration.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Discourse-Aware Scientific Paper Recommendation via QA-Style Summarization and Multi-Level Contrastive Learning",
      "link": "https://arxiv.org/abs/2511.03330",
      "description": "arXiv:2511.03330v1 Announce Type: cross \nAbstract: The rapid growth of open-access (OA) publications has intensified the challenge of identifying relevant scientific papers. Due to privacy constraints and limited access to user interaction data, recent efforts have shifted toward content-based recommendation, which relies solely on textual information. However, existing models typically treat papers as unstructured text, neglecting their discourse organization and thereby limiting semantic completeness and interpretability. To address these limitations, we propose OMRC-MR, a hierarchical framework that integrates QA-style OMRC (Objective, Method, Result, Conclusion) summarization, multi-level contrastive learning, and structure-aware re-ranking for scholarly recommendation. The QA-style summarization module converts raw papers into structured and discourse-consistent representations, while multi-level contrastive objectives align semantic representations across metadata, section, and document levels. The final re-ranking stage further refines retrieval precision through contextual similarity calibration. Experiments on DBLP, S2ORC, and the newly constructed Sci-OMRC dataset demonstrate that OMRC-MR consistently surpasses state-of-the-art baselines, achieving up to 7.2% and 3.8% improvements in Precision@10 and Recall@10, respectively. Additional evaluations confirm that QA-style summarization produces more coherent and factually complete representations. Overall, OMRC-MR provides a unified and interpretable content-based paradigm for scientific paper recommendation, advancing trustworthy and privacy-aware scholarly information retrieval.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Generative Artificial Intelligence in Bioinformatics: A Systematic Review of Models, Applications, and Methodological Advances",
      "link": "https://arxiv.org/abs/2511.03354",
      "description": "arXiv:2511.03354v1 Announce Type: cross \nAbstract: Generative artificial intelligence (GenAI) has become a transformative approach in bioinformatics that often enables advancements in genomics, proteomics, transcriptomics, structural biology, and drug discovery. To systematically identify and evaluate these growing developments, this review proposed six research questions (RQs), according to the preferred reporting items for systematic reviews and meta-analysis methods. The objective is to evaluate impactful GenAI strategies in methodological advancement, predictive performance, and specialization, and to identify promising approaches for advanced modeling, data-intensive discovery, and integrative biological analysis. RQ1 highlights diverse applications across multiple bioinformatics subfields (sequence analysis, molecular design, and integrative data modeling), which demonstrate superior performance over traditional methods through pattern recognition and output generation. RQ2 reveals that adapted specialized model architectures outperformed general-purpose models, an advantage attributed to targeted pretraining and context-aware strategies. RQ3 identifies significant benefits in the bioinformatics domains, focusing on molecular analysis and data integration, which improves accuracy and reduces errors in complex analysis. RQ4 indicates improvements in structural modeling, functional prediction, and synthetic data generation, validated by established benchmarks. RQ5 suggests the main constraints, such as the lack of scalability and biases in data that impact generalizability, and proposes future directions focused on robust evaluation and biologically grounded modeling. RQ6 examines that molecular datasets (such as UniProtKB and ProteinNet12), cellular datasets (such as CELLxGENE and GTEx) and textual resources (such as PubMedQA and OMIM) broadly support the training and generalization of GenAI models.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Open Source State-Of-the-Art Solution for Romanian Speech Recognition",
      "link": "https://arxiv.org/abs/2511.03361",
      "description": "arXiv:2511.03361v1 Announce Type: cross \nAbstract: In this work, we present a new state-of-the-art Romanian Automatic Speech Recognition (ASR) system based on NVIDIA's FastConformer architecture--explored here for the first time in the context of Romanian. We train our model on a large corpus of, mostly, weakly supervised transcriptions, totaling over 2,600 hours of speech. Leveraging a hybrid decoder with both Connectionist Temporal Classification (CTC) and Token-Duration Transducer (TDT) branches, we evaluate a range of decoding strategies including greedy, ALSD, and CTC beam search with a 6-gram token-level language model. Our system achieves state-of-the-art performance across all Romanian evaluation benchmarks, including read, spontaneous, and domain-specific speech, with up to 27% relative WER reduction compared to previous best-performing systems. In addition to improved transcription accuracy, our approach demonstrates practical decoding efficiency, making it suitable for both research and deployment in low-latency ASR applications.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Decoupling Augmentation Bias in Prompt Learning for Vision-Language Models",
      "link": "https://arxiv.org/abs/2511.03367",
      "description": "arXiv:2511.03367v1 Announce Type: cross \nAbstract: Recent advances in large-scale vision and language models have led to significant progress in zero-shot learning tasks. Methods such as CoOp and CoCoOp have shown that replacing handcrafted prompts with learnable vectors, known as prompt learning, can result in improved performance. However, these models often struggle to generalize to entirely unseen categories. While traditional zero-shot learning techniques benefit from various data augmentation strategies, prompt learning has primarily focused on text-based modifications, leaving the potential of image-based augmentation largely unexplored. In this work, we explore how image-level augmentations, particularly those that introduce attribute-specific variations, can support and enhance prompt learning. Our analysis examines the interaction between these augmentations and soft prompt frameworks, revealing their potential to improve generalization. We also identify a limitation in existing methods, such as CoCoOp, which do not provide explicit guidance for learning prompts that focus on semantically meaningful visual features. To address this, we propose Adding Attributes to Prompt Learning, AAPL, a novel method that introduces adversarial token embeddings to decouple superficial visual variations introduced by augmentation from class-relevant semantic representations. This decoupling enables the learned prompts to concentrate on visually discriminative features that align with the target categories. We conduct comprehensive experiments on eleven benchmark datasets, and AAPL consistently outperforms existing methods across few-shot, zero-shot, cross-dataset, and domain generalization settings. Our source code is publicly available at: https://github.com/Gahyeonkim09/AAPL",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Computational Imaging Meets LLMs: Zero-Shot IDH Mutation Prediction in Brain Gliomas",
      "link": "https://arxiv.org/abs/2511.03376",
      "description": "arXiv:2511.03376v1 Announce Type: cross \nAbstract: We present a framework that combines Large Language Models with computational image analytics for non-invasive, zero-shot prediction of IDH mutation status in brain gliomas. For each subject, coregistered multi-parametric MRI scans and multi-class tumor segmentation maps were processed to extract interpretable semantic (visual) attributes and quantitative features, serialized in a standardized JSON file, and used to query GPT 4o and GPT 5 without fine-tuning. We evaluated this framework on six publicly available datasets (N = 1427) and results showcased high accuracy and balanced classification performance across heterogeneous cohorts, even in the absence of manual annotations. GPT 5 outperformed GPT 4o in context-driven phenotype interpretation. Volumetric features emerged as the most important predictors, supplemented by subtype-specific imaging markers and clinical information. Our results demonstrate the potential of integrating LLM-based reasoning with computational image analytics for precise, non-invasive tumor genotyping, advancing diagnostic strategies in neuro-oncology. The code is available at https://github.com/ATPLab-LUMS/CIM-LLM.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Adaptable Hindsight Experience Replay for Search-Based Learning",
      "link": "https://arxiv.org/abs/2511.03405",
      "description": "arXiv:2511.03405v1 Announce Type: cross \nAbstract: AlphaZero-like Monte Carlo Tree Search systems, originally introduced for two-player games, dynamically balance exploration and exploitation using neural network guidance. This combination makes them also suitable for classical search problems. However, the original method of training the network with simulation results is limited in sparse reward settings, especially in the early stages, where the network cannot yet give guidance. Hindsight Experience Replay (HER) addresses this issue by relabeling unsuccessful trajectories from the search tree as supervised learning signals. We introduce Adaptable HER (\\ours{}), a flexible framework that integrates HER with AlphaZero, allowing easy adjustments to HER properties such as relabeled goals, policy targets, and trajectory selection. Our experiments, including equation discovery, show that the possibility of modifying HER is beneficial and surpasses the performance of pure supervised or reinforcement learning.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Light over Heavy: Automated Performance Requirements Quantification with Linguistic Inducement",
      "link": "https://arxiv.org/abs/2511.03421",
      "description": "arXiv:2511.03421v1 Announce Type: cross \nAbstract: Elicited performance requirements need to be quantified for compliance in different engineering tasks, e.g., configuration tuning and performance testing. Much existing work has relied on manual quantification, which is expensive and error-prone due to the imprecision. In this paper, we present LQPR, a highly efficient automatic approach for performance requirements quantification.LQPR relies on a new theoretical framework that converts quantification as a classification problem. Despite the prevalent applications of Large Language Models (LLMs) for requirement analytics, LQPR takes a different perspective to address the classification: we observed that performance requirements can exhibit strong patterns and are often short/concise, therefore we design a lightweight linguistically induced matching mechanism. We compare LQPR against nine state-of-the-art learning-based approaches over diverse datasets, demonstrating that it is ranked as the sole best for 75% or more cases with two orders less cost. Our work proves that, at least for performance requirement quantification, specialized methods can be more suitable than the general LLM-driven approaches.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Inter-Agent Trust Models: A Comparative Study of Brief, Claim, Proof, Stake, Reputation and Constraint in Agentic Web Protocol Design-A2A, AP2, ERC-8004, and Beyond",
      "link": "https://arxiv.org/abs/2511.03434",
      "description": "arXiv:2511.03434v1 Announce Type: cross \nAbstract: As the \"agentic web\" takes shape-billions of AI agents (often LLM-powered) autonomously transacting and collaborating-trust shifts from human oversight to protocol design. In 2025, several inter-agent protocols crystallized this shift, including Google's Agent-to-Agent (A2A), Agent Payments Protocol (AP2), and Ethereum's ERC-8004 \"Trustless Agents,\" yet their underlying trust assumptions remain under-examined. This paper presents a comparative study of trust models in inter-agent protocol design: Brief (self- or third-party verifiable claims), Claim (self-proclaimed capabilities and identity, e.g. AgentCard), Proof (cryptographic verification, including zero-knowledge proofs and trusted execution environment attestations), Stake (bonded collateral with slashing and insurance), Reputation (crowd feedback and graph-based trust signals), and Constraint (sandboxing and capability bounding). For each, we analyze assumptions, attack surfaces, and design trade-offs, with particular emphasis on LLM-specific fragilities-prompt injection, sycophancy/nudge-susceptibility, hallucination, deception, and misalignment-that render purely reputational or claim-only approaches brittle. Our findings indicate no single mechanism suffices. We argue for trustless-by-default architectures anchored in Proof and Stake to gate high-impact actions, augmented by Brief for identity and discovery and Reputation overlays for flexibility and social signals. We comparatively evaluate A2A, AP2, ERC-8004 and related historical variations in academic research under metrics spanning security, privacy, latency/cost, and social robustness (Sybil/collusion/whitewashing resistance). We conclude with hybrid trust model recommendations that mitigate reputation gaming and misinformed LLM behavior, and we distill actionable design guidelines for safer, interoperable, and scalable agent economies.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "CareMedEval dataset: Evaluating Critical Appraisal and Reasoning in the Biomedical Field",
      "link": "https://arxiv.org/abs/2511.03441",
      "description": "arXiv:2511.03441v1 Announce Type: cross \nAbstract: Critical appraisal of scientific literature is an essential skill in the biomedical field. While large language models (LLMs) can offer promising support in this task, their reliability remains limited, particularly for critical reasoning in specialized domains. We introduce CareMedEval, an original dataset designed to evaluate LLMs on biomedical critical appraisal and reasoning tasks. Derived from authentic exams taken by French medical students, the dataset contains 534 questions based on 37 scientific articles. Unlike existing benchmarks, CareMedEval explicitly evaluates critical reading and reasoning grounded in scientific papers. Benchmarking state-of-the-art generalist and biomedical-specialized LLMs under various context conditions reveals the difficulty of the task: open and commercial models fail to exceed an Exact Match Rate of 0.5 even though generating intermediate reasoning tokens considerably improves the results. Yet, models remain challenged especially on questions about study limitations and statistical analysis. CareMedEval provides a challenging benchmark for grounded reasoning, exposing current LLM limitations and paving the way for future development of automated support for critical appraisal.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Development of the Bioinspired Tendon-Driven DexHand 021 with Proprioceptive Compliance Control",
      "link": "https://arxiv.org/abs/2511.03481",
      "description": "arXiv:2511.03481v1 Announce Type: cross \nAbstract: The human hand plays a vital role in daily life and industrial applications, yet replicating its multifunctional capabilities-including motion, sensing, and coordinated manipulation-with robotic systems remains a formidable challenge. Developing a dexterous robotic hand requires balancing human-like agility with engineering constraints such as complexity, size-to-weight ratio, durability, and force-sensing performance. This letter presents Dex-Hand 021, a high-performance, cable-driven five-finger robotic hand with 12 active and 7 passive degrees of freedom (DoFs), achieving 19 DoFs dexterity in a lightweight 1 kg design. We propose a proprioceptive force-sensing-based admittance control method to enhance manipulation. Experimental results demonstrate its superior performance: a single-finger load capacity exceeding 10 N, fingertip repeatability under 0.001 m, and force estimation errors below 0.2 N. Compared to PID control, joint torques in multi-object grasping are reduced by 31.19%, significantly improves force-sensing capability while preventing overload during collisions. The hand excels in both power and precision grasps, successfully executing 33 GRASP taxonomy motions and complex manipulation tasks. This work advances the design of lightweight, industrial-grade dexterous hands and enhances proprioceptive control, contributing to robotic manipulation and intelligent manufacturing.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "ROSBag MCP Server: Analyzing Robot Data with LLMs for Agentic Embodied AI Applications",
      "link": "https://arxiv.org/abs/2511.03497",
      "description": "arXiv:2511.03497v1 Announce Type: cross \nAbstract: Agentic AI systems and Physical or Embodied AI systems have been two key research verticals at the forefront of Artificial Intelligence and Robotics, with Model Context Protocol (MCP) increasingly becoming a key component and enabler of agentic applications. However, the literature at the intersection of these verticals, i.e., Agentic Embodied AI, remains scarce. This paper introduces an MCP server for analyzing ROS and ROS 2 bags, allowing for analyzing, visualizing and processing robot data with natural language through LLMs and VLMs. We describe specific tooling built with robotics domain knowledge, with our initial release focused on mobile robotics and supporting natively the analysis of trajectories, laser scan data, transforms, or time series data. This is in addition to providing an interface to standard ROS 2 CLI tools (\"ros2 bag list\" or \"ros2 bag info\"), as well as the ability to filter bags with a subset of topics or trimmed in time. Coupled with the MCP server, we provide a lightweight UI that allows the benchmarking of the tooling with different LLMs, both proprietary (Anthropic, OpenAI) and open-source (through Groq). Our experimental results include the analysis of tool calling capabilities of eight different state-of-the-art LLM/VLM models, both proprietary and open-source, large and small. Our experiments indicate that there is a large divide in tool calling capabilities, with Kimi K2 and Claude Sonnet 4 demonstrating clearly superior performance. We also conclude that there are multiple factors affecting the success rates, from the tool description schema to the number of arguments, as well as the number of tools available to the models. The code is available with a permissive license at https://github.com/binabik-ai/mcp-rosbags.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "A Theoretical Framework for Environmental Similarity and Vessel Mobility as Coupled Predictors of Marine Invasive Species Pathways",
      "link": "https://arxiv.org/abs/2511.03499",
      "description": "arXiv:2511.03499v1 Announce Type: cross \nAbstract: Marine invasive species spread through global shipping and generate substantial ecological and economic impacts. Traditional risk assessments require detailed records of ballast water and traffic patterns, which are often incomplete, limiting global coverage. This work advances a theoretical framework that quantifies invasion risk by combining environmental similarity across ports with observed and forecasted maritime mobility. Climate-based feature representations characterize each port's marine conditions, while mobility networks derived from Automatic Identification System data capture vessel flows and potential transfer pathways. Clustering and metric learning reveal climate analogues and enable the estimation of species survival likelihood along shipping routes. A temporal link prediction model captures how traffic patterns may change under shifting environmental conditions. The resulting fusion of environmental similarity and predicted mobility provides exposure estimates at the port and voyage levels, supporting targeted monitoring, routing adjustments, and management interventions.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Efficient Neural Networks with Discrete Cosine Transform Activations",
      "link": "https://arxiv.org/abs/2511.03531",
      "description": "arXiv:2511.03531v1 Announce Type: cross \nAbstract: In this paper, we extend our previous work on the Expressive Neural Network (ENN), a multilayer perceptron with adaptive activation functions parametrized using the Discrete Cosine Transform (DCT). Building upon previous work that demonstrated the strong expressiveness of ENNs with compact architectures, we now emphasize their efficiency, interpretability and pruning capabilities. The DCT-based parameterization provides a structured and decorrelated representation that reveals the functional role of each neuron and allows direct identification of redundant components. Leveraging this property, we propose an efficient pruning strategy that removes unnecessary DCT coefficients with negligible or no loss in performance. Experimental results across classification and implicit neural representation tasks confirm that ENNs achieve state-of-the-art accuracy while maintaining a low number of parameters. Furthermore, up to 40% of the activation coefficients can be safely pruned, thanks to the orthogonality and bounded nature of the DCT basis. Overall, these findings demonstrate that the ENN framework offers a principled integration of signal processing concepts into neural network design, achieving a balanced trade-off between expressiveness, compactness, and interpretability.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "SOLVE-Med: Specialized Orchestration for Leading Vertical Experts across Medical Specialties",
      "link": "https://arxiv.org/abs/2511.03542",
      "description": "arXiv:2511.03542v1 Announce Type: cross \nAbstract: Medical question answering systems face deployment challenges including hallucinations, bias, computational demands, privacy concerns, and the need for specialized expertise across diverse domains. Here, we present SOLVE-Med, a multi-agent architecture combining domain-specialized small language models for complex medical queries. The system employs a Router Agent for dynamic specialist selection, ten specialized models (1B parameters each) fine-tuned on specific medical domains, and an Orchestrator Agent that synthesizes responses. Evaluated on Italian medical forum data across ten specialties, SOLVE-Med achieves superior performance with ROUGE-1 of 0.301 and BERTScore F1 of 0.697, outperforming standalone models up to 14B parameters while enabling local deployment. Our code is publicly available on GitHub: https://github.com/PRAISELab-PicusLab/SOLVE-Med.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Uncovering Code Insights: Leveraging GitHub Artifacts for Deeper Code Understanding",
      "link": "https://arxiv.org/abs/2511.03549",
      "description": "arXiv:2511.03549v1 Announce Type: cross \nAbstract: Understanding the purpose of source code is a critical task in software maintenance, onboarding, and modernization. While large language models (LLMs) have shown promise in generating code explanations, they often lack grounding in the broader software engineering context. We propose a novel approach that leverages natural language artifacts from GitHub -- such as pull request descriptions, issue descriptions and discussions, and commit messages -- to enhance LLM-based code understanding. Our system consists of three components: one that extracts and structures relevant GitHub context, another that uses this context to generate high-level explanations of the code's purpose, and a third that validates the explanation. We implemented this as a standalone tool, as well as a server within the Model Context Protocol (MCP), enabling integration with other AI-assisted development tools. Our main use case is that of enhancing a standard LLM-based code explanation with code insights that our system generates. To evaluate explanations' quality, we conducted a small scale user study, with developers of several open projects, as well as developers of proprietary projects. Our user study indicates that when insights are generated they often are helpful and non trivial, and are free from hallucinations.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "MultiZebraLogic: A Multilingual Logical Reasoning Benchmark",
      "link": "https://arxiv.org/abs/2511.03553",
      "description": "arXiv:2511.03553v1 Announce Type: cross \nAbstract: Measuring the full abilities of large language models (LLMs) requires benchmarks representing multiple tasks. We aim to create large, high-quality datasets for comparison of logical reasoning skills across several languages and of suitable difficulty for LLMs of various reasoning ability. We explore multiple ways of increasing difficulty. We generate zebra puzzles in multiple languages, themes, sizes and including 14 different clue types and 8 red herring types (uninformative clues). We find puzzle sizes 2x3 and 4x5 are sufficiently challenging for GPT-4o mini (a non-reasoning model) and o3-mini (a reasoning model), respectively. Including 5 red herrings decreases o3-mini puzzle-level accuracy on 4x5 puzzles by 15$\\pm$7 %. Scores of o3-mini on 4x5 puzzles are not significantly affected by use of English vs. Danish or the common houses theme vs. the country-specific smoerrebroed theme. We find no correlation between difficulty and the selected clue types. Datasets of 128+1024 puzzles are published as MultiZebraLogic in each of nine Germanic languages for sizes 2x3 and 4x5. We publish code for puzzle generation, designed for adaptablity into more languages and themes.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "AILA--First Experiments with Localist Language Models",
      "link": "https://arxiv.org/abs/2511.03559",
      "description": "arXiv:2511.03559v1 Announce Type: cross \nAbstract: This paper presents the first empirical demonstration of controllable locality in transformer language models, a novel architectural framework that enables continuous control over the degree of representation localization through a tunable locality dial parameter. Unlike traditional language models that rely exclusively on distributed representations, our approach allows dynamic interpolation between highly interpretable localist encodings and efficient distributed representations without requiring model retraining. We conducted experiments on the WikiText corpus using a two-layer transformer architecture, systematically varying the locality parameter {\\lambda} across the full spectrum from 1.0 (fully localist) to 0.0 (fully distributed). Our results demonstrate that localist configurations achieve dramatically lower attention entropy, with {\\lambda} = 1.0 yielding 5.36 bits compared to 7.18 bits at {\\lambda} = 0.0, while maintaining substantially higher pointer fidelity scores reflecting stronger alignment with rule-specified targets. Prediction experiments reveal that intermediate locality values optimize the tradeoff between interpretability and performance, with {\\lambda} = 0.6 achieving test perplexity of 4.65 and accuracy of 84.7%. These findings establish that localist language models provide a practical framework for applications in regulated domains requiring both transparency and capability, offering precise mathematical control over the interpretability-performance spectrum through explicit penalty thresholds and information-theoretic design principles.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Imitation Learning in the Deep Learning Era: A Novel Taxonomy and Recent Advances",
      "link": "https://arxiv.org/abs/2511.03565",
      "description": "arXiv:2511.03565v1 Announce Type: cross \nAbstract: Imitation learning (IL) enables agents to acquire skills by observing and replicating the behavior of one or multiple experts. In recent years, advances in deep learning have significantly expanded the capabilities and scalability of imitation learning across a range of domains, where expert data can range from full state-action trajectories to partial observations or unlabeled sequences. Alongside this growth, novel approaches have emerged, with new methodologies being developed to address longstanding challenges such as generalization, covariate shift, and demonstration quality. In this survey, we review the latest advances in imitation learning research, highlighting recent trends, methodological innovations, and practical applications. We propose a novel taxonomy that is distinct from existing categorizations to better reflect the current state of the IL research stratum and its trends. Throughout the survey, we critically examine the strengths, limitations, and evaluation practices of representative works, and we outline key challenges and open directions for future research.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Multi-User Personalisation in Human-Robot Interaction: Using Quantitative Bipolar Argumentation Frameworks for Preferences Conflict Resolution",
      "link": "https://arxiv.org/abs/2511.03576",
      "description": "arXiv:2511.03576v1 Announce Type: cross \nAbstract: While personalisation in Human-Robot Interaction (HRI) has advanced significantly, most existing approaches focus on single-user adaptation, overlooking scenarios involving multiple stakeholders with potentially conflicting preferences. To address this, we propose the Multi-User Preferences Quantitative Bipolar Argumentation Framework (MUP-QBAF), a novel multi-user personalisation framework based on Quantitative Bipolar Argumentation Frameworks (QBAFs) that explicitly models and resolves multi-user preference conflicts. Unlike prior work in Argumentation Frameworks, which typically assumes static inputs, our approach is tailored to robotics: it incorporates both users' arguments and the robot's dynamic observations of the environment, allowing the system to adapt over time and respond to changing contexts. Preferences, both positive and negative, are represented as arguments whose strength is recalculated iteratively based on new information. The framework's properties and capabilities are presented and validated through a realistic case study, where an assistive robot mediates between the conflicting preferences of a caregiver and a care recipient during a frailty assessment task. This evaluation further includes a sensitivity analysis of argument base scores, demonstrating how preference outcomes can be shaped by user input and contextual observations. By offering a transparent, structured, and context-sensitive approach to resolving competing user preferences, this work advances the field of multi-user HRI. It provides a principled alternative to data-driven methods, enabling robots to navigate conflicts in real-world environments.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Learning Under Laws: A Constraint-Projected Neural PDE Solver that Eliminates Hallucinations",
      "link": "https://arxiv.org/abs/2511.03578",
      "description": "arXiv:2511.03578v1 Announce Type: cross \nAbstract: Neural networks can approximate solutions to partial differential equations, but they often break the very laws they are meant to model-creating mass from nowhere, drifting shocks, or violating conservation and entropy. We address this by training within the laws of physics rather than beside them. Our framework, called Constraint-Projected Learning (CPL), keeps every update physically admissible by projecting network outputs onto the intersection of constraint sets defined by conservation, Rankine-Hugoniot balance, entropy, and positivity. The projection is differentiable and adds only about 10% computational overhead, making it fully compatible with back-propagation. We further stabilize training with total-variation damping (TVD) to suppress small oscillations and a rollout curriculum that enforces consistency over long prediction horizons. Together, these mechanisms eliminate both hard and soft violations: conservation holds at machine precision, total-variation growth vanishes, and entropy and error remain bounded. On Burgers and Euler systems, CPL produces stable, physically lawful solutions without loss of accuracy. Instead of hoping neural solvers will respect physics, CPL makes that behavior an intrinsic property of the learning process.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "PerfDojo: Automated ML Library Generation for Heterogeneous Architectures",
      "link": "https://arxiv.org/abs/2511.03586",
      "description": "arXiv:2511.03586v1 Announce Type: cross \nAbstract: The increasing complexity of machine learning models and the proliferation of diverse hardware architectures (CPUs, GPUs, accelerators) make achieving optimal performance a significant challenge. Heterogeneity in instruction sets, specialized kernel requirements for different data types and model features (e.g., sparsity, quantization), and architecture-specific optimizations complicate performance tuning. Manual optimization is resource-intensive, while existing automatic approaches often rely on complex hardware-specific heuristics and uninterpretable intermediate representations, hindering performance portability. We introduce PerfLLM, a novel automatic optimization methodology leveraging Large Language Models (LLMs) and Reinforcement Learning (RL). Central to this is PerfDojo, an environment framing optimization as an RL game using a human-readable, mathematically-inspired code representation that guarantees semantic validity through transformations. This allows effective optimization without prior hardware knowledge, facilitating both human analysis and RL agent training. We demonstrate PerfLLM's ability to achieve significant performance gains across diverse CPU (x86, Arm, RISC-V) and GPU architectures.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Step-Audio-EditX Technical Report",
      "link": "https://arxiv.org/abs/2511.03601",
      "description": "arXiv:2511.03601v1 Announce Type: cross \nAbstract: We present Step-Audio-EditX, the first open-source LLM-based audio model excelling at expressive and iterative audio editing encompassing emotion, speaking style, and paralinguistics alongside robust zero-shot text-to-speech (TTS) capabilities.Our core innovation lies in leveraging only large-margin synthetic data, which circumvents the need for embedding-based priors or auxiliary modules. This large-margin learning approach enables both iterative control and high expressivity across voices, and represents a fundamental pivot from the conventional focus on representation-level disentanglement. Evaluation results demonstrate that Step-Audio-EditX surpasses both MiniMax-2.6-hd and Doubao-Seed-TTS-2.0 in emotion editing and other fine-grained control tasks.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Visualization Biases MLLM's Decision Making in Network Data Tasks",
      "link": "https://arxiv.org/abs/2511.03617",
      "description": "arXiv:2511.03617v1 Announce Type: cross \nAbstract: We evaluate how visualizations can influence the judgment of MLLMs about the presence or absence of bridges in a network. We show that the inclusion of visualization improves confidence over a structured text-based input that could theoretically be helpful for answering the question. On the other hand, we observe that standard visualization techniques create a strong bias towards accepting or refuting the presence of a bridge -- independently of whether or not a bridge actually exists in the network. While our results indicate that the inclusion of visualization techniques can effectively influence the MLLM's judgment without compromising its self-reported confidence, they also imply that practitioners must be careful of allowing users to include visualizations in generative AI applications so as to avoid undesired hallucinations.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "LiveTradeBench: Seeking Real-World Alpha with Large Language Models",
      "link": "https://arxiv.org/abs/2511.03628",
      "description": "arXiv:2511.03628v1 Announce Type: cross \nAbstract: Large language models (LLMs) achieve strong performance across benchmarks--from knowledge quizzes and math reasoning to web-agent tasks--but these tests occur in static settings, lacking real dynamics and uncertainty. Consequently, they evaluate isolated reasoning or problem-solving rather than decision-making under uncertainty. To address this, we introduce LiveTradeBench, a live trading environment for evaluating LLM agents in realistic and evolving markets. LiveTradeBench follows three design principles: (i) Live data streaming of market prices and news, eliminating dependence on offline backtesting and preventing information leakage while capturing real-time uncertainty; (ii) a portfolio-management abstraction that extends control from single-asset actions to multi-asset allocation, integrating risk management and cross-asset reasoning; and (iii) multi-market evaluation across structurally distinct environments--U.S. stocks and Polymarket prediction markets--differing in volatility, liquidity, and information flow. At each step, an agent observes prices, news, and its portfolio, then outputs percentage allocations that balance risk and return. Using LiveTradeBench, we run 50-day live evaluations of 21 LLMs across families. Results show that (1) high LMArena scores do not imply superior trading outcomes; (2) models display distinct portfolio styles reflecting risk appetite and reasoning dynamics; and (3) some LLMs effectively leverage live signals to adapt decisions. These findings expose a gap between static evaluation and real-world competence, motivating benchmarks that test sequential decision making and consistency under live uncertainty.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Watermarking Large Language Models in Europe: Interpreting the AI Act in Light of Technology",
      "link": "https://arxiv.org/abs/2511.03641",
      "description": "arXiv:2511.03641v1 Announce Type: cross \nAbstract: To foster trustworthy Artificial Intelligence (AI) within the European Union, the AI Act requires providers to mark and detect the outputs of their general-purpose models. The Article 50 and Recital 133 call for marking methods that are ''sufficiently reliable, interoperable, effective and robust''. Yet, the rapidly evolving and heterogeneous landscape of watermarks for Large Language Models (LLMs) makes it difficult to determine how these four standards can be translated into concrete and measurable evaluations. Our paper addresses this challenge, anchoring the normativity of European requirements in the multiplicity of watermarking techniques. Introducing clear and distinct concepts on LLM watermarking, our contribution is threefold. (1) Watermarking Categorisation: We propose an accessible taxonomy of watermarking methods according to the stage of the LLM lifecycle at which they are applied - before, during, or after training, and during next-token distribution or sampling. (2) Watermarking Evaluation: We interpret the EU AI Act's requirements by mapping each criterion with state-of-the-art evaluations on robustness and detectability of the watermark, and of quality of the LLM. Since interoperability remains largely untheorised in LLM watermarking research, we propose three normative dimensions to frame its assessment. (3) Watermarking Comparison: We compare current watermarking methods for LLMs against the operationalised European criteria and show that no approach yet satisfies all four standards. Encouraged by emerging empirical tests, we recommend further research into watermarking directly embedded within the low-level architecture of LLMs.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Explaining Human Choice Probabilities with Simple Vector Representations",
      "link": "https://arxiv.org/abs/2511.03643",
      "description": "arXiv:2511.03643v1 Announce Type: cross \nAbstract: When people pursue rewards in stochastic environments, they often match their choice frequencies to the observed target frequencies, even when this policy is demonstrably sub-optimal. We used a ``hide and seek'' task to evaluate this behavior under conditions where pursuit (seeking) could be toggled to avoidance (hiding), while leaving the probability distribution fixed, or varying complexity by changing the number of possible choices. We developed a model for participant choice built from choice frequency histograms treated as vectors. We posited the existence of a probability antimatching strategy for avoidance (hiding) rounds, and formalized this as a vector reflection of probability matching. We found that only two basis policies: matching/antimatching and maximizing/minimizing were sufficient to account for participant choices across a range of room numbers and opponent probability distributions. This schema requires only that people have the ability to remember the relative frequency of the different outcomes. With this knowledge simple operations can construct the maximizing and minimizing policies as well as matching and antimatching strategies. A mixture of these two policies captures human choice patterns in a stochastic environment.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "ChiMDQA: Towards Comprehensive Chinese Document QA with Fine-grained Evaluation",
      "link": "https://arxiv.org/abs/2511.03656",
      "description": "arXiv:2511.03656v1 Announce Type: cross \nAbstract: With the rapid advancement of natural language processing (NLP) technologies, the demand for high-quality Chinese document question-answering datasets is steadily growing. To address this issue, we present the Chinese Multi-Document Question Answering Dataset(ChiMDQA), specifically designed for downstream business scenarios across prevalent domains including academic, education, finance, law, medical treatment, and news. ChiMDQA encompasses long-form documents from six distinct fields, consisting of 6,068 rigorously curated, high-quality question-answer (QA) pairs further classified into ten fine-grained categories. Through meticulous document screening and a systematic question-design methodology, the dataset guarantees both diversity and high quality, rendering it applicable to various NLP tasks such as document comprehension, knowledge extraction, and intelligent QA systems. Additionally, this paper offers a comprehensive overview of the dataset's design objectives, construction methodologies, and fine-grained evaluation system, supplying a substantial foundation for future research and practical applications in Chinese QA. The code and data are available at: https://anonymous.4open.science/r/Foxit-CHiMDQA/.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "DQN Performance with Epsilon Greedy Policies and Prioritized Experience Replay",
      "link": "https://arxiv.org/abs/2511.03670",
      "description": "arXiv:2511.03670v1 Announce Type: cross \nAbstract: We present a detailed study of Deep Q-Networks in finite environments, emphasizing the impact of epsilon-greedy exploration schedules and prioritized experience replay. Through systematic experimentation, we evaluate how variations in epsilon decay schedules affect learning efficiency, convergence behavior, and reward optimization. We investigate how prioritized experience replay leads to faster convergence and higher returns and show empirical results comparing uniform, no replay, and prioritized strategies across multiple simulations. Our findings illuminate the trade-offs and interactions between exploration strategies and memory management in DQN training, offering practical recommendations for robust reinforcement learning in resource-constrained settings.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Whisper Leak: a side-channel attack on Large Language Models",
      "link": "https://arxiv.org/abs/2511.03675",
      "description": "arXiv:2511.03675v1 Announce Type: cross \nAbstract: Large Language Models (LLMs) are increasingly deployed in sensitive domains including healthcare, legal services, and confidential communications, where privacy is paramount. This paper introduces Whisper Leak, a side-channel attack that infers user prompt topics from encrypted LLM traffic by analyzing packet size and timing patterns in streaming responses. Despite TLS encryption protecting content, these metadata patterns leak sufficient information to enable topic classification. We demonstrate the attack across 28 popular LLMs from major providers, achieving near-perfect classification (often >98% AUPRC) and high precision even at extreme class imbalance (10,000:1 noise-to-target ratio). For many models, we achieve 100% precision in identifying sensitive topics like \"money laundering\" while recovering 5-20% of target conversations. This industry-wide vulnerability poses significant risks for users under network surveillance by ISPs, governments, or local adversaries. We evaluate three mitigation strategies - random padding, token batching, and packet injection - finding that while each reduces attack effectiveness, none provides complete protection. Through responsible disclosure, we have collaborated with providers to implement initial countermeasures. Our findings underscore the need for LLM providers to address metadata leakage as AI systems handle increasingly sensitive information.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Structured Matrix Scaling for Multi-Class Calibration",
      "link": "https://arxiv.org/abs/2511.03685",
      "description": "arXiv:2511.03685v1 Announce Type: cross \nAbstract: Post-hoc recalibration methods are widely used to ensure that classifiers provide faithful probability estimates. We argue that parametric recalibration functions based on logistic regression can be motivated from a simple theoretical setting for both binary and multiclass classification. This insight motivates the use of more expressive calibration methods beyond standard temperature scaling. For multi-class calibration however, a key challenge lies in the increasing number of parameters introduced by more complex models, often coupled with limited calibration data, which can lead to overfitting. Through extensive experiments, we demonstrate that the resulting bias-variance tradeoff can be effectively managed by structured regularization, robust preprocessing and efficient optimization. The resulting methods lead to substantial gains over existing logistic-based calibration techniques. We provide efficient and easy-to-use open-source implementations of our methods, making them an attractive alternative to common temperature, vector, and matrix scaling implementations.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "The OpenHands Software Agent SDK: A Composable and Extensible Foundation for Production Agents",
      "link": "https://arxiv.org/abs/2511.03690",
      "description": "arXiv:2511.03690v1 Announce Type: cross \nAbstract: Agents are now used widely in the process of software development, but building production-ready software engineering agents is a complex task. Deploying software agents effectively requires flexibility in implementation and experimentation, reliable and secure execution, and interfaces for users to interact with agents. In this paper, we present the OpenHands Software Agent SDK, a toolkit for implementing software development agents that satisfy these desiderata. This toolkit is a complete architectural redesign of the agent components of the popular OpenHands framework for software development agents, which has 64k+ GitHub stars. To achieve flexibility, we design a simple interface for implementing agents that requires only a few lines of code in the default case, but is easily extensible to more complex, full-featured agents with features such as custom tools, memory management, and more. For security and reliability, it delivers seamless local-to-remote execution portability, integrated REST/WebSocket services. For interaction with human users, it can connect directly to a variety of interfaces, such as visual workspaces (VS Code, VNC, browser), command-line interfaces, and APIs. Compared with existing SDKs from OpenAI, Claude, and Google, OpenHands uniquely integrates native sandboxed execution, lifecycle control, model-agnostic multi-LLM routing, and built-in security analysis. Empirical results on SWE-Bench Verified and GAIA benchmarks demonstrate strong performance. Put together, these elements allow the OpenHands Software Agent SDK to provide a practical foundation for prototyping, unlocking new classes of custom applications, and reliably deploying agents at scale.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "AnaFlow: Agentic LLM-based Workflow for Reasoning-Driven Explainable and Sample-Efficient Analog Circuit Sizing",
      "link": "https://arxiv.org/abs/2511.03697",
      "description": "arXiv:2511.03697v1 Announce Type: cross \nAbstract: Analog/mixed-signal circuits are key for interfacing electronics with the physical world. Their design, however, remains a largely handcrafted process, resulting in long and error-prone design cycles. While the recent rise of AI-based reinforcement learning and generative AI has created new techniques to automate this task, the need for many time-consuming simulations is a critical bottleneck hindering the overall efficiency. Furthermore, the lack of explainability of the resulting design solutions hampers widespread adoption of the tools. To address these issues, a novel agentic AI framework for sample-efficient and explainable analog circuit sizing is presented. It employs a multi-agent workflow where specialized Large Language Model (LLM)-based agents collaborate to interpret the circuit topology, to understand the design goals, and to iteratively refine the circuit's design parameters towards the target goals with human-interpretable reasoning. The adaptive simulation strategy creates an intelligent control that yields a high sample efficiency. The AnaFlow framework is demonstrated for two circuits of varying complexity and is able to complete the sizing task fully automatically, differently from pure Bayesian optimization and reinforcement learning approaches. The system learns from its optimization history to avoid past mistakes and to accelerate convergence. The inherent explainability makes this a powerful tool for analog design space exploration and a new paradigm in analog EDA, where AI agents serve as transparent design assistants.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Grounded Misunderstandings in Asymmetric Dialogue: A Perspectivist Annotation Scheme for MapTask",
      "link": "https://arxiv.org/abs/2511.03718",
      "description": "arXiv:2511.03718v1 Announce Type: cross \nAbstract: Collaborative dialogue relies on participants incrementally establishing common ground, yet in asymmetric settings they may believe they agree while referring to different entities. We introduce a perspectivist annotation scheme for the HCRC MapTask corpus (Anderson et al., 1991) that separately captures speaker and addressee grounded interpretations for each reference expression, enabling us to trace how understanding emerges, diverges, and repairs over time. Using a scheme-constrained LLM annotation pipeline, we obtain 13k annotated reference expressions with reliability estimates and analyze the resulting understanding states. The results show that full misunderstandings are rare once lexical variants are unified, but multiplicity discrepancies systematically induce divergences, revealing how apparent grounding can mask referential misalignment. Our framework provides both a resource and an analytic lens for studying grounded misunderstanding and for evaluating (V)LLMs' capacity to model perspective-dependent grounding in collaborative dialogue.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Beyond Single Pass, Looping Through Time: KG-IRAG with Iterative Knowledge Retrieval",
      "link": "https://arxiv.org/abs/2503.14234",
      "description": "arXiv:2503.14234v4 Announce Type: replace \nAbstract: Graph Retrieval-Augmented Generation (GraphRAG) has proven highly effective in enhancing the performance of Large Language Models (LLMs) on tasks that require external knowledge. By leveraging Knowledge Graphs (KGs), GraphRAG improves information retrieval for complex reasoning tasks, providing more precise and comprehensive retrieval and generating more accurate responses to QAs. However, most RAG methods fall short in addressing multi-step reasoning, particularly when both information extraction and inference are necessary. To address this limitation, this paper presents Knowledge Graph-Based Iterative Retrieval-Augmented Generation (KG-IRAG), a novel framework that integrates KGs with iterative reasoning to improve LLMs' ability to handle queries involving temporal and logical dependencies. Through iterative retrieval steps, KG-IRAG incrementally gathers relevant data from external KGs, enabling step-by-step reasoning. The proposed approach is particularly suited for scenarios where reasoning is required alongside dynamic temporal data extraction, such as determining optimal travel times based on weather conditions or traffic patterns. Experimental results show that KG-IRAG improves accuracy in complex reasoning tasks by effectively integrating external knowledge with iterative, logic-based retrieval. Additionally, three new datasets: weatherQA-Irish, weatherQA-Sydney, and trafficQA-TFNSW, are formed to evaluate KG-IRAG's performance, demonstrating its potential beyond traditional RAG applications.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "TAMO: Fine-Grained Root Cause Analysis via Tool-Assisted LLM Agent with Multi-Modality Observation Data in Cloud-Native Systems",
      "link": "https://arxiv.org/abs/2504.20462",
      "description": "arXiv:2504.20462v5 Announce Type: replace \nAbstract: Implementing large language models (LLMs)-driven root cause analysis (RCA) in cloud-native systems has become a key topic of modern software operations and maintenance. However, existing LLM-based approaches face three key challenges: multi-modality input constraint, context window limitation, and dynamic dependence graph. To address these issues, we propose a tool-assisted LLM agent with multi-modality observation data for fine-grained RCA, namely TAMO, including multimodality alignment tool, root cause localization tool, and fault types classification tool. In detail, TAMO unifies multi-modal observation data into time-aligned representations for cross-modal feature consistency. Based on the unified representations, TAMO then invokes its specialized root cause localization tool and fault types classification tool for further identifying root cause and fault type underlying system context. This approach overcomes the limitations of LLMs in processing real-time raw observational data and dynamic service dependencies, guiding the model to generate repair strategies that align with system context through structured prompt design. Experiments on two benchmark datasets demonstrate that TAMO outperforms state-of-the-art (SOTA) approaches with comparable performance.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Leveraging LLMs to Automate Energy-Aware Refactoring of Parallel Scientific Codes",
      "link": "https://arxiv.org/abs/2505.02184",
      "description": "arXiv:2505.02184v2 Announce Type: replace \nAbstract: While large language models (LLMs) are increasingly used for generating parallel scientific codes, most efforts emphasize functional correctness, often overlooking performance, especially energy efficiency. We propose LASSI-EE, an automated LLM-based refactoring framework that generates energy-efficient parallel codes through a multi-stage, iterative approach integrating runtime power profiling, energy-aware prompting, self-correcting feedback loops, and an LLM-as-a-Judge agent for automated screening of code solutions. We introduce energy-reduction@k, a novel metric that quantifies expected energy reduction when generating k code candidates and selecting the most energy-efficient, enabling systematic evaluation of multi-attempt generation strategies. Evaluating 20 HeCBench applications and two miniApps on NVIDIA A100 and AMD MI100 GPUs, a single run (k=1) with LASSI-EE delivers refactored parallel codes with an average 29% expected energy reduction at an 81% pass rate, representing a 2.8x improvement over vanilla LLM prompting. Multiple runs (k=3) achieve an average 48% expected energy reduction at a 97% pass rate. These results are consistent across devices, demonstrating LASSI-EE's effectiveness across diverse hardware architectures.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Meta-Semantics Augmented Few-Shot Relational Learning",
      "link": "https://arxiv.org/abs/2505.05684",
      "description": "arXiv:2505.05684v4 Announce Type: replace \nAbstract: Few-shot relational learning on knowledge graph (KGs) aims to perform reasoning over relations with only a few training examples. While current methods have focused primarily on leveraging specific relational information, rich semantics inherent in KGs have been largely overlooked. To bridge this gap, we propose PromptMeta, a novel prompted meta-learning framework that seamlessly integrates meta-semantics with relational information for few-shot relational learning. PromptMeta introduces two core innovations: (1) a Meta-Semantic Prompt (MSP) pool that learns and consolidates high-level meta-semantics shared across tasks, enabling effective knowledge transfer and adaptation to newly emerging relations; and (2) a learnable fusion mechanism that dynamically combines meta-semantics with task-specific relational information tailored to different few-shot tasks. Both components are optimized jointly with model parameters within a meta-learning framework. Extensive experiments and analyses on two real-world KG benchmarks validate the effectiveness of PromptMeta in adapting to new relations with limited supervision.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Divide by Question, Conquer by Agent: SPLIT-RAG with Question-Driven Graph Partitioning",
      "link": "https://arxiv.org/abs/2505.13994",
      "description": "arXiv:2505.13994v2 Announce Type: replace \nAbstract: Retrieval-Augmented Generation (RAG) systems empower large language models (LLMs) with external knowledge, yet struggle with efficiency-accuracy trade-offs when scaling to large knowledge graphs. Existing approaches often rely on monolithic graph retrieval, incurring unnecessary latency for simple queries and fragmented reasoning for complex multi-hop questions. To address these challenges, this paper propose SPLIT-RAG, a multi-agent RAG framework that addresses these limitations with question-driven semantic graph partitioning and collaborative subgraph retrieval. The innovative framework first create Semantic Partitioning of Linked Information, then use the Type-Specialized knowledge base to achieve Multi-Agent RAG. The attribute-aware graph segmentation manages to divide knowledge graphs into semantically coherent subgraphs, ensuring subgraphs align with different query types, while lightweight LLM agents are assigned to partitioned subgraphs, and only relevant partitions are activated during retrieval, thus reduce search space while enhancing efficiency. Finally, a hierarchical merging module resolves inconsistencies across subgraph-derived answers through logical verifications. Extensive experimental validation demonstrates considerable improvements compared to existing approaches.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "s3: You Don't Need That Much Data to Train a Search Agent via RL",
      "link": "https://arxiv.org/abs/2505.14146",
      "description": "arXiv:2505.14146v2 Announce Type: replace \nAbstract: Retrieval-augmented generation (RAG) systems empower large language models (LLMs) to access external knowledge during inference. Recent advances have enabled LLMs to act as search agents via reinforcement learning (RL), improving information acquisition through multi-turn interactions with retrieval engines. However, existing approaches either optimize retrieval using search-only metrics (e.g., NDCG) that ignore downstream utility or fine-tune the entire LLM to jointly reason and retrieve-entangling retrieval with generation and limiting the real search utility and compatibility with frozen or proprietary models. In this work, we propose s3, a lightweight, model-agnostic framework that decouples the searcher from the generator and trains the searcher using a Gain Beyond RAG reward: the improvement in generation accuracy over naive RAG. s3 requires only 2.4k training samples to outperform baselines trained on over 70x more data, consistently delivering stronger downstream performance across six general QA and five medical QA benchmarks.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Multi-Agent Reinforcement Learning for Autonomous Multi-Satellite Earth Observation: A Realistic Case Study",
      "link": "https://arxiv.org/abs/2506.15207",
      "description": "arXiv:2506.15207v2 Announce Type: replace \nAbstract: The exponential growth of Low Earth Orbit (LEO) satellites has revolutionised Earth Observation (EO) missions, addressing challenges in climate monitoring, disaster management, and more. However, autonomous coordination in multi-satellite systems remains a fundamental challenge. Traditional optimisation approaches struggle to handle the real-time decision-making demands of dynamic EO missions, necessitating the use of Reinforcement Learning (RL) and Multi-Agent Reinforcement Learning (MARL). In this paper, we investigate RL-based autonomous EO mission planning by modelling single-satellite operations and extending to multi-satellite constellations using MARL frameworks. We address key challenges, including energy and data storage limitations, uncertainties in satellite observations, and the complexities of decentralised coordination under partial observability. By leveraging a near-realistic satellite simulation environment, we evaluate the training stability and performance of state-of-the-art MARL algorithms, including PPO, IPPO, MAPPO, and HAPPO. Our results demonstrate that MARL can effectively balance imaging and resource management while addressing non-stationarity and reward interdependency in multi-satellite coordination. The insights gained from this study provide a foundation for autonomous satellite operations, offering practical guidelines for improving policy learning in decentralised EO missions.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Why Isn't Relational Learning Taking Over the World?",
      "link": "https://arxiv.org/abs/2507.13558",
      "description": "arXiv:2507.13558v5 Announce Type: replace \nAbstract: Artificial intelligence seems to be taking over the world with systems that model pixels, words, and phonemes. The world is arguably made up, not of pixels, words, and phonemes but of entities (objects, things, including events) with properties and relations among them. Surely we should model these, not the perception or description of them. You might suspect that concentrating on modeling words and pixels is because all of the (valuable) data in the world is in terms of text and images. If you look into almost any company you will find their most valuable data is in spreadsheets, databases and other relational formats. These are not the form that are studied in introductory machine learning, but are full of product numbers, student numbers, transaction numbers and other identifiers that can't be interpreted naively as numbers. The field that studies this sort of data has various names including relational learning, statistical relational AI, and many others. This paper explains why relational learning is not taking over the world -- except in a few cases with restricted relations -- and what needs to be done to bring it to it's rightful prominence.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "LLM-Driven Collaborative Model for Untangling Commits via Explicit and Implicit Dependency Reasoning",
      "link": "https://arxiv.org/abs/2507.16395",
      "description": "arXiv:2507.16395v2 Announce Type: replace \nAbstract: Atomic commits, which address a single development concern, are a best practice in software development. In practice, however, developers often produce tangled commits that mix unrelated changes, complicating code review and maintenance. Prior untangling approaches (rule-based, feature-based, or graph-based) have made progress but typically rely on shallow signals and struggle to distinguish explicit dependencies (e.g., control/data flow) from implicit ones (e.g., semantic or conceptual relationships). In this paper, we propose ColaUntangle, a new collaborative consultation framework for commit untangling that models both explicit and implicit dependencies among code changes. ColaUntangle integrates Large Language Model (LLM)-driven agents in a multi-agent architecture: one agent specializes in explicit dependencies, another in implicit ones, and a reviewer agent synthesizes their perspectives through iterative consultation. To capture structural and contextual information, we construct Explicit and Implicit Contexts, enabling agents to reason over code relationships with both symbolic and semantic depth. We evaluate ColaUntangle on two widely-used datasets (1,612 C# and 14k Java tangled commits). Experimental results show that ColaUntangle outperforms the best-performing baseline, achieving an improvement of 44% on the C# dataset and 82% on the Java dataset. These findings highlight the potential of LLM-based collaborative frameworks for advancing automated commit untangling tasks.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Data Dependency-Aware Code Generation from Enhanced UML Sequence Diagrams",
      "link": "https://arxiv.org/abs/2508.03379",
      "description": "arXiv:2508.03379v3 Announce Type: replace \nAbstract: Large language models (LLMs) excel at generating code from natural language (NL) descriptions. However, the plain textual descriptions are inherently ambiguous and often fail to capture complex requirements like intricate system behaviors, conditional logic, and architectural constraints; implicit data dependencies in service-oriented architectures are difficult to infer and handle correctly. To bridge this gap, we propose a novel step-by-step code generation framework named UML2Dep by leveraging unambiguous formal specifications of complex requirements. First, we introduce an enhanced Unified Modeling Language (UML) sequence diagram tailored for service-oriented architectures. This diagram extends traditional visual syntax by integrating decision tables and API specifications, explicitly formalizing structural relationships and business logic flows in service interactions to rigorously eliminate linguistic ambiguity. Second, recognizing the critical role of data flow, we introduce a dedicated data dependency inference (DDI) task. DDI systematically constructs an explicit data dependency graph prior to actual code synthesis. To ensure reliability, we formalize DDI as a constrained mathematical reasoning task through novel prompting strategies, aligning with LLMs' excellent mathematical strengths. Additional static parsing and dependency pruning further reduce context complexity and cognitive load associated with intricate specifications, thereby enhancing reasoning accuracy and efficiency.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Reinforcement Learning Foundations for Deep Research Systems: A Survey",
      "link": "https://arxiv.org/abs/2509.06733",
      "description": "arXiv:2509.06733v2 Announce Type: replace \nAbstract: Deep research systems, agentic AI that solve complex, multi-step tasks by coordinating reasoning, search across the open web and user files, and tool use, are moving toward hierarchical deployments with a Planner, Coordinator, and Executors. In practice, training entire stacks end-to-end remains impractical, so most work trains a single planner connected to core tools such as search, browsing, and code. While SFT imparts protocol fidelity, it suffers from imitation and exposure biases and underuses environment feedback. Preference alignment methods such as DPO are schema and proxy-dependent, off-policy, and weak for long-horizon credit assignment and multi-objective trade-offs. A further limitation of SFT and DPO is their reliance on human defined decision points and subskills through schema design and labeled comparisons. Reinforcement learning aligns with closed-loop, tool-interaction research by optimizing trajectory-level policies, enabling exploration, recovery behaviors, and principled credit assignment, and it reduces dependence on such human priors and rater biases.\n  This survey is, to our knowledge, the first dedicated to the RL foundations of deep research systems. It systematizes recent work along three axes: (i) data synthesis and curation; (ii) RL methods for agentic research covering stability, sample efficiency, long context handling, reward and credit design, multi-objective optimization, and multimodal integration; and (iii) agentic RL training systems and frameworks. We also cover agent architecture and coordination, as well as evaluation and benchmarks, including recent QA, VQA, long-form synthesis, and domain-grounded, tool-interaction tasks. We distill recurring patterns, surface infrastructure bottlenecks, and offer practical guidance for training robust, transparent deep research agents with RL.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "ForTIFAI: Fending Off Recursive Training Induced Failure for AI Model Collapse",
      "link": "https://arxiv.org/abs/2509.08972",
      "description": "arXiv:2509.08972v4 Announce Type: replace \nAbstract: The increasing reliance on generative AI models is rapidly increasing the volume of synthetic data, with some projections suggesting that most available new data for training could be machine-generated by 2030. This shift to a mainly synthetic content presents a critical challenge: repeated training in synthetic data leads to a phenomenon known as model collapse, where model performance degrades over generations of training, eventually rendering the models ineffective. While the causes of model collapse are increasingly understood, effective mitigation strategies remain scarce. We address this challenge by leveraging a key insight: auto-regressive models tend to generate text sequences to which they assign high confidence (i.e., high log-likelihood). Based on this observation, we introduce the Truncated-Cross-Entropy (TCE) loss function. TCE mitigates collapse by selectively ignoring high-confidence tokens during training, effectively filtering out likely machine-generated artifacts from the learning process. Our experiments demonstrate that models trained with TCE not only learn effectively but also exhibit significantly increased resilience, tolerating over 2.3x more synthetic data before the onset of collapse. In addition, we provide an open-source benchmark for collapse dynamics in mixed-data settings. Our results demonstrate that confidence-aware training objectives can substantially delay collapse onset, offering a practical and generalizable tool for model robustness under synthetic-data exposure.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "VoiceAgentBench: Are Voice Assistants ready for agentic tasks?",
      "link": "https://arxiv.org/abs/2510.07978",
      "description": "arXiv:2510.07978v2 Announce Type: replace \nAbstract: Large-scale Speech Language Models (SpeechLMs) have enabled voice assistants capable of understanding natural spoken queries and performing complex tasks. However, existing speech benchmarks primarily focus on isolated capabilities such as transcription, or question-answering, and do not systematically evaluate agentic scenarios encompassing multilingual and cultural understanding, as well as adversarial robustness. To address this, we introduce VoiceAgentBench, a comprehensive benchmark designed to evaluate SpeechLMs in realistic spoken agentic settings. It comprises over 5,500 synthetic spoken queries, including dialogues grounded in Indian context, covering single-tool invocations, multi-tool workflows, multi-turn interactions, and safety evaluations. The benchmark supports English, Hindi, and 5 other Indian languages, reflecting real-world linguistic and cultural diversity. We simulate speaker variability using a novel sampling algorithm that selects audios for TTS voice conversion based on its speaker embeddings, maximizing acoustic and speaker diversity. Our evaluation measures tool selection accuracy, structural consistency, and the correctness of tool invocations, including adversarial robustness. Our experiments reveal significant gaps in contextual tool orchestration tasks, Indic generalization, and adversarial robustness, exposing critical limitations of current SpeechLMs.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "PlanU: Large Language Model Reasoning through Planning under Uncertainty",
      "link": "https://arxiv.org/abs/2510.18442",
      "description": "arXiv:2510.18442v2 Announce Type: replace \nAbstract: Large Language Models (LLMs) are increasingly being explored across a range of reasoning tasks. However, LLMs sometimes struggle with reasoning tasks under uncertainty that are relatively easy for humans, such as planning actions in stochastic environments. The adoption of LLMs for reasoning is impeded by uncertainty challenges, such as LLM uncertainty and environmental uncertainty. LLM uncertainty arises from the stochastic sampling process inherent to LLMs. Most LLM-based Decision-Making (LDM) approaches address LLM uncertainty through multiple reasoning chains or search trees. However, these approaches overlook environmental uncertainty, which leads to poor performance in environments with stochastic state transitions. Some recent LDM approaches deal with uncertainty by forecasting the probability of unknown variables. However, they are not designed for multi-step reasoning tasks that require interaction with the environment. To address uncertainty in LLM decision-making, we introduce PlanU, an LLM-based planning method that captures uncertainty within Monte Carlo Tree Search (MCTS). PlanU models the return of each node in the MCTS as a quantile distribution, which uses a set of quantiles to represent the return distribution. To balance exploration and exploitation during tree search, PlanU introduces an Upper Confidence Bounds with Curiosity (UCC) score which estimates the uncertainty of MCTS nodes. Through extensive experiments, we demonstrate the effectiveness of PlanU in LLM-based reasoning tasks under uncertainty.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Misalignment Bounty: Crowdsourcing AI Agent Misbehavior",
      "link": "https://arxiv.org/abs/2510.19738",
      "description": "arXiv:2510.19738v2 Announce Type: replace \nAbstract: Advanced AI systems sometimes act in ways that differ from human intent. To gather clear, reproducible examples, we ran the Misalignment Bounty: a crowdsourced project that collected cases of agents pursuing unintended or unsafe goals. The bounty received 295 submissions, of which nine were awarded.\n  This report explains the program's motivation and evaluation criteria, and walks through the nine winning submissions step by step.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Agentic Meta-Orchestrator for Multi-task Copilots",
      "link": "https://arxiv.org/abs/2510.22781",
      "description": "arXiv:2510.22781v2 Announce Type: replace \nAbstract: Microsoft Copilot suites serve as the universal entry point for various agents skilled in handling important tasks, ranging from assisting a customer with product purchases to detecting vulnerabilities in corporate programming code. Each agent can be powered by language models, software engineering operations, such as database retrieval, and internal \\& external knowledge. The repertoire of a copilot can expand dynamically with new agents. This requires a robust orchestrator that can distribute tasks from user prompts to the right agents. In this work, we propose an Agentic Meta-orchestrator (AMO) for handling multiple tasks and scalable agents in copilot services, which can provide both natural language and action responses. We will also demonstrate the planning that leverages meta-learning, i.e., a trained decision tree model for deciding the best inference strategy among various agents/models. We showcase the effectiveness of our AMO through two production use cases: Microsoft 365 (M365) E-Commerce Copilot and code compliance copilot. M365 E-Commerce Copilot advertises Microsoft products to external customers to promote sales success. The M365 E-Commerce Copilot provides up-to-date product information and connects to multiple agents, such as relational databases and human customer support. The code compliance copilot scans the internal DevOps code to detect known and new compliance issues in pull requests (PR).",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Mirror-Neuron Patterns in AI Alignment",
      "link": "https://arxiv.org/abs/2511.01885",
      "description": "arXiv:2511.01885v2 Announce Type: replace \nAbstract: As artificial intelligence (AI) advances toward superhuman capabilities, aligning these systems with human values becomes increasingly critical. Current alignment strategies rely largely on externally specified constraints that may prove insufficient against future super-intelligent AI capable of circumventing top-down controls.\n  This research investigates whether artificial neural networks (ANNs) can develop patterns analogous to biological mirror neurons cells that activate both when performing and observing actions, and how such patterns might contribute to intrinsic alignment in AI. Mirror neurons play a crucial role in empathy, imitation, and social cognition in humans. The study therefore asks: (1) Can simple ANNs develop mirror-neuron patterns? and (2) How might these patterns contribute to ethical and cooperative decision-making in AI systems?\n  Using a novel Frog and Toad game framework designed to promote cooperative behaviors, we identify conditions under which mirror-neuron patterns emerge, evaluate their influence on action circuits, introduce the Checkpoint Mirror Neuron Index (CMNI) to quantify activation strength and consistency, and propose a theoretical framework for further study.\n  Our findings indicate that appropriately scaled model capacities and self/other coupling foster shared neural representations in ANNs similar to biological mirror neurons. These empathy-like circuits support cooperative behavior and suggest that intrinsic motivations modeled through mirror-neuron dynamics could complement existing alignment techniques by embedding empathy-like mechanisms directly within AI architectures.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "TabDSR: Decompose, Sanitize, and Reason for Complex Numerical Reasoning in Tabular Data",
      "link": "https://arxiv.org/abs/2511.02219",
      "description": "arXiv:2511.02219v2 Announce Type: replace \nAbstract: Complex reasoning over tabular data is crucial in real-world data analysis, yet large language models (LLMs) often underperform due to complex queries, noisy data, and limited numerical capabilities. To address these issues, we propose TabDSR, a framework consisting of: (1) a query decomposer that breaks down complex questions, (2) a table sanitizer that cleans and filters noisy tables, and (3) a program-of-thoughts (PoT)-based reasoner that generates executable code to derive the final answer from the sanitized table. To ensure unbiased evaluation and mitigate data leakage, we introduce a new dataset, CalTab151, specifically designed for complex numerical reasoning over tables. Experimental results demonstrate that TabDSR consistently outperforms existing methods, achieving state-of-the-art (SOTA) performance with 8.79%, 6.08%, and 19.87% accuracy improvement on TAT-QA, TableBench, and TabDSR, respectively. Moreover, our framework integrates seamlessly with mainstream LLMs, providing a robust solution for complex tabular numerical reasoning. These findings highlight the effectiveness of our framework in enhancing LLM performance for complex tabular numerical reasoning. Data and code are available upon request.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "The ORCA Benchmark: Evaluating Real-World Calculation Accuracy in Large Language Models",
      "link": "https://arxiv.org/abs/2511.02589",
      "description": "arXiv:2511.02589v2 Announce Type: replace \nAbstract: We present ORCA (Omni Research on Calculation in AI) Benchmark - a novel benchmark that evaluates large language models (LLMs) on multi-domain, real-life quantitative reasoning using verified outputs from Omni's calculator engine. In 500 natural-language tasks across domains such as finance, physics, health, and statistics, the five state-of-the-art systems (ChatGPT-5, Gemini~2.5~Flash, Claude~Sonnet~4.5, Grok~4, and DeepSeek~V3.2) achieved only $45\\text{--}63\\,\\%$ accuracy, with errors mainly related to rounding ($35\\,\\%$) and calculation mistakes ($33\\,\\%$). Results in specific domains indicate strengths in mathematics and engineering, but weaknesses in physics and natural sciences. Correlation analysis ($r \\approx 0.40\\text{--}0.65$) shows that the models often fail together but differ in the types of errors they make, highlighting their partial complementarity rather than redundancy. Unlike standard math datasets, ORCA evaluates step-by-step reasoning, numerical precision, and domain generalization across real problems from finance, physics, health, and statistics.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Kosmos: An AI Scientist for Autonomous Discovery",
      "link": "https://arxiv.org/abs/2511.02824",
      "description": "arXiv:2511.02824v2 Announce Type: replace \nAbstract: Data-driven scientific discovery requires iterative cycles of literature search, hypothesis generation, and data analysis. Substantial progress has been made towards AI agents that can automate scientific research, but all such agents remain limited in the number of actions they can take before losing coherence, thus limiting the depth of their findings. Here we present Kosmos, an AI scientist that automates data-driven discovery. Given an open-ended objective and a dataset, Kosmos runs for up to 12 hours performing cycles of parallel data analysis, literature search, and hypothesis generation before synthesizing discoveries into scientific reports. Unlike prior systems, Kosmos uses a structured world model to share information between a data analysis agent and a literature search agent. The world model enables Kosmos to coherently pursue the specified objective over 200 agent rollouts, collectively executing an average of 42,000 lines of code and reading 1,500 papers per run. Kosmos cites all statements in its reports with code or primary literature, ensuring its reasoning is traceable. Independent scientists found 79.4% of statements in Kosmos reports to be accurate, and collaborators reported that a single 20-cycle Kosmos run performed the equivalent of 6 months of their own research time on average. Furthermore, collaborators reported that the number of valuable scientific findings generated scales linearly with Kosmos cycles (tested up to 20 cycles). We highlight seven discoveries made by Kosmos that span metabolomics, materials science, neuroscience, and statistical genetics. Three discoveries independently reproduce findings from preprinted or unpublished manuscripts that were not accessed by Kosmos at runtime, while four make novel contributions to the scientific literature.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Agent-Omni: Test-Time Multimodal Reasoning via Model Coordination for Understanding Anything",
      "link": "https://arxiv.org/abs/2511.02834",
      "description": "arXiv:2511.02834v2 Announce Type: replace \nAbstract: Multimodal large language models (MLLMs) have shown strong capabilities but remain limited to fixed modality pairs and require costly fine-tuning with large aligned datasets. Building fully omni-capable models that can integrate text, images, audio, and video remains impractical and lacks robust reasoning support. In this paper, we propose an Agent-Omni framework that coordinates existing foundation models through a master-agent system, enabling flexible multimodal reasoning without retraining. The master agent interprets user intent, delegates subtasks to modality-specific agents, and integrates their outputs into coherent responses. Extensive experiments across text, image, audio, video, and omni benchmarks show that Agent-Omni consistently achieves state-of-the-art performance, particularly on tasks requiring complex cross-modal reasoning. Its agent-based design enables seamless integration of specialized foundation models, ensuring adaptability to diverse inputs while maintaining transparency and interpretability. In addition, the framework is modular and easily extensible, allowing future improvements as stronger models become available.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Emotion Detection From Social Media Posts",
      "link": "https://arxiv.org/abs/2302.05610",
      "description": "arXiv:2302.05610v2 Announce Type: replace-cross \nAbstract: Over the last few years, social media has evolved into a medium for expressing personal views, emotions, and even business and political proposals, recommendations, and advertisements. We address the topic of identifying emotions from text data obtained from social media posts like Twitter in this research. We have deployed different traditional machine learning techniques such as Support Vector Machines (SVM), Naive Bayes, Decision Trees, and Random Forest, as well as deep neural network models such as LSTM, CNN, GRU, BiLSTM, BiGRU to classify these tweets into four emotion categories (Fear, Anger, Joy, and Sadness). Furthermore, we have constructed a BiLSTM and BiGRU ensemble model. The evaluation result shows that the deep neural network models(BiGRU, to be specific) produce the most promising results compared to traditional machine learning models, with an 87.53 % accuracy rate. The ensemble model performs even better (87.66 %), albeit the difference is not significant. This result will aid in the development of a decision-making tool that visualizes emotional fluctuations.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Transfer Learning-based Real-time Handgun Detection",
      "link": "https://arxiv.org/abs/2311.13559",
      "description": "arXiv:2311.13559v3 Announce Type: replace-cross \nAbstract: Traditional surveillance systems rely on human attention, limiting their effectiveness. This study employs convolutional neural networks and transfer learning to develop a real-time computer vision system for automatic handgun detection. Comprehensive analysis of online handgun detection methods is conducted, emphasizing reducing false positives and learning time. Transfer learning is demonstrated as an effective approach. Despite technical challenges, the proposed system achieves a precision rate of 84.74%, demonstrating promising performance comparable to related works, enabling faster learning and accurate automatic handgun detection for enhanced security. This research advances security measures by reducing human monitoring dependence, showcasing the potential of transfer learning-based approaches for efficient and reliable handgun detection.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Survey on AI Ethics: A Socio-technical Perspective",
      "link": "https://arxiv.org/abs/2311.17228",
      "description": "arXiv:2311.17228v2 Announce Type: replace-cross \nAbstract: The past decade has observed a significant advancement in AI with deep learning-based models being deployed in diverse scenarios, including safety-critical applications. As these AI systems become deeply embedded in our societal infrastructure, the repercussions of their decisions and actions have significant consequences, making the ethical implications of AI deployment highly relevant and essential. The ethical concerns associated with AI are multifaceted, including challenging issues of fairness, privacy and data protection, responsibility and accountability, safety and robustness, transparency and explainability, and environmental impact. These principles together form the foundations of ethical AI considerations that concern every stakeholder in the AI system lifecycle. In light of the present ethical and future x-risk concerns, governments have shown increasing interest in establishing guidelines for the ethical deployment of AI. This work unifies the current and future ethical concerns of deploying AI into society. While we acknowledge and appreciate the technical surveys for each of the ethical principles concerned, in this paper, we aim to provide a comprehensive overview that not only addresses each principle from a technical point of view but also discusses them from a social perspective.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Neural Physics: Using AI Libraries to Develop Physics-Based Solvers for Incompressible Computational Fluid Dynamics",
      "link": "https://arxiv.org/abs/2402.17913",
      "description": "arXiv:2402.17913v2 Announce Type: replace-cross \nAbstract: Numerical discretisations of partial differential equations (PDEs) can be written as discrete convolutions, which, themselves, are a key tool in AI libraries and used in convolutional neural networks (CNNs). We therefore propose to implement numerical discretisations as convolutional layers of a neural network, where the weights or filters are determined analytically rather than by training. Furthermore, we demonstrate that these systems can be solved entirely by functions in AI libraries, either by using Jacobi iteration or multigrid methods, the latter realised through a U-Net architecture. Some advantages of the Neural Physics approach are that (1) the methods are platform agnostic; (2) the resulting solvers are fully differentiable, ideal for optimisation tasks; and (3) writing CFD solvers as (untrained) neural networks means that they can be seamlessly integrated with trained neural networks to form hybrid models. We demonstrate the proposed approach on a number of test cases of increasing complexity from advection-diffusion problems, the non-linear Burgers equation to the Navier-Stokes equations. We validate the approach by comparing our results with solutions obtained from traditionally written code and common benchmarks from the literature. We show that the proposed methodology can solve all these problems using repurposed AI libraries in an efficient way, without training, and presents a new avenue to explore in the development of methods to solve PDEs with implicit methods.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "A Survey of Graph Neural Networks in Real world: Imbalance, Noise, Privacy and OOD Challenges",
      "link": "https://arxiv.org/abs/2403.04468",
      "description": "arXiv:2403.04468v2 Announce Type: replace-cross \nAbstract: Graph-structured data exhibits universality and widespread applicability across diverse domains, such as social network analysis, biochemistry, financial fraud detection, and network security. Significant strides have been made in leveraging Graph Neural Networks (GNNs) to achieve remarkable success in these areas. However, in real-world scenarios, the training environment for models is often far from ideal, leading to substantial performance degradation of GNN models due to various unfavorable factors, including imbalance in data distribution, the presence of noise in erroneous data, privacy protection of sensitive information, and generalization capability for out-of-distribution (OOD) scenarios. To tackle these issues, substantial efforts have been devoted to improving the performance of GNN models in practical real-world scenarios, as well as enhancing their reliability and robustness. In this paper, we present a comprehensive survey that systematically reviews existing GNN models, focusing on solutions to the four mentioned real-world challenges including imbalance, noise, privacy, and OOD in practical scenarios that many existing reviews have not considered. Specifically, we first highlight the four key challenges faced by existing GNNs, paving the way for our exploration of real-world GNN models. Subsequently, we provide detailed discussions on these four aspects, dissecting how these solutions contribute to enhancing the reliability and robustness of GNN models. Last but not least, we outline promising directions and offer future perspectives in the field.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "A Reliable Cryptographic Framework for Empirical Machine Unlearning Evaluation",
      "link": "https://arxiv.org/abs/2404.11577",
      "description": "arXiv:2404.11577v4 Announce Type: replace-cross \nAbstract: Machine unlearning updates machine learning models to remove information from specific training samples, complying with data protection regulations that allow individuals to request the removal of their personal data. Despite the recent development of numerous unlearning algorithms, reliable evaluation of these algorithms remains an open research question. In this work, we focus on membership inference attack (MIA) based evaluation, one of the most common approaches for evaluating unlearning algorithms, and address various pitfalls of existing evaluation metrics lacking theoretical understanding and reliability. Specifically, by modeling the proposed evaluation process as a \\emph{cryptographic game} between unlearning algorithms and MIA adversaries, the naturally induced evaluation metric measures the data removal efficacy of unlearning algorithms and enjoys provable guarantees that existing evaluation metrics fail to satisfy. Furthermore, we propose a practical and efficient approximation of the induced evaluation metric and demonstrate its effectiveness through both theoretical analysis and empirical experiments. Overall, this work presents a novel and reliable approach to empirically evaluating unlearning algorithms, paving the way for the development of more effective unlearning techniques.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Autonomous Robotic Drilling System for Mice Cranial Window Creation",
      "link": "https://arxiv.org/abs/2406.14135",
      "description": "arXiv:2406.14135v2 Announce Type: replace-cross \nAbstract: Robotic assistance for experimental manipulation in the life sciences is expected to enable favorable outcomes, regardless of the skill of the scientist. Experimental specimens in the life sciences are subject to individual variability and hence require intricate algorithms for successful autonomous robotic control. As a use case, we are studying the cranial window creation in mice. This operation requires the removal of an 8-mm circular patch of the skull, which is approximately 300 um thick, but the shape and thickness of the mouse skull significantly varies depending on the strain of the mouse, sex, and age. In this work, we develop an autonomous robotic drilling system with no offline planning, consisting of a trajectory planner with execution-time feedback with drilling completion level recognition based on image and force information. In the experiments, we first evaluate the image-and-force-based drilling completion level recognition by comparing it with other state-of-the-art deep learning image processing methods and conduct an ablation study in eggshell drilling to evaluate the impact of each module on system performance. Finally, the system performance is further evaluated in postmortem mice, achieving a success rate of 70% (14/20 trials) with an average drilling time of 9.3 min.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "MSDNet: Multi-Scale Decoder for Few-Shot Semantic Segmentation via Transformer-Guided Prototyping",
      "link": "https://arxiv.org/abs/2409.11316",
      "description": "arXiv:2409.11316v5 Announce Type: replace-cross \nAbstract: Few-shot Semantic Segmentation addresses the challenge of segmenting objects in query images with only a handful of annotated examples. However, many previous state-of-the-art methods either have to discard intricate local semantic features or suffer from high computational complexity. To address these challenges, we propose a new Few-shot Semantic Segmentation framework based on the Transformer architecture. Our approach introduces the spatial transformer decoder and the contextual mask generation module to improve the relational understanding between support and query images. Moreover, we introduce a multi scale decoder to refine the segmentation mask by incorporating features from different resolutions in a hierarchical manner. Additionally, our approach integrates global features from intermediate encoder stages to improve contextual understanding, while maintaining a lightweight structure to reduce complexity. This balance between performance and efficiency enables our method to achieve competitive results on benchmark datasets such as PASCAL-5^i and COCO-20^i in both 1-shot and 5-shot settings. Notably, our model with only 1.5 million parameters demonstrates competitive performance while overcoming limitations of existing methodologies. https://github.com/amirrezafateh/MSDNet",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Inverse Entropic Optimal Transport Solves Semi-supervised Learning via Data Likelihood Maximization",
      "link": "https://arxiv.org/abs/2410.02628",
      "description": "arXiv:2410.02628v4 Announce Type: replace-cross \nAbstract: Learning conditional distributions $\\pi^*(\\cdot|x)$ is a central problem in machine learning, which is typically approached via supervised methods with paired data $(x,y) \\sim \\pi^*$. However, acquiring paired data samples is often challenging, especially in problems such as domain translation. This necessitates the development of $\\textit{semi-supervised}$ models that utilize both limited paired data and additional unpaired i.i.d. samples $x \\sim \\pi^*_x$ and $y \\sim \\pi^*_y$ from the marginal distributions. The usage of such combined data is complex and often relies on heuristic approaches. To tackle this issue, we propose a new learning paradigm that integrates both paired and unpaired data $\\textbf{seamlessly}$ using the data likelihood maximization techniques. We demonstrate that our approach also connects intriguingly with inverse entropic optimal transport (OT). This finding allows us to apply recent advances in computational OT to establish an $\\textbf{end-to-end}$ learning algorithm to get $\\pi^*(\\cdot|x)$. In addition, we derive the universal approximation property, demonstrating that our approach can theoretically recover true conditional distributions with arbitrarily small error. Furthermore, we demonstrate through empirical tests that our method effectively learns conditional distributions using paired and unpaired data simultaneously.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Mastering Contact-rich Tasks by Combining Soft and Rigid Robotics with Imitation Learning",
      "link": "https://arxiv.org/abs/2410.07787",
      "description": "arXiv:2410.07787v3 Announce Type: replace-cross \nAbstract: Soft robots have the potential to revolutionize the use of robotic systems with their capability of establishing safe, robust, and adaptable interactions with their environment, but their precise control remains challenging. In contrast, traditional rigid robots offer high accuracy and repeatability but lack the flexibility of soft robots. We argue that combining these characteristics in a hybrid robotic platform can significantly enhance overall capabilities. This work presents a novel hybrid robotic platform that integrates a rigid manipulator with a fully developed soft arm. This system is equipped with the intelligence necessary to perform flexible and generalizable tasks through imitation learning autonomously. The physical softness and machine learning enable our platform to achieve highly generalizable skills, while the rigid components ensure precision and repeatability.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Intelligent Computing Social Modeling and Methodological Innovations in Political Science in the Era of Large Language Models",
      "link": "https://arxiv.org/abs/2410.16301",
      "description": "arXiv:2410.16301v2 Announce Type: replace-cross \nAbstract: The recent wave of artificial intelligence, epitomized by large language models (LLMs),has presented opportunities and challenges for methodological innovation in political science,sparking discussions on a potential paradigm shift in the social sciences. However, how can weunderstand the impact of LLMs on knowledge production and paradigm transformation in thesocial sciences from a comprehensive perspective that integrates technology and methodology? What are LLMs' specific applications and representative innovative methods in political scienceresearch? These questions, particularly from a practical methodological standpoint, remainunderexplored. This paper proposes the \"Intelligent Computing Social Modeling\" (ICSM) methodto address these issues by clarifying the critical mechanisms of LLMs. ICSM leverages thestrengths of LLMs in idea synthesis and action simulation, advancing intellectual exploration inpolitical science through \"simulated social construction\" and \"simulation validation.\" Bysimulating the U.S. presidential election, this study empirically demonstrates the operationalpathways and methodological advantages of ICSM. By integrating traditional social scienceparadigms, ICSM not only enhances the quantitative paradigm's capability to apply big data toassess the impact of factors but also provides qualitative paradigms with evidence for socialmechanism discovery at the individual level, offering a powerful tool that balances interpretabilityand predictability in social science research. The findings suggest that LLMs will drivemethodological innovation in political science through integration and improvement rather thandirect substitution.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Graph Sampling for Scalable and Expressive Graph Neural Networks on Homophilic Graphs",
      "link": "https://arxiv.org/abs/2410.16593",
      "description": "arXiv:2410.16593v5 Announce Type: replace-cross \nAbstract: Graph Neural Networks (GNNs) excel in many graph machine learning tasks but face challenges when scaling to large networks. GNN transferability allows training on smaller graphs and applying the model to larger ones, but existing methods often rely on random subsampling, leading to disconnected subgraphs and reduced model expressivity. We propose a novel graph sampling algorithm that leverages feature homophily to preserve graph structure. By minimizing the trace of the data correlation matrix, our method better preserves the graph Laplacian trace -- a proxy for the graph connectivity -- than random sampling, while achieving lower complexity than spectral methods. Experiments on citation networks show improved performance in preserving Laplacian trace and GNN transferability compared to random sampling.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Matryoshka Pilot: Learning to Drive Black-Box LLMs with LLMs",
      "link": "https://arxiv.org/abs/2410.20749",
      "description": "arXiv:2410.20749v3 Announce Type: replace-cross \nAbstract: Despite the impressive generative abilities of black-box large language models (LLMs), their inherent opacity hinders further advancements in capabilities such as reasoning, planning, and personalization. Existing works aim to enhance LLM capabilities via domain-specific adaptation, which require additional training on accessible model parameters, an infeasible option for black-box LLMs. To address this challenge, we introduce Matryoshka Pilot (M-Pilot), a lightweight white-box LLM controller that guides a large-scale black-box LLM generator by decomposing complex tasks into a series of intermediate outputs. Specifically, we consider the black-box LLM as an environment, with M-Pilot serving as a policy to provide intermediate guidance through prompts for driving the black-box LLM. M-Pilot is trained to pivot the outputs of the black-box LLM aligning with preferences during iterative interaction, which enables controllable multi-turn generation and self-improvement in optimizing intermediate guidance. Empirical evaluations on diverse tasks demonstrate that our method effectively enhances the capabilities of black-box LLMs in complex, long-horizon tasks. Our code is publicly available at: https://github.com/lichangh20/Matryoshka.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Do Automatic Factuality Metrics Measure Factuality? A Critical Evaluation",
      "link": "https://arxiv.org/abs/2411.16638",
      "description": "arXiv:2411.16638v4 Announce Type: replace-cross \nAbstract: Modern LLMs can now produce highly readable abstractive summaries, to the point that traditional automated metrics for evaluating summary quality, such as ROUGE, have saturated. However, LLMs still sometimes introduce inaccuracies into summaries, i.e., information inconsistent with or unsupported by the corresponding source. Measuring the occurrence of these often subtle factual inconsistencies automatically has proved challenging. This in turn has motivated development of metrics intended to measure the factual consistency of generated summaries against sources. But are these approaches measuring what they purport to? Or are they mostly exploiting artifacts? In this work, we stress test a range of automatic factuality metrics, including specialized models and LLM-based prompting methods, to probe what they actually capture. Using a shallow classifier to separate ``easy'' examples for factual evaluation where surface features suffice from ``hard'' cases requiring deeper reasoning, we find that all metrics show substantial performance drops on the latter. Furthermore, some metrics are more sensitive to benign, fact-preserving edits than to factual corrections. Building on this observation, we demonstrate that most automatic factuality metrics can be gamed, i.e., their scores can be artificially inflated by appending innocuous, content-free sentences to summaries. Among the metrics tested, the prompt based ChatGPT-DA approach is the most robust and reliable. However, this comes with a notable caveat: Prompting LLMs to assess factuality may overly rely on their parametric knowledge rather than the provided reference when making judgments. Taken together, our findings call into question the reliability of current factuality metrics and prompt a broader reflection on what these metrics are truly measuring.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "RAG-IT: Retrieval-Augmented Instruction Tuning for Automated Financial Analysis",
      "link": "https://arxiv.org/abs/2412.08179",
      "description": "arXiv:2412.08179v2 Announce Type: replace-cross \nAbstract: Financial analysis relies heavily on the interpretation of earnings reports to assess company performance and guide decision-making. Traditional methods for generating such analyses demand significant financial expertise and are often time-consuming. With the rapid advancement of Large Language Models (LLMs), domain-specific adaptations have emerged for financial tasks such as sentiment analysis and entity recognition. This paper introduces RAG-IT (Retrieval-Augmented Instruction Tuning), a novel framework designed to automate the generation of earnings report analyses through an LLM fine-tuned specifically for the financial domain. Our approach integrates retrieval augmentation with instruction-based fine-tuning to enhance factual accuracy, contextual relevance, and domain adaptability. We construct a comprehensive financial instruction dataset derived from extensive financial documents and earnings reports to guide the LLM's adaptation to specialized financial reasoning. Experimental results demonstrate that RAG-IT outperforms general-purpose open-source models and achieves performance comparable to commercial systems like GPT-3.5 on financial report generation tasks. This research highlights the potential of retrieval-augmented instruction tuning to streamline and elevate financial analysis automation, advancing the broader field of intelligent financial reporting.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "REFA: Reference Free Alignment for multi-preference optimization",
      "link": "https://arxiv.org/abs/2412.16378",
      "description": "arXiv:2412.16378v4 Announce Type: replace-cross \nAbstract: To mitigate reward hacking from response verbosity, modern preference optimization methods are increasingly adopting length normalization (e.g., SimPO, ORPO, LN-DPO). While effective against this bias, we demonstrate that length normalization itself introduces a failure mode: the URSLA shortcut. Here models learn to satisfy the alignment objective by prematurely truncating low-quality responses rather than learning from their semantic content. To address this, we introduce REFA, a new alignment framework that proposes probabilistic control on a structural token that controls termination. Our core innovation is a new class of regularizers that operate directly on the probability of the End-of-Sequence (EOS) token, a previously unexploited control lever. This token-level intervention provides a principled solution to the URSLA shortcut, ensuring genuine quality improvements. Furthermore, it unlocks a versatile mechanism for managing the alignment-efficiency tradeoff, enabling practitioners to fine-tune models that adhere to specific token budgets. Empirically, REFA achieves a 60.29% win rate and a 52.17% length-controlled win rate on AlpacaEval2 with Llama-3-8B-Instruct, demonstrating the power of our token-level control paradigm.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "From Haystack to Needle: Label Space Reduction for Zero-shot Classification",
      "link": "https://arxiv.org/abs/2502.08436",
      "description": "arXiv:2502.08436v2 Announce Type: replace-cross \nAbstract: We present Label Space Reduction (LSR), a novel method for improving zero-shot classification performance of Large Language Models (LLMs). LSR iteratively refines the classification label space by systematically ranking and reducing candidate classes, enabling the model to concentrate on the most relevant options. By leveraging unlabeled data with the statistical learning capabilities of data-driven models, LSR dynamically optimizes the label space representation at test time. Our experiments across seven benchmarks demonstrate that LSR improves macro-F1 scores by an average of 7.0% (up to 14.2%) with Llama-3.1-70B and 3.3% (up to 11.1%) with Claude-3.5-Sonnet compared to standard zero-shot classification baselines. To reduce the computational overhead of LSR, which requires an additional LLM call at each iteration, we propose distilling the model into a probabilistic classifier, allowing for efficient inference.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Beyond Covariance Matrix: The Statistical Complexity of Private Linear Regression",
      "link": "https://arxiv.org/abs/2502.13115",
      "description": "arXiv:2502.13115v2 Announce Type: replace-cross \nAbstract: We study the statistical complexity of private linear regression under an unknown, potentially ill-conditioned covariate distribution. Somewhat surprisingly, under privacy constraints the intrinsic complexity is \\emph{not} captured by the usual covariance matrix but rather its $L_1$ analogues. Building on this insight, we establish minimax convergence rates for both the central and local privacy models and introduce an Information-Weighted Regression method that attains the optimal rates.\n  As application, in private linear contextual bandits, we propose an efficient algorithm that achieves rate-optimal regret bounds of order $\\sqrt{T}+\\frac{1}{\\alpha}$ and $\\sqrt{T}/\\alpha$ under joint and local $\\alpha$-privacy models, respectively. Notably, our results demonstrate that joint privacy comes at almost no additional cost, addressing the open problems posed by Azize and Basu (2024).",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "A Survey on Text-Driven 360-Degree Panorama Generation",
      "link": "https://arxiv.org/abs/2502.14799",
      "description": "arXiv:2502.14799v3 Announce Type: replace-cross \nAbstract: The advent of text-driven 360-degree panorama generation, enabling the synthesis of 360-degree panoramic images directly from textual descriptions, marks a transformative advancement in immersive visual content creation. This innovation significantly simplifies the traditionally complex process of producing such content. Recent progress in text-to-image diffusion models has accelerated the rapid development in this emerging field. This survey presents a comprehensive review of text-driven 360-degree panorama generation, offering an in-depth analysis of state-of-the-art algorithms. We extend our analysis to two closely related domains: text-driven 360-degree 3D scene generation and text-driven 360-degree panoramic video generation. Furthermore, we critically examine current limitations and propose promising directions for future research. A curated project page with relevant resources and research papers is available at https://littlewhitesea.github.io/Text-Driven-Pano-Gen/.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Assessing the Macro and Micro Effects of Random Seeds on Fine-Tuning Large Language Models",
      "link": "https://arxiv.org/abs/2503.07329",
      "description": "arXiv:2503.07329v2 Announce Type: replace-cross \nAbstract: The impact of random seeds in fine-tuning large language models (LLMs) has been largely overlooked despite its potential influence on model performance.In this study, we systematically evaluate the effects of random seeds on LLMs using the GLUE and SuperGLUE benchmarks. We analyze the macro-level impact through traditional metrics like accuracy and F1, calculating their mean and variance to quantify performance fluctuations. To capture the micro-level effects, we introduce a novel metric, consistency, measuring the stability of individual predictions across runs. Our experiments reveal significant variance at both macro and micro levels, underscoring the need for careful consideration of random seeds in fine-tuning and evaluation.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Revisiting semi-supervised learning in the era of foundation models",
      "link": "https://arxiv.org/abs/2503.09707",
      "description": "arXiv:2503.09707v4 Announce Type: replace-cross \nAbstract: Semi-supervised learning (SSL) leverages abundant unlabeled data alongside limited labeled data to enhance learning. As vision foundation models (VFMs) increasingly serve as the backbone of vision applications, it remains unclear how SSL interacts with these pre-trained models. To address this gap, we develop new SSL benchmark datasets where frozen VFMs underperform and systematically evaluate representative SSL methods. We make a surprising observation: parameter-efficient fine-tuning (PEFT) using only labeled data often matches SSL performance, even without leveraging unlabeled data. This motivates us to revisit self-training, a conceptually simple SSL baseline, where we use the supervised PEFT model to pseudo-label unlabeled data for further training. To overcome the notorious issue of noisy pseudo-labels, we propose ensembling multiple PEFT approaches and VFM backbones to produce more robust pseudo-labels. Empirical results validate the effectiveness of this simple yet powerful approach, providing actionable insights into SSL with VFMs and paving the way for more scalable and practical semi-supervised learning in the era of foundation models.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "SecRepoBench: Benchmarking Code Agents for Secure Code Completion in Real-World Repositories",
      "link": "https://arxiv.org/abs/2504.21205",
      "description": "arXiv:2504.21205v2 Announce Type: replace-cross \nAbstract: This paper introduces SecRepoBench, a benchmark to evaluate code agents on secure code completion in real-world repositories. SecRepoBench has 318 code completion tasks in 27 C/C++ repositories, covering 15 CWEs. We evaluate 28 standalone LLMs and 13 code agents across 3 state-of-the-art agent frameworks using our benchmark. We find that state-of-the-art LLMs struggle with generating correct and secure code completions. However, code agents significantly outperform standalone LLMs. We show that SecRepoBench is more difficult than the prior state-of-the-art benchmark. Finally, our comprehensive analysis provides insights into potential directions for enhancing the ability of code agents to write correct and secure code in real-world repositories.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "A data-driven framework for team selection in Fantasy Premier League",
      "link": "https://arxiv.org/abs/2505.02170",
      "description": "arXiv:2505.02170v2 Announce Type: replace-cross \nAbstract: Fantasy football is a billion-dollar industry with millions of participants. Under a fixed budget, managers select squads to maximize future Fantasy Premier League (FPL) points. This study formulates lineup selection as data-driven optimization and develops deterministic and robust mixed-integer linear programs that choose the starting eleven, bench, and captain under budget, formation, and club-quota constraints (maximum three players per club). The objective is parameterized by a hybrid scoring metric that combines realized FPL points with predictions from a linear regression model trained on match-performance features identified using exploratory data analysis techniques. The study benchmarks alternative objectives and cost estimators, including simple and recency-weighted averages, exponential smoothing, autoregressive integrated moving average (ARIMA), and Monte Carlo simulation. Experiments on the 2023/24 Premier League season show that ARIMA with a constrained budget and a rolling window yields the most consistent out-of-sample performance; weighted averages and Monte Carlo are also competitive. Robust variants improve some objectives but are not uniformly superior. The framework provides transparent decision support for fantasy roster construction and extends to FPL chips, multi-week rolling-horizon transfer planning, and week-by-week dynamic captaincy.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Deep Learning Warm Starts for Trajectory Optimization on the International Space Station",
      "link": "https://arxiv.org/abs/2505.05588",
      "description": "arXiv:2505.05588v3 Announce Type: replace-cross \nAbstract: Trajectory optimization is a cornerstone of modern robot autonomy, enabling systems to compute trajectories and controls in real-time while respecting safety and physical constraints. However, it has seen limited usage in spaceflight applications due to its heavy computational demands that exceed the capability of most flight computers. In this work, we provide results on the first in-space demonstration of using machine learning-based warm starts for accelerating trajectory optimization for the Astrobee free-flying robot onboard the International Space Station (ISS). We formulate a data-driven optimal control approach that trains a neural network to learn the structure of the trajectory generation problem being solved using sequential convex programming (SCP). Onboard, this trained neural network predicts solutions for the trajectory generation problem and relies on using the SCP solver to enforce safety constraints for the system. Our trained network reduces the number of solver iterations required for convergence in cases including rotational dynamics by 60% and in cases with obstacles drawn from the training distribution of the warm start model by 50%. This work represents a significant milestone in the use of learning-based control for spaceflight applications and a stepping stone for future advances in the use of machine learning for autonomous guidance, navigation, & control.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Traversal Verification for Speculative Tree Decoding",
      "link": "https://arxiv.org/abs/2505.12398",
      "description": "arXiv:2505.12398v2 Announce Type: replace-cross \nAbstract: Speculative decoding is a promising approach for accelerating large language models. The primary idea is to use a lightweight draft model to speculate the output of the target model for multiple subsequent timesteps, and then verify them in parallel to determine whether the drafted tokens should be accepted or rejected. To enhance acceptance rates, existing frameworks typically construct token trees containing multiple candidates in each timestep. However, their reliance on token-level verification mechanisms introduces two critical limitations: First, the probability distribution of a sequence differs from that of individual tokens, leading to suboptimal acceptance length. Second, current verification schemes begin from the root node and proceed layer by layer in a top-down manner. Once a parent node is rejected, all its child nodes should be discarded, resulting in inefficient utilization of speculative candidates. This paper introduces Traversal Verification, a novel speculative decoding algorithm that fundamentally rethinks the verification paradigm through leaf-to-root traversal. Our approach considers the acceptance of the entire token sequence from the current node to the root, and preserves potentially valid subsequences that would be prematurely discarded by existing methods. We theoretically prove that the probability distribution obtained through Traversal Verification is identical to that of the target model, guaranteeing lossless inference while achieving substantial acceleration gains. Experimental results across different large language models and multiple tasks show that our method consistently improves acceptance length and throughput over existing methods.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "RoboRAN: A Unified Robotics Framework for Reinforcement Learning-Based Autonomous Navigation",
      "link": "https://arxiv.org/abs/2505.14526",
      "description": "arXiv:2505.14526v2 Announce Type: replace-cross \nAbstract: Autonomous robots must navigate and operate in diverse environments, from terrestrial and aquatic settings to aerial and space domains. While Reinforcement Learning (RL) has shown promise in training policies for specific autonomous robots, existing frameworks and benchmarks are often constrained to unique platforms, limiting generalization and fair comparisons across different mobility systems. In this paper, we present a multi-domain framework for training, evaluating and deploying RL-based navigation policies across diverse robotic platforms and operational environments. Our work presents four key contributions: (1) a scalable and modular framework, facilitating seamless robot-task interchangeability and reproducible training pipelines; (2) sim-to-real transfer demonstrated through real-world experiments with multiple robots, including a satellite robotic simulator, an unmanned surface vessel, and a wheeled ground vehicle; (3) the release of the first open-source API for deploying Isaac Lab-trained policies to real robots, enabling lightweight inference and rapid field validation; and (4) uniform tasks and metrics for cross-medium evaluation, through a unified evaluation testbed to assess performance of navigation tasks in diverse operational conditions (aquatic, terrestrial and space). By ensuring consistency between simulation and real-world deployment, RoboRAN lowers the barrier to developing adaptable RL-based navigation strategies. Its modular design enables straightforward integration of new robots and tasks through predefined templates, fostering reproducibility and extension to diverse domains. To support the community, we release RoboRAN as open-source.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "This Time is Different: An Observability Perspective on Time Series Foundation Models",
      "link": "https://arxiv.org/abs/2505.14766",
      "description": "arXiv:2505.14766v2 Announce Type: replace-cross \nAbstract: We introduce Toto, a time series forecasting foundation model with 151 million parameters. Toto uses a modern decoder-only architecture coupled with architectural innovations designed to account for specific challenges found in multivariate observability time series data. Toto's pre-training corpus is a mixture of observability data, open datasets, and synthetic data, and is 4-10$\\times$ larger than those of leading time series foundation models. Additionally, we introduce BOOM, a large-scale benchmark consisting of 350 million observations across 2,807 real-world time series. For both Toto and BOOM, we source observability data exclusively from Datadog's own telemetry and internal observability metrics. Extensive evaluations demonstrate that Toto achieves state-of-the-art performance on both BOOM and on established general purpose time series forecasting benchmarks. Toto's model weights, inference code, and evaluation scripts, as well as BOOM's data and evaluation code, are all available as open source under the Apache 2.0 License available at https://huggingface.co/Datadog/Toto-Open-Base-1.0 and https://github.com/DataDog/toto.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Distilling LLM Agent into Small Models with Retrieval and Code Tools",
      "link": "https://arxiv.org/abs/2505.17612",
      "description": "arXiv:2505.17612v2 Announce Type: replace-cross \nAbstract: Large language models (LLMs) excel at complex reasoning tasks but remain computationally expensive, limiting their practical deployment. To address this, recent works have focused on distilling reasoning capabilities into smaller language models (sLMs) using chain-of-thought (CoT) traces from teacher LLMs. However, this approach struggles in scenarios requiring rare factual knowledge or precise computation, where sLMs often hallucinate due to limited capability. In this work, we propose Agent Distillation, a framework for transferring not only reasoning capability but full task-solving behavior from LLM-based agents into sLMs with retrieval and code tools. We improve agent distillation along two complementary axes: (1) we introduce a prompting method called first-thought prefix to enhance the quality of teacher-generated trajectories; and (2) we propose a self-consistent action generation for improving test-time robustness of small agents. We evaluate our method on eight reasoning tasks across factual and mathematical domains, covering both in-domain and out-of-domain generalization. Our results show that sLMs as small as 0.5B, 1.5B, 3B parameters can achieve performance competitive with next-tier larger 1.5B, 3B, 7B models fine-tuned using CoT distillation, demonstrating the potential of agent distillation for building practical, tool-using small agents. Our code is available at https://github.com/Nardien/agent-distillation.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Large Language Models Miss the Multi-Agent Mark",
      "link": "https://arxiv.org/abs/2505.21298",
      "description": "arXiv:2505.21298v3 Announce Type: replace-cross \nAbstract: Recent interest in Multi-Agent Systems of Large Language Models (MAS LLMs) has led to an increase in frameworks leveraging multiple LLMs to tackle complex tasks. However, much of this literature appropriates the terminology of MAS without engaging with its foundational principles. In this position paper, we highlight critical discrepancies between MAS theory and current MAS LLMs implementations, focusing on four key areas: the social aspect of agency, environment design, coordination and communication protocols, and measuring emergent behaviours. Our position is that many MAS LLMs lack multi-agent characteristics such as autonomy, social interaction, and structured environments, and often rely on oversimplified, LLM-centric architectures. The field may slow down and lose traction by revisiting problems the MAS literature has already addressed. Therefore, we systematically analyse this issue and outline associated research opportunities; we advocate for better integrating established MAS concepts and more precise terminology to avoid mischaracterisation and missed opportunities.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "R2R: Efficiently Navigating Divergent Reasoning Paths with Small-Large Model Token Routing",
      "link": "https://arxiv.org/abs/2505.21600",
      "description": "arXiv:2505.21600v2 Announce Type: replace-cross \nAbstract: Large Language Models (LLMs) achieve impressive reasoning capabilities at the cost of substantial inference overhead, posing substantial deployment challenges. Although distilled Small Language Models (SLMs) significantly enhance efficiency, their performance suffers as they fail to follow LLMs' reasoning paths. Luckily, we reveal that only a small fraction of tokens genuinely diverge reasoning paths between LLMs and SLMs. Most generated tokens are either identical or exhibit neutral differences, such as minor variations in abbreviations or expressions. Leveraging this insight, we introduce **Roads to Rome (R2R)**, a neural token routing method that selectively utilizes LLMs only for these critical, path-divergent tokens, while leaving the majority of token generation to the SLM. We also develop an automatic data generation pipeline that identifies divergent tokens and generates token-level routing labels to train the lightweight router. We apply R2R to combine R1-1.5B and R1-32B models from the DeepSeek family, and evaluate on challenging math, coding, and QA benchmarks. With an average activated parameter size of 5.6B, R2R surpasses the average accuracy of R1-7B by 1.6x, outperforming even the R1-14B model. Compared to R1-32B, it delivers a 2.8x wall-clock speedup with comparable performance, advancing the Pareto frontier of test-time scaling efficiency. Our code is available at https://github.com/thu-nics/R2R.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "SLED: A Speculative LLM Decoding Framework for Efficient Edge Serving",
      "link": "https://arxiv.org/abs/2506.09397",
      "description": "arXiv:2506.09397v5 Announce Type: replace-cross \nAbstract: The growing gap between the increasing complexity of large language models (LLMs) and the limited computational budgets of edge devices poses a key challenge for efficient on-device inference, despite gradual improvements in hardware capabilities. Existing strategies, such as aggressive quantization, pruning, or remote inference, trade accuracy for efficiency or lead to substantial cost burdens. This position paper introduces a new framework that leverages speculative decoding, previously viewed primarily as a decoding acceleration technique for autoregressive generation of LLMs, as a promising approach specifically adapted for edge computing by orchestrating computation across heterogeneous devices. We propose \\acronym, a framework that allows lightweight edge devices to draft multiple candidate tokens locally using diverse draft models, while a single, shared edge server verifies the tokens utilizing a more precise target model. To further increase the efficiency of verification, the edge server batch the diverse verification requests from devices. This approach supports device heterogeneity and reduces server-side memory footprint by sharing the same upstream target model across multiple devices. Our initial experiments with Jetson Orin Nano, Raspberry Pi 4B/5, and an edge server equipped with 4 Nvidia A100 GPUs indicate substantial benefits: 2.2 more system throughput, 2.8 more system capacity, and better cost efficiency, all without sacrificing model accuracy.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Balancing Tails when Comparing Distributions: Comprehensive Equity Index (CEI) with Application to Bias Evaluation in Operational Face Biometrics",
      "link": "https://arxiv.org/abs/2506.10564",
      "description": "arXiv:2506.10564v2 Announce Type: replace-cross \nAbstract: Demographic bias in high-performance face recognition (FR) systems often eludes detection by existing metrics, especially with respect to subtle disparities in the tails of the score distribution. We introduce the Comprehensive Equity Index (CEI), a novel metric designed to address this limitation. CEI uniquely analyzes genuine and impostor score distributions separately, enabling a configurable focus on tail probabilities while also considering overall distribution shapes. Our extensive experiments (evaluating state-of-the-art FR systems, intentionally biased models, and diverse datasets) confirm CEI's superior ability to detect nuanced biases where previous methods fall short. Furthermore, we present CEI^A, an automated version of the metric that enhances objectivity and simplifies practical application. CEI provides a robust and sensitive tool for operational FR fairness assessment. The proposed methods have been developed particularly for bias evaluation in face biometrics but, in general, they are applicable for comparing statistical distributions in any problem where one is interested in analyzing the distribution tails.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "AlphaDecay: Module-wise Weight Decay for Heavy-Tailed Balancing in LLMs",
      "link": "https://arxiv.org/abs/2506.14562",
      "description": "arXiv:2506.14562v3 Announce Type: replace-cross \nAbstract: Weight decay is a standard regularization technique for training large language models (LLMs). While it is common to assign a uniform decay rate to every layer, this approach overlooks the structural diversity of LLMs and the varying spectral properties across modules. In this paper, we introduce AlphaDecay, a simple yet effective method that adaptively assigns different weight decay strengths to each module of an LLM. Our approach is guided by Heavy-Tailed Self-Regularization (HT-SR) theory, which analyzes the empirical spectral density (ESD) of weight correlation matrices to quantify \"heavy-tailedness.\" Modules exhibiting more pronounced heavy-tailed ESDs, reflecting stronger feature learning, are assigned weaker decay, while modules with lighter-tailed spectra receive stronger decay. Our method leverages tailored weight decay assignments to balance the module-wise differences in spectral properties, leading to improved performance. Extensive pre-training tasks with various model sizes from 60M to 1B demonstrate that AlphaDecay achieves better perplexity and generalization than conventional uniform decay and other adaptive decay baselines. Our code is available at https://github.com/hed-ucas/AlphaDecay.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Dense SAE Latents Are Features, Not Bugs",
      "link": "https://arxiv.org/abs/2506.15679",
      "description": "arXiv:2506.15679v2 Announce Type: replace-cross \nAbstract: Sparse autoencoders (SAEs) are designed to extract interpretable features from language models by enforcing a sparsity constraint. Ideally, training an SAE would yield latents that are both sparse and semantically meaningful. However, many SAE latents activate frequently (i.e., are \\emph{dense}), raising concerns that they may be undesirable artifacts of the training procedure. In this work, we systematically investigate the geometry, function, and origin of dense latents and show that they are not only persistent but often reflect meaningful model representations. We first demonstrate that dense latents tend to form antipodal pairs that reconstruct specific directions in the residual stream, and that ablating their subspace suppresses the emergence of new dense features in retrained SAEs -- suggesting that high density features are an intrinsic property of the residual space. We then introduce a taxonomy of dense latents, identifying classes tied to position tracking, context binding, entropy regulation, letter-specific output signals, part-of-speech, and principal component reconstruction. Finally, we analyze how these features evolve across layers, revealing a shift from structural features in early layers, to semantic features in mid layers, and finally to output-oriented signals in the last layers of the model. Our findings indicate that dense latents serve functional roles in language model computation and should not be dismissed as training noise.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Benchmarking Foundation Models and Parameter-Efficient Fine-Tuning for Prognosis Prediction in Medical Imaging",
      "link": "https://arxiv.org/abs/2506.18434",
      "description": "arXiv:2506.18434v2 Announce Type: replace-cross \nAbstract: Despite the significant potential of Foundation Models (FMs) in medical imaging, their application to prognosis prediction remains challenging due to data scarcity, class imbalance, and task complexity, which limit their clinical adoption. This study introduces the first structured benchmark to assess the robustness and efficiency of transfer learning strategies for FMs compared with convolutional neural networks (CNNs) in predicting COVID-19 patient outcomes from chest X-rays. The goal is to systematically compare finetuning strategies, both classical and parameter efficient, under realistic clinical constraints related to data scarcity and class imbalance, offering empirical guidance for AI deployment in clinical workflows. Four publicly available COVID-19 chest X-ray datasets were used, covering mortality, severity, and ICU admission, with varying sample sizes and class imbalances. CNNs pretrained on ImageNet and FMs pretrained on general or biomedical datasets were adapted using full finetuning, linear probing, and parameter-efficient methods. Models were evaluated under full data and few shot regimes using the Matthews Correlation Coefficient (MCC) and Precision Recall AUC (PR-AUC), with cross validation and class weighted losses. CNNs with full fine-tuning performed robustly on small, imbalanced datasets, while FMs with Parameter-Efficient Fine-Tuning (PEFT), particularly LoRA and BitFit, achieved competitive results on larger datasets. Severe class imbalance degraded PEFT performance, whereas balanced data mitigated this effect. In few-shot settings, FMs showed limited generalization, with linear probing yielding the most stable results. No single fine-tuning strategy proved universally optimal: CNNs remain dependable for low-resource scenarios, whereas FMs benefit from parameter-efficient methods when data are sufficient.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Layer Importance for Mathematical Reasoning is Forged in Pre-Training and Invariant after Post-Training",
      "link": "https://arxiv.org/abs/2506.22638",
      "description": "arXiv:2506.22638v2 Announce Type: replace-cross \nAbstract: Large language models improve at math after instruction tuning, reinforcement learning, or knowledge distillation. We ask whether these gains come from major changes in the transformer layers or from smaller adjustments that keep the original structure. Using layer-wise ablation on base and trained variants, we find that math reasoning depends on a few critical layers, which stay important across all post- training methods. Removing these layers reduces math accuracy by as much as 80%, whereas factual recall tasks only show relatively smaller drops. This suggests that specialized layers for mathematical tasks form during pre-training and remain stable afterward. As measured by Normalized Mutual Information (NMI), we find that near these critical layers, tokens drift from their original syntactic clusters toward representations aligned with tokens less syntactically related but potentially more useful for downstream task.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "FedRef: Communication-Efficient Bayesian Fine-Tuning using a Reference Model",
      "link": "https://arxiv.org/abs/2506.23210",
      "description": "arXiv:2506.23210v3 Announce Type: replace-cross \nAbstract: Federated learning (FL) collaboratively trains artificial intelligence (AI) models to ensure user data privacy. Sharing only model updates generated from local training on client data with the server enhances user data privacy. However, model performance may suffer due to data and system heterogeneity among clients in FL scenarios. Previous studies have proposed model optimization, fine-tuning, and personalization to achieve improved model performance. Despite these efforts, models resulting from FL scenarios often exhibit catastrophic forgetting, which increases the communication and computational costs of clients for model optimization and raises energy consumption. To address these challenges, we propose a reference model-based fine-tuning method for federated learning that overcomes catastrophic forgetting in each round. Our method is derived from Bayesian parameter-efficient transfer learning and includes an proximal term. It employs a reference model that incorporates previous model parameters and reviews previous global features in the model optimization step to mitigate catastrophic forgetting. As a result, our method achieves higher model performance and lower communication and computational costs for clients than existing methods.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Omni-Router: Sharing Routing Decisions in Sparse Mixture-of-Experts for Speech Recognition",
      "link": "https://arxiv.org/abs/2507.05724",
      "description": "arXiv:2507.05724v3 Announce Type: replace-cross \nAbstract: Mixture-of-experts (MoE) architectures have expanded from language modeling to automatic speech recognition (ASR). Traditional MoE methods, such as the Switch Transformer, route experts independently within each layer. Our analysis reveals that routers in most layers make expert choices that are not strongly correlated with the choices of the routers in other layers. To increase the cooperation between experts in different layers and encourage greater specialization, we use a shared router across different MoE layers. We call this model Omni-router Transformer. Extensive experiments on a large-scale pseudo-labeled dataset and evaluations across 10 diverse, out-of-domain ASR benchmarks demonstrate that the Omni-router Transformer is able to achieve lower training loss and consistently outperform dense and Switch Transformer models, reducing average word error rates by 11.2% and 8.2%, respectively, while providing structured expert usage and improved robustness to diverse data.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "DiffSpectra: Molecular Structure Elucidation from Spectra using Diffusion Models",
      "link": "https://arxiv.org/abs/2507.06853",
      "description": "arXiv:2507.06853v2 Announce Type: replace-cross \nAbstract: Molecular structure elucidation from spectra is a fundamental challenge in molecular science. Conventional approaches rely heavily on expert interpretation and lack scalability, while retrieval-based machine learning approaches remain constrained by limited reference libraries. Generative models offer a promising alternative, yet most adopt autoregressive architectures that overlook 3D geometry and struggle to integrate diverse spectral modalities. In this work, we present DiffSpectra, a generative framework that formulates molecular structure elucidation as a conditional generation process, directly inferring 2D and 3D molecular structures from multi-modal spectra using diffusion models. Its denoising network is parameterized by the Diffusion Molecule Transformer, an SE(3)-equivariant architecture for geometric modeling, conditioned by SpecFormer, a Transformer-based spectral encoder capturing multi-modal spectral dependencies. Extensive experiments demonstrate that DiffSpectra accurately elucidates molecular structures, achieving 40.76% top-1 and 99.49% top-10 accuracy. Its performance benefits substantially from 3D geometric modeling, SpecFormer pre-training, and multi-modal conditioning. To our knowledge, DiffSpectra is the first framework that unifies multi-modal spectral reasoning and joint 2D/3D generative modeling for de novo molecular structure elucidation.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Flow matching for reaction pathway generation",
      "link": "https://arxiv.org/abs/2507.10530",
      "description": "arXiv:2507.10530v4 Announce Type: replace-cross \nAbstract: Elucidating reaction mechanisms hinges on efficiently generating transition states (TSs), products, and complete reaction networks. Recent generative models, such as diffusion models for TS sampling and sequence-based architectures for product generation, offer faster alternatives to quantum-chemistry searches. But diffusion models remain constrained by their stochastic differential equation (SDE) dynamics, which suffer from inefficiency and limited controllability. We show that flow matching, a deterministic ordinary differential (ODE) formulation, can replace SDE-based diffusion for molecular and reaction generation. We introduce MolGEN, a conditional flow-matching framework that learns an optimal transport path to transport Gaussian priors to target chemical distributions. On benchmarks used by TSDiff and OA-ReactDiff, MolGEN surpasses TS geometry accuracy and barrier-height prediction while reducing sampling to sub-second inference. MolGEN also supports open-ended product generation with competitive top-k accuracy and avoids mass/electron-balance violations common to sequence models. In a realistic test on the $\\gamma$-ketohydroperoxide decomposition network, MolGEN yields higher fractions of valid and intended TSs with markedly fewer quantum-chemistry evaluations than string-based baselines. These results demonstrate that deterministic flow matching provides a unified, accurate, and computationally efficient foundation for molecular generative modeling, signaling that flow matching is the future for molecular generation across chemistry.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Automatic Road Subsurface Distress Recognition from Ground Penetrating Radar Images using Deep Learning-based Cross-verification",
      "link": "https://arxiv.org/abs/2507.11081",
      "description": "arXiv:2507.11081v2 Announce Type: replace-cross \nAbstract: Ground penetrating radar (GPR) has become a rapid and non-destructive solution for road subsurface distress (RSD) detection. Deep learning-based automatic RSD recognition, though ameliorating the burden of data processing, suffers from data scarcity and insufficient capability to recognize defects. In this study, a rigorously validated 3D GPR dataset containing 2134 samples of diverse types was constructed through field scanning. A novel cross-verification strategy was proposed to fully exploit the complementary abilities of region proposal networks in object recognition from different views of GPR images. The method achieves outstanding accuracy with a recall over 98.6% in field tests. The approach, integrated into an online RSD detection system, can reduce the human labor of inspection by around 90%.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Enhancing Fatigue Detection through Heterogeneous Multi-Source Data Integration and Cross-Domain Modality Imputation",
      "link": "https://arxiv.org/abs/2507.16859",
      "description": "arXiv:2507.16859v3 Announce Type: replace-cross \nAbstract: Fatigue detection for human operators plays a key role in safety critical applications such as aviation, mining, and long haul transport. While numerous studies have demonstrated the effectiveness of high fidelity sensors in controlled laboratory environments, their performance often degrades when ported to real world settings due to noise, lighting conditions, and field of view constraints, thereby limiting their practicality. This paper formalizes a deployment oriented setting for real world fatigue detection, where high quality sensors are often unavailable in practical applications. To address this challenge, we propose leveraging knowledge from heterogeneous source domains, including high fidelity sensors that are difficult to deploy in the field but commonly used in controlled environments, to assist fatigue detection in the real world target domain. Building on this idea, we design a heterogeneous and multiple source fatigue detection framework that adaptively utilizes the available modalities in the target domain while exploiting diverse configurations in the source domains through alignment across domains and modality imputation. Our experiments, conducted using a field deployed sensor setup and two publicly available human fatigue datasets, demonstrate the practicality, robustness, and improved generalization of our approach across subjects and domains. The proposed method achieves consistent gains over strong baselines in sensor constrained scenarios.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "PhysicsEval: Inference-Time Techniques to Improve the Reasoning Proficiency of Large Language Models on Physics Problems",
      "link": "https://arxiv.org/abs/2508.00079",
      "description": "arXiv:2508.00079v2 Announce Type: replace-cross \nAbstract: The discipline of physics stands as a cornerstone of human intellect, driving the evolution of technology and deepening our understanding of the fundamental principles of the cosmos. Contemporary literature includes some works centered on the task of solving physics problems - a crucial domain of natural language reasoning. In this paper, we evaluate the performance of frontier LLMs in solving physics problems, both mathematical and descriptive. We also employ a plethora of inference-time techniques and agentic frameworks to improve the performance of the models. This includes the verification of proposed solutions in a cumulative fashion by other, smaller LLM agents, and we perform a comparative analysis of the performance that the techniques entail. There are significant improvements when the multi-agent framework is applied to problems that the models initially perform poorly on. Furthermore, we introduce a new evaluation benchmark for physics problems, ${\\rm P{\\small HYSICS}E{\\small VAL}}$, consisting of 19,609 problems sourced from various physics textbooks and their corresponding correct solutions scraped from physics forums and educational websites. Our code and data are publicly available at https://github.com/areebuzair/PhysicsEval.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "TensorHyper-VQC: A Tensor-Train-Guided Hypernetwork for Robust and Scalable Variational Quantum Computing",
      "link": "https://arxiv.org/abs/2508.01116",
      "description": "arXiv:2508.01116v2 Announce Type: replace-cross \nAbstract: Variational Quantum Computing (VQC) faces fundamental scalability barriers, primarily due to the presence of barren plateaus and its sensitivity to quantum noise. To address these challenges, we introduce TensorHyper-VQC, a novel tensor-train (TT)-guided hypernetwork framework that significantly improves the robustness and scalability of VQC. Our framework fully delegates the generation of quantum circuit parameters to a classical TT network, effectively decoupling optimization from quantum hardware. This innovative parameterization mitigates gradient vanishing, enhances noise resilience through structured low-rank representations, and facilitates efficient gradient propagation. Grounded in Neural Tangent Kernel and statistical learning theory, our rigorous theoretical analyses establish strong guarantees on approximation capability, optimization stability, and generalization performance. Extensive empirical results across quantum dot classification, Max-Cut optimization, and molecular quantum simulation tasks demonstrate that TensorHyper-VQC consistently achieves superior performance and robust noise tolerance, including hardware-level validation on a 156-qubit IBM Heron processor. These results position TensorHyper-VQC as a scalable and noise-resilient framework for advancing practical quantum machine learning on near-term devices.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Decentralized Aerial Manipulation of a Cable-Suspended Load using Multi-Agent Reinforcement Learning",
      "link": "https://arxiv.org/abs/2508.01522",
      "description": "arXiv:2508.01522v3 Announce Type: replace-cross \nAbstract: This paper presents the first decentralized method to enable real-world 6-DoF manipulation of a cable-suspended load using a team of Micro-Aerial Vehicles (MAVs). Our method leverages multi-agent reinforcement learning (MARL) to train an outer-loop control policy for each MAV. Unlike state-of-the-art controllers that utilize a centralized scheme, our policy does not require global states, inter-MAV communications, nor neighboring MAV information. Instead, agents communicate implicitly through load pose observations alone, which enables high scalability and flexibility. It also significantly reduces computing costs during inference time, enabling onboard deployment of the policy. In addition, we introduce a new action space design for the MAVs using linear acceleration and body rates. This choice, combined with a robust low-level controller, enables reliable sim-to-real transfer despite significant uncertainties caused by cable tension during dynamic 3D motion. We validate our method in various real-world experiments, including full-pose control under load model uncertainties, showing setpoint tracking performance comparable to the state-of-the-art centralized method. We also demonstrate cooperation amongst agents with heterogeneous control policies, and robustness to the complete in-flight loss of one MAV. Videos of experiments: https://autonomousrobots.nl/paper_websites/aerial-manipulation-marl",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "CoTox: Chain-of-Thought-Based Molecular Toxicity Reasoning and Prediction",
      "link": "https://arxiv.org/abs/2508.03159",
      "description": "arXiv:2508.03159v2 Announce Type: replace-cross \nAbstract: Drug toxicity remains a major challenge in pharmaceutical development. Recent machine learning models have improved in silico toxicity prediction, but their reliance on annotated data and lack of interpretability limit their applicability. This limits their ability to capture organ-specific toxicities driven by complex biological mechanisms. Large language models (LLMs) offer a promising alternative through step-by-step reasoning and integration of textual data, yet prior approaches lack biological context and transparent rationale. To address this issue, we propose CoTox, a novel framework that integrates LLM with chain-of-thought (CoT) reasoning for multi-toxicity prediction. CoTox combines chemical structure data, biological pathways, and gene ontology (GO) terms to generate interpretable toxicity predictions through step-by-step reasoning. Using GPT-4o, we show that CoTox outperforms both traditional machine learning and deep learning model. We further examine its performance across various LLMs to identify where CoTox is most effective. Additionally, we find that representing chemical structures with IUPAC names, which are easier for LLMs to understand than SMILES, enhances the model's reasoning ability and improves predictive performance. To demonstrate its practical utility in drug development, we simulate the treatment of relevant cell types with drug and incorporated the resulting biological context into the CoTox framework. This approach allow CoTox to generate toxicity predictions aligned with physiological responses, as shown in case study. This result highlights the potential of LLM-based frameworks to improve interpretability and support early-stage drug safety assessment. The code and prompt used in this work are available at https://github.com/dmis-lab/CoTox.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "ViFP: A Framework for Visual False Positive Detection to Enhance Reasoning Reliability in VLMs",
      "link": "https://arxiv.org/abs/2508.04201",
      "description": "arXiv:2508.04201v2 Announce Type: replace-cross \nAbstract: During reasoning in vision-language models (VLMs), false positive (FP) reasoning occurs when a model produces the correct answer but follows an incorrect reasoning path, resulting in undermined reasoning reliability. Existing approaches mainly rely on prompt engineering, knowledge distillation or reinforcement learning to improve reasoning reliability, both of which require large amounts of high-quality data and thus limit practical applicability. Few approaches have focused on directly detecting and correcting FPs. To address these issues, we propose ViFP, a framework for Visual False Positive Detection to Enhance Reasoning Reliability in VLMs. ViFP builds effective reasoning paths through multi-turn QA and dynamically analyzes the consistency of the reasoning path to identify potential FPs. It also introduces a targeted reasoning chain correction mechanism to modify FP reasoning, thereby improving logical consistency and accuracy. Finally, we introduce a reliability evaluation metric, VoC, which integrates answer accuracy and the FP rate, providing a quantitative tool to assess whether a VLM not only answers correctly but also reasons reliably. Our experiments on closed-source VLMs show that ViFP consistently improves performance across three datasets: A-OKVQA, OK-VQA, and FVQA. On A-OKVQA, ViFP improves accuracy by up to 5.4%, surpassing the previous state-of-the-art by 4.3%, and significantly reduces the number of FPs, validating its benefits in enhancing reasoning reliability.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Voost: A Unified and Scalable Diffusion Transformer for Bidirectional Virtual Try-On and Try-Off",
      "link": "https://arxiv.org/abs/2508.04825",
      "description": "arXiv:2508.04825v2 Announce Type: replace-cross \nAbstract: Virtual try-on aims to synthesize a realistic image of a person wearing a target garment, but accurately modeling garment-body correspondence remains a persistent challenge, especially under pose and appearance variation. In this paper, we propose Voost - a unified and scalable framework that jointly learns virtual try-on and try-off with a single diffusion transformer. By modeling both tasks jointly, Voost enables each garment-person pair to supervise both directions and supports flexible conditioning over generation direction and garment category, enhancing garment-body relational reasoning without task-specific networks, auxiliary losses, or additional labels. In addition, we introduce two inference-time techniques: attention temperature scaling for robustness to resolution or mask variation, and self-corrective sampling that leverages bidirectional consistency between tasks. Extensive experiments demonstrate that Voost achieves state-of-the-art results on both try-on and try-off benchmarks, consistently outperforming strong baselines in alignment accuracy, visual fidelity, and generalization.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Fast weight programming and linear transformers: from machine learning to neurobiology",
      "link": "https://arxiv.org/abs/2508.08435",
      "description": "arXiv:2508.08435v2 Announce Type: replace-cross \nAbstract: Recent advances in artificial neural networks for machine learning, and language modeling in particular, have established a family of recurrent neural network (RNN) architectures that, unlike conventional RNNs with vector-form hidden states, use two-dimensional (2D) matrix-form hidden states. Such 2D-state RNNs, known as Fast Weight Programmers (FWPs), can be interpreted as a neural network whose synaptic weights (called fast weights) dynamically change over time as a function of input observations, and serve as short-term memory storage; corresponding synaptic weight modifications are controlled or programmed by another network (the programmer) whose parameters are trained (e.g., by gradient descent). In this Primer, we review the technical foundations of FWPs, their computational characteristics, and their connections to transformers and state space models. We also discuss connections between FWPs and models of synaptic plasticity in the brain, suggesting a convergence of natural and artificial intelligence.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Geometry-Aware Global Feature Aggregation for Real-Time Indirect Illumination",
      "link": "https://arxiv.org/abs/2508.08826",
      "description": "arXiv:2508.08826v3 Announce Type: replace-cross \nAbstract: Real-time rendering with global illumination is crucial to afford the user realistic experience in virtual environments. We present a learning-based estimator to predict diffuse indirect illumination in screen space, which then is combined with direct illumination to synthesize globally-illuminated high dynamic range (HDR) results. Our approach tackles the challenges of capturing long-range/long-distance indirect illumination when employing neural networks and is generalized to handle complex lighting and scenarios.\n  From the neural network thinking of the solver to the rendering equation, we present a novel network architecture to predict indirect illumination. Our network is equipped with a modified attention mechanism that aggregates global information guided by spacial geometry features, as well as a monochromatic design that encodes each color channel individually.\n  We conducted extensive evaluations, and the experimental results demonstrate our superiority over previous learning-based techniques. Our approach excels at handling complex lighting such as varying-colored lighting and environment lighting. It can successfully capture distant indirect illumination and simulates the interreflections between textured surfaces well (i.e., color bleeding effects); it can also effectively handle new scenes that are not present in the training dataset.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Automated Segmentation of Coronal Brain Tissue Slabs for 3D Neuropathology",
      "link": "https://arxiv.org/abs/2508.09805",
      "description": "arXiv:2508.09805v2 Announce Type: replace-cross \nAbstract: Advances in image registration and machine learning have recently enabled volumetric analysis of postmortem brain tissue from conventional photographs of coronal slabs, which are routinely collected in brain banks and neuropathology laboratories worldwide. One caveat of this methodology is the requirement of segmentation of the tissue from photographs, which currently requires costly manual intervention. In this article, we present a deep learning model to automate this process. The automatic segmentation tool relies on a U-Net architecture that was trained with a combination of 1,414 manually segmented images of both fixed and fresh tissue, from specimens with varying diagnoses, photographed at two different sites. Automated model predictions on a subset of photographs not seen in training were analyzed to estimate performance compared to manual labels, including both inter- and intra-rater variability. Our model achieved a median Dice score over 0.98, mean surface distance under 0.4mm, and 95\\% Hausdorff distance under 1.60mm, which approaches inter-/intra-rater levels. Our tool is publicly available at surfer.nmr.mgh.harvard.edu/fswiki/PhotoTools.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "AnalogSeeker: An Open-source Foundation Language Model for Analog Circuit Design",
      "link": "https://arxiv.org/abs/2508.10409",
      "description": "arXiv:2508.10409v2 Announce Type: replace-cross \nAbstract: In this paper, we propose AnalogSeeker, an effort toward an open-source foundation language model for analog circuit design, with the aim of integrating domain knowledge and giving design assistance. To overcome the scarcity of data in this field, we employ a corpus collection strategy based on the domain knowledge framework of analog circuits. High-quality, accessible textbooks across relevant subfields are systematically curated and cleaned into a textual domain corpus. To address the complexity of knowledge of analog circuits, we introduce a granular domain knowledge distillation method. Raw, unlabeled domain corpus is decomposed into typical, granular learning nodes, where a multi-agent framework distills implicit knowledge embedded in unstructured text into question-answer data pairs with detailed reasoning processes, yielding a fine-grained, learnable dataset for fine-tuning. To address the unexplored challenges in training analog circuit foundation models, we explore and share our training methods through both theoretical analysis and experimental validation. We finally establish a fine-tuning-centric training paradigm, customizing and implementing a neighborhood self-constrained supervised fine-tuning algorithm. This approach enhances training outcomes by constraining the perturbation magnitude between the model's output distributions before and after training. In practice, we train the Qwen2.5-32B-Instruct model to obtain AnalogSeeker, which achieves 85.04% accuracy on AMSBench-TQA, the analog circuit knowledge evaluation benchmark, with a 15.67% point improvement over the original model and is competitive with mainstream commercial models. Furthermore, AnalogSeeker also shows effectiveness in the downstream operational amplifier design task. AnalogSeeker is open-sourced at https://huggingface.co/analogllm/analogseeker for research use.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "\"Accessibility people, you go work on that thing of yours over there\": Addressing Disability Inclusion in AI Product Organizations",
      "link": "https://arxiv.org/abs/2508.16607",
      "description": "arXiv:2508.16607v2 Announce Type: replace-cross \nAbstract: The rapid emergence of generative AI has changed the way that technology is designed, constructed, maintained, and evaluated. Decisions made when creating AI-powered systems may impact some users disproportionately, such as people with disabilities. In this paper, we report on an interview study with 25 AI practitioners across multiple roles (engineering, research, UX, and responsible AI) about how their work processes and artifacts may impact end users with disabilities. We found that practitioners experienced friction when triaging problems at the intersection of responsible AI and accessibility practices, navigated contradictions between accessibility and responsible AI guidelines, identified gaps in data about users with disabilities, and gathered support for addressing the needs of disabled stakeholders by leveraging informal volunteer and community groups within their company. Based on these findings, we offer suggestions for new resources and process changes to better support people with disabilities as end users of AI.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Activation Transport Operators",
      "link": "https://arxiv.org/abs/2508.17540",
      "description": "arXiv:2508.17540v2 Announce Type: replace-cross \nAbstract: The residual stream mediates communication between transformer decoder layers via linear reads and writes of non-linear computations. While sparse-dictionary learning-based methods locate features in the residual stream, and activation patching methods discover circuits within the model, the mechanism by which features flow through the residual stream remains understudied. Understanding this dynamic can better inform jailbreaking protections, enable early detection of model mistakes, and their correction. In this work, we propose Activation Transport Operators (ATO), linear maps from upstream to downstream residuals $k$ layers later, evaluated in feature space using downstream SAE decoder projections. We empirically demonstrate that these operators can determine whether a feature has been linearly transported from a previous layer or synthesised from non-linear layer computation. We develop the notion of transport efficiency, for which we provide an upper bound, and use it to estimate the size of the residual stream subspace that corresponds to linear transport. We empirically demonstrate the linear transport, report transport efficiency and the size of the residual stream's subspace involved in linear transport. This compute-light (no finetuning, <50 GPU-h) method offers practical tools for safety, debugging, and a clearer picture of where computation in LLMs behaves linearly.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "GDS Agent for Graph Algorithmic Reasoning",
      "link": "https://arxiv.org/abs/2508.20637",
      "description": "arXiv:2508.20637v2 Announce Type: replace-cross \nAbstract: Large language models (LLMs) have shown remarkable multimodal information processing and reasoning ability. When equipped with tools through function calling and enhanced with retrieval-augmented techniques, compound LLM-based systems can access closed data sources and answer questions about them. However, they still struggle to process and reason over large-scale graph-structure data. We introduce the GDS (Graph Data Science) agent in this technical report. The GDS agent introduces a comprehensive set of graph algorithms as tools, together with preprocessing (retrieval) and postprocessing of algorithm results, in a model context protocol (MCP) server. The server can be used with any modern LLM out-of-the-box. GDS agent allows users to ask any question that implicitly and intrinsically requires graph algorithmic reasoning about their data, and quickly obtain accurate and grounded answers. We introduce new benchmarks that evaluate intermediate tool calls as well as final responses. The results indicate that GDS agent is able to solve a wide spectrum of graph tasks. We also provide detailed case studies for more open-ended tasks and study scenarios where the agent struggles. Finally, we discuss the remaining challenges and the future roadmap.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "SME-TEAM: Leveraging Trust and Ethics for Secure and Responsible Use of AI and LLMs in SMEs",
      "link": "https://arxiv.org/abs/2509.10594",
      "description": "arXiv:2509.10594v2 Announce Type: replace-cross \nAbstract: Artificial Intelligence (AI) and Large Language Models (LLMs) are revolutionizing today's business practices; however, their adoption within small and medium-sized enterprises (SMEs) raises serious trust, ethical, and technical issues. In this perspective paper, we introduce a structured, multi-phased framework, \"SME-TEAM\" for the secure and responsible use of these technologies in SMEs. Based on a conceptual structure of four key pillars, i.e., Data, Algorithms, Human Oversight, and Model Architecture, SME-TEAM bridges theoretical ethical principles with operational practice, enhancing AI capabilities across a wide range of applications in SMEs. Ultimately, this paper provides a structured roadmap for the adoption of these emerging technologies, positioning trust and ethics as a driving force for resilience, competitiveness, and sustainable innovation within the area of business analytics and SMEs.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Evaluating Large Language Models for Detecting Antisemitism",
      "link": "https://arxiv.org/abs/2509.18293",
      "description": "arXiv:2509.18293v2 Announce Type: replace-cross \nAbstract: Detecting hateful content is a challenging and important problem. Automated tools, like machine-learning models, can help, but they require continuous training to adapt to the ever-changing landscape of social media. In this work, we evaluate eight open-source LLMs' capability to detect antisemitic content, specifically leveraging in-context definition. We also study how LLMs understand and explain their decisions given a moderation policy as a guideline. First, we explore various prompting techniques and design a new CoT-like prompt, Guided-CoT, and find that injecting domain-specific thoughts increases performance and utility. Guided-CoT handles the in-context policy well, improving performance and utility by reducing refusals across all evaluated models, regardless of decoding configuration, model size, or reasoning capability. Notably, Llama 3.1 70B outperforms fine-tuned GPT-3.5. Additionally, we examine LLM errors and introduce metrics to quantify semantic divergence in model-generated rationales, revealing notable differences and paradoxical behaviors among LLMs. Our experiments highlight the differences observed across LLMs' utility, explainability, and reliability. Code and resources available at: https://github.com/idramalab/quantify-llm-explanations",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "A Unified Formal Theory on the Logical Limits of Symbol Grounding",
      "link": "https://arxiv.org/abs/2509.20409",
      "description": "arXiv:2509.20409v3 Announce Type: replace-cross \nAbstract: This paper synthesizes a series of formal proofs to construct a unified theory on the logical limits of the Symbol Grounding Problem. We demonstrate through a four-stage argument that meaning within a formal system must arise from a process that is external, dynamic, and non-algorithmic. First, we prove that any purely symbolic system, devoid of external connections, cannot internally establish a consistent foundation for meaning due to self-referential paradoxes. Second, we extend this limitation to systems with any finite, static set of pre-established meanings, proving they are inherently incomplete. Third, we demonstrate that the grounding process is logically incomplete; specifically, the 'act' of connecting internal symbols to novel, emergent external meanings cannot be a product of logical inference within the system but must be an axiomatic, meta-level update. Finally, we prove that any attempt to automate this update process using a fixed, external \"judgment\" algorithm will inevitably construct a larger, yet equally incomplete, symbolic system. Together, these conclusions formally establish that the grounding of meaning is a necessarily open-ended, non-algorithmic process, revealing a fundamental, G\\\"odel-style limitation for any self-contained intelligent system.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Automatic Discovery of One-Parameter Subgroups of Lie Groups: Compact and Non-Compact Cases of $\\mathbf{SO(n)}$ and $\\mathbf{SL(n)}$",
      "link": "https://arxiv.org/abs/2509.22219",
      "description": "arXiv:2509.22219v3 Announce Type: replace-cross \nAbstract: We introduce a novel framework for the automatic discovery of one-parameter subgroups ($H_{\\gamma}$) of $SO(3)$ and, more generally, $SO(n)$. One-parameter subgroups of $SO(n)$ are crucial in a wide range of applications, including robotics, quantum mechanics, and molecular structure analysis. Our method utilizes the standard Jordan form of skew-symmetric matrices, which define the Lie algebra of $SO(n)$, to establish a canonical form for orbits under the action of $H_{\\gamma}$. This canonical form is then employed to derive a standardized representation for $H_{\\gamma}$-invariant functions. By learning the appropriate parameters, the framework uncovers the underlying one-parameter subgroup $H_{\\gamma}$. The effectiveness of the proposed approach is demonstrated through tasks such as double pendulum modeling, moment of inertia prediction, top quark tagging and invariant polynomial regression, where it successfully recovers meaningful subgroup structure and produces interpretable, symmetry-aware representations.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "CardioForest: An Explainable Ensemble Learning Model for Automatic Wide QRS Complex Tachycardia Diagnosis from ECG",
      "link": "https://arxiv.org/abs/2509.25804",
      "description": "arXiv:2509.25804v2 Announce Type: replace-cross \nAbstract: This study aims to develop and evaluate an ensemble machine learning-based framework for the automatic detection of Wide QRS Complex Tachycardia (WCT) from ECG signals, emphasizing diagnostic accuracy and interpretability using Explainable AI. The proposed system integrates ensemble learning techniques, i.e., an optimized Random Forest known as CardioForest, and models like XGBoost and LightGBM. The models were trained and tested on ECG data from the publicly available MIMIC-IV dataset. The testing was carried out with the assistance of accuracy, balanced accuracy, precision, recall, F1 score, ROC-AUC, and error rate (RMSE, MAE) measures. In addition, SHAP (SHapley Additive exPlanations) was used to ascertain model explainability and clinical relevance. The CardioForest model performed best on all metrics, achieving a test accuracy of 95.19%, a balanced accuracy of 88.76%, a precision of 95.26%, a recall of 78.42%, and an ROC-AUC of 0.8886. SHAP analysis confirmed the model's ability to rank the most relevant ECG features, such as QRS duration, in accordance with clinical intuitions, thereby fostering trust and usability in clinical practice. The findings recognize CardioForest as an extremely dependable and interpretable WCT detection model. Being able to offer accurate predictions and transparency through explainability makes it a valuable tool to help cardiologists make timely and well-informed diagnoses, especially for high-stakes and emergency scenarios.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Training Optimal Large Diffusion Language Models",
      "link": "https://arxiv.org/abs/2510.03280",
      "description": "arXiv:2510.03280v2 Announce Type: replace-cross \nAbstract: We introduce Quokka, the first systematic scaling law for diffusion language models (DLMs), encompassing both compute-constrained and data-constrained regimes, and studying the key modeling and optimization designs. Quokka is a good friend of Chinchilla and provides wider scopes. We hope the results would bring short-term practical guidance in DLMs training and long-term inspirations for the whole AI community.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Efficient Latent Variable Causal Discovery: Combining Score Search and Targeted Testing",
      "link": "https://arxiv.org/abs/2510.04263",
      "description": "arXiv:2510.04263v3 Announce Type: replace-cross \nAbstract: Learning causal structure from observational data is especially challenging when latent variables or selection bias are present. The Fast Causal Inference (FCI) algorithm addresses this setting but performs exhaustive conditional independence tests across many subsets, often leading to spurious independences, missing or extra edges, and unreliable orientations. We present a family of score-guided mixed-strategy causal search algorithms that extend this framework. First, we introduce BOSS-FCI and GRaSP-FCI, variants of GFCI (Greedy Fast Causal Inference) that substitute BOSS (Best Order Score Search) or GRaSP (Greedy Relaxations of Sparsest Permutation) for FGES (Fast Greedy Equivalence Search), preserving correctness while trading off scalability and conservativeness. Second, we develop FCI Targeted-Testing (FCIT), a novel hybrid method that replaces exhaustive testing with targeted, score-informed tests guided by BOSS. FCIT guarantees well-formed PAGs and achieves higher precision and efficiency across sample sizes. Finally, we propose a lightweight heuristic, LV-Dumb (Latent Variable \"Dumb\"), which returns the PAG of the BOSS DAG (Directed Acyclic Graph). Though not strictly sound for latent confounding, LV-Dumb often matches FCIT's accuracy while running substantially faster. Simulations and real-data analyses show that BOSS-FCI and GRaSP-FCI provide robust baselines, FCIT yields the best balance of precision and reliability, and LV-Dumb offers a fast, near-equivalent alternative. Together, these methods demonstrate that targeted and score-guided strategies can dramatically improve the efficiency and correctness of latent-variable causal discovery.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "DE3S: Dual-Enhanced Soft-Sparse-Shape Learning for Medical Early Time-Series Classification",
      "link": "https://arxiv.org/abs/2510.12214",
      "description": "arXiv:2510.12214v2 Announce Type: replace-cross \nAbstract: Early Time Series Classification (ETSC) is critical in time-sensitive medical applications such as sepsis, yet it presents an inherent trade-off between accuracy and earliness. This trade-off arises from two core challenges: 1) models should effectively model inherently weak and noisy early-stage snippets, and 2) they should resolve the complex, dual requirement of simultaneously capturing local, subject-specific variations and overarching global temporal patterns. Existing methods struggle to overcome these underlying challenges, often forcing a severe compromise: sacrificing accuracy to achieve earliness, or vice-versa. We propose \\textbf{DE3S}, a \\textbf{D}ual-\\textbf{E}nhanced \\textbf{S}oft-\\textbf{S}parse \\textbf{S}equence Learning framework, which systematically solves these challenges. A dual enhancement mechanism is proposed to enhance the modeling of weak, early signals. Then, an attention-based patch module is introduced to preserve discriminative information while reducing noise and complexity. A dual-path fusion architecture is designed, using a sparse mixture of experts to model local, subject-specific variations. A multi-scale inception module is also employed to capture global dependencies. Experiments on six real-world medical datasets show the competitive performance of DE3S, particularly in early prediction windows. Ablation studies confirm the effectiveness of each component in addressing its targeted challenge. The source code is available \\href{https://github.com/kuxit/DE3S}{\\textbf{here}}.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "FaStfact: Faster, Stronger Long-Form Factuality Evaluations in LLMs",
      "link": "https://arxiv.org/abs/2510.12839",
      "description": "arXiv:2510.12839v2 Announce Type: replace-cross \nAbstract: Evaluating the factuality of long-form generations from Large Language Models (LLMs) remains challenging due to efficiency bottlenecks and reliability concerns. Prior efforts attempt this by decomposing text into claims, searching for evidence, and verifying claims, but suffer from critical drawbacks: (1) inefficiency due to overcomplicated pipeline components, and (2) ineffectiveness stemming from inaccurate claim sets and insufficient evidence. To address these limitations, we propose \\textbf{FaStfact}, an evaluation framework that achieves the highest alignment with human evaluation and time/token efficiency among existing baselines. FaStfact first employs chunk-level claim extraction integrated with confidence-based pre-verification, significantly reducing the time and token cost while ensuring reliability. For searching and verification, it collects document-level evidence from crawled web-pages and selectively retrieves it during verification. Extensive experiments based on an annotated benchmark \\textbf{FaStfact-Bench} demonstrate the reliability of FaStfact in both efficiently and effectively evaluating long-form factuality. Code, benchmark data, and annotation interface tool are available at https://github.com/Yingjia-Wan/FaStfact.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "A Survey on Collaborating Small and Large Language Models for Performance, Cost-effectiveness, Cloud-edge Privacy, and Trustworthiness",
      "link": "https://arxiv.org/abs/2510.13890",
      "description": "arXiv:2510.13890v2 Announce Type: replace-cross \nAbstract: Large language models (LLMs) have achieved remarkable progress across domains and applications but face challenges such as high fine-tuning costs, inference latency, limited edge deployability, and reliability concerns. Small language models (SLMs), with compact, efficient, and adaptable features, offer promising solutions. Building on this potential, recent research explores collaborative frameworks that integrate their complementary strengths, leveraging SLMs' specialization and efficiency with LLMs' generalization and reasoning to address diverse objectives across tasks and deployment scenarios. Motivated by these developments, this paper presents a systematic survey of SLM-LLM collaboration from the perspective of collaboration objectives. We propose a taxonomy covering four goals: performance enhancement, cost-effectiveness, cloud-edge privacy, and trustworthiness. Under this framework, we review representative methods, summarize design paradigms, and outline open challenges and future directions toward efficient and secure SLM-LLM collaboration. The collected papers are available at https://github.com/FairyFali/SLMs-Survey.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "ADPO: Anchored Direct Preference Optimization",
      "link": "https://arxiv.org/abs/2510.18913",
      "description": "arXiv:2510.18913v4 Announce Type: replace-cross \nAbstract: Direct Preference Optimization (DPO) has become a standard for aligning models with human feedback, yet its reliance on hard, pairwise preferences makes it brittle to annotator noise and distribution shift. We propose Anchored Direct Preference Optimization (ADPO), a theoretically grounded framework that extends preference learning to soft, listwise supervision through reference anchoring. Our key theoretical contributions are threefold: (1) we establish that ADPO unifies major learning paradigms, including supervised fine-tuning, knowledge distillation, maximum-entropy reinforcement learning, and DPO, as special cases through different choices of target distribution, anchor policy, and temperature; (2) we prove that anchoring induces an implicit trust region governed by the softmax Fisher metric; and (3) we formalize the stability of dynamic anchor updates. Empirically, we discover a task-dependent tradeoff: dynamic anchors suit online exploration, while fixed anchors excel at offline distillation, reducing teacher-student KL divergence by two to three orders of magnitude (170 to 5000 times).",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "StutterZero and StutterFormer: End-to-End Speech Conversion for Stuttering Transcription and Correction",
      "link": "https://arxiv.org/abs/2510.18938",
      "description": "arXiv:2510.18938v2 Announce Type: replace-cross \nAbstract: Over 70 million people worldwide experience stuttering, yet most automatic speech systems misinterpret disfluent utterances or fail to transcribe them accurately. Existing methods for stutter correction rely on handcrafted feature extraction or multi-stage automatic speech recognition (ASR) and text-to-speech (TTS) pipelines, which separate transcription from audio reconstruction and often amplify distortions. This work introduces StutterZero and StutterFormer, the first end-to-end waveform-to-waveform models that directly convert stuttered speech into fluent speech while jointly predicting its transcription. StutterZero employs a convolutional-bidirectional LSTM encoder-decoder with attention, whereas StutterFormer integrates a dual-stream Transformer with shared acoustic-linguistic representations. Both architectures are trained on paired stuttered-fluent data synthesized from the SEP-28K and LibriStutter corpora and evaluated on unseen speakers from the FluencyBank dataset. Across all benchmarks, StutterZero had a 24% decrease in Word Error Rate (WER) and a 31% improvement in semantic similarity (BERTScore) compared to the leading Whisper-Medium model. StutterFormer achieved better results, with a 28% decrease in WER and a 34% improvement in BERTScore. The results validate the feasibility of direct end-to-end stutter-to-fluent speech conversion, offering new opportunities for inclusive human-computer interaction, speech therapy, and accessibility-oriented AI systems.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "AgenticMath: Enhancing LLM Reasoning via Agentic-based Math Data Generation",
      "link": "https://arxiv.org/abs/2510.19361",
      "description": "arXiv:2510.19361v2 Announce Type: replace-cross \nAbstract: The creation of high-quality datasets to improve Large Language Model (LLM) reasoning remains a significant challenge, as current methods often suffer from generating low-quality/incorrect answers and limited information richness from available data sources. To address this, we propose AgenticMath, a novel agentic pipeline for generating high-quality mathematical question-answer pairs to enhance the supervised fine-tuning of LLMs. Our method operates through four stages: (1) Seed Question Filter that selects questions with high information richness, complexity, and clarity; (2) an Agentic Question Rephrase step that employs a multi-agent system to generate diverse, logically consistent paraphrases; (3) an Answer Augment step where rewrite answers using chain-of-thought reasoning to enhance numerical and logical correctness, without reliance on human-provided labels; and (4) a final Question and Answer Evaluation that retains only the most superior pairs. Extensive experiments demonstrate that, fine-tuning 3B-8B parameter LLMs on AgenticMath generated datasets (comprising only 30-60K math samples) achieves competitive or superior performance on diverse in domain and out-of-domain mathematical reasoning benchmarks compared to baselines trained on much more data (e.g., 400K or 2.3M samples). Our work demonstrates that targeted, high-quality data generation is a more efficient path to improving mathematical reasoning in LLMs than large-scale, low-quality alternatives.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "A Foundational Theory of Quantitative Abstraction: Adjunctions, Duality, and Logic for Probabilistic Systems",
      "link": "https://arxiv.org/abs/2510.19444",
      "description": "arXiv:2510.19444v2 Announce Type: replace-cross \nAbstract: The analysis and control of stochastic dynamical systems rely on probabilistic models such as (continuous-space) Markov decision processes, but large or continuous state spaces make exact analysis intractable and call for principled quantitative abstraction. This work develops a unified theory of such abstraction by integrating category theory, coalgebra, quantitative logic, and optimal transport, centred on a canonical $\\varepsilon$-quotient of the behavioral pseudo-metric with a universal property: among all abstractions that collapse behavioral differences below $\\varepsilon$, it is the most detailed, and every other abstraction achieving the same discounted value-loss guarantee factors uniquely through it. Categorically, a quotient functor $Q_\\varepsilon$ from a category of probabilistic systems to a category of metric specifications admits, via the Special Adjoint Functor Theorem, a right adjoint $R_\\varepsilon$, yielding an adjunction $Q_\\varepsilon \\dashv R_\\varepsilon$ that formalizes a duality between abstraction and realization; logically, a quantitative modal $\\mu$-calculus with separate reward and transition modalities is shown, for a broad class of systems, to be expressively complete for the behavioral pseudo-metric, with a countable fully abstract fragment suitable for computation. The theory is developed coalgebraically over Polish spaces and the Giry monad and validated on finite-state models using optimal-transport solvers, with experiments corroborating the predicted contraction properties and structural stability and aligning with the theoretical value-loss bounds, thereby providing a rigorous foundation for quantitative state abstraction and representation learning in probabilistic domains.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "The Mirror Loop: Recursive Non-Convergence in Generative Reasoning Systems",
      "link": "https://arxiv.org/abs/2510.21861",
      "description": "arXiv:2510.21861v2 Announce Type: replace-cross \nAbstract: Large language models are often described as capable of reflective reasoning, yet recursive self-evaluation without external feedback frequently yields reformulation rather than progress. We test this prediction in a cross-provider study of 144 reasoning sequences across three models (OpenAI GPT-4o-mini, Anthropic Claude 3 Haiku, and Google Gemini 2.0 Flash) and four task families (arithmetic, code, explanation, reflection), each iterated ten times under two conditions: ungrounded self-critique and a minimal grounding intervention (a single verification step at iteration three). Mean informational change (delta I, measured via normalized edit distance) declined by 55% from early (0.193) to late (0.087) iterations in ungrounded runs, with consistent patterns across all three providers. Grounded runs showed a +28% rebound in informational change immediately after the intervention and sustained non-zero variance thereafter. Complementary measures-n-gram novelty, embedding drift, and character-level entropy-converged on the same pattern: reflection without contact tends toward informational closure. We interpret this as evidence for a structural limit on self-correction in generative reasoning: without an exchange of information with an independent verifier or environment, recursive inference approaches an attractor state of epistemic stasis. Minimal grounding functions as dissipative coupling, reintroducing informational flux. The cross-architecture consistency suggests the mirror loop arises from shared autoregressive training objectives rather than provider-specific alignment schemes. The results delineate when reflection is performative rather than epistemic and motivate design principles for grounded, cooperative reasoning. Materials and code are publicly available.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Enabling Robust In-Context Memory and Rapid Task Adaptation in Transformers with Hebbian and Gradient-Based Plasticity",
      "link": "https://arxiv.org/abs/2510.21908",
      "description": "arXiv:2510.21908v2 Announce Type: replace-cross \nAbstract: Large language models display in-context learning as an emergent effect of scale, but they rely on static weights during inference. In contrast, biological systems continually adapt via synaptic plasticity. We investigate whether explicit, biologically inspired plasticity can endow Transformers with faster in-sequence adaptation. To this end, we augment decoder-only Transformers with fast-weight modules updated either by (i) a neuromodulated Hebbian rule or (ii) the gradient-based plasticity mechanism of Duan et al. (2023). Across copying, regression, and few-shot classification tasks (CIFAR-FS, Omniglot), Hebbian plasticity consistently achieves lower loss and stronger few-shot generalization, while gradient-based updates perform best on long-horizon credit assignment. When associations are short and linearly separable, static weights suffice, defining a clear boundary condition for when plasticity helps. Analysis of learned modulatory signals reveals that gradient-based rules maintain large, persistent updates, whereas Hebbian plasticity is sharply gated around salient events. Together, these results show that explicit plasticity complements attention by enabling rapid, task-specific adaptation, and clarify when different plasticity mechanisms are most effective.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Toward Humanoid Brain-Body Co-design: Joint Optimization of Control and Morphology for Fall Recovery",
      "link": "https://arxiv.org/abs/2510.22336",
      "description": "arXiv:2510.22336v2 Announce Type: replace-cross \nAbstract: Humanoid robots represent a central frontier in embodied intelligence, as their anthropomorphic form enables natural deployment in humans' workspace. Brain-body co-design for humanoids presents a promising approach to realizing this potential by jointly optimizing control policies and physical morphology. Within this context, fall recovery emerges as a critical capability. It not only enhances safety and resilience but also integrates naturally with locomotion systems, thereby advancing the autonomy of humanoids. In this paper, we propose RoboCraft, a scalable humanoid co-design framework for fall recovery that iteratively improves performance through the coupled updates of control policy and morphology. A shared policy pretrained across multiple designs is progressively finetuned on high-performing morphologies, enabling efficient adaptation without retraining from scratch. Concurrently, morphology search is guided by human-inspired priors and optimization algorithms, supported by a priority buffer that balances reevaluation of promising candidates with the exploration of novel designs. Experiments show that RoboCraft achieves an average performance gain of 44.55% on seven public humanoid robots, with morphology optimization drives at least 40% of improvements in co-designing four humanoid robots, underscoring the critical role of humanoid co-design.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "LLMComp: A Language Modeling Paradigm for Error-Bounded Scientific Data Compression (Technical Report)",
      "link": "https://arxiv.org/abs/2510.23632",
      "description": "arXiv:2510.23632v2 Announce Type: replace-cross \nAbstract: The rapid growth of high-resolution scientific simulations and observation systems is generating massive spatiotemporal datasets, making efficient, error-bounded compression increasingly important. Meanwhile, decoder-only large language models (LLMs) have demonstrated remarkable capabilities in modeling complex sequential data. In this paper, we propose LLMCOMP, a novel lossy compression paradigm that leverages decoder-only large LLMs to model scientific data. LLMCOMP first quantizes 3D fields into discrete tokens, arranges them via Z-order curves to preserve locality, and applies coverage-guided sampling to enhance training efficiency. An autoregressive transformer is then trained with spatial-temporal embeddings to model token transitions. During compression, the model performs top-k prediction, storing only rank indices and fallback corrections to ensure strict error bounds. Experiments on multiple reanalysis datasets show that LLMCOMP consistently outperforms state-of-the-art compressors, achieving up to 30% higher compression ratios under strict error bounds. These results highlight the potential of LLMs as general-purpose compressors for high-fidelity scientific data.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Using latent representations to link disjoint longitudinal data for mixed-effects regression",
      "link": "https://arxiv.org/abs/2510.25531",
      "description": "arXiv:2510.25531v2 Announce Type: replace-cross \nAbstract: Many rare diseases offer limited established treatment options, leading patients to switch therapies when new medications emerge. To analyze the impact of such treatment switches within the low sample size limitations of rare disease trials, it is important to use all available data sources. This, however, is complicated when usage of measurement instruments change during the observation period, for example when instruments are adapted to specific age ranges. The resulting disjoint longitudinal data trajectories, complicate the application of traditional modeling approaches like mixed-effects regression. We tackle this by mapping observations of each instrument to a aligned low-dimensional temporal trajectory, enabling longitudinal modeling across instruments. Specifically, we employ a set of variational autoencoder architectures to embed item values into a shared latent space for each time point. Temporal disease dynamics and treatment switch effects are then captured through a mixed-effects regression model applied to latent representations. To enable statistical inference, we present a novel statistical testing approach that accounts for the joint parameter estimation of mixed-effects regression and variational autoencoders. The methodology is applied to quantify the impact of treatment switches for patients with spinal muscular atrophy. Here, our approach aligns motor performance items from different measurement instruments for mixed-effects regression and maps estimated effects back to the observed item level to quantify the treatment switch effect. Our approach allows for model selection as well as for assessing effects of treatment switching. The results highlight the potential of modeling in joint latent representations for addressing small data challenges.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "WOD-E2E: Waymo Open Dataset for End-to-End Driving in Challenging Long-tail Scenarios",
      "link": "https://arxiv.org/abs/2510.26125",
      "description": "arXiv:2510.26125v2 Announce Type: replace-cross \nAbstract: Vision-based end-to-end (E2E) driving has garnered significant interest in the research community due to its scalability and synergy with multimodal large language models (MLLMs). However, current E2E driving benchmarks primarily feature nominal scenarios, failing to adequately test the true potential of these systems. Furthermore, existing open-loop evaluation metrics often fall short in capturing the multi-modal nature of driving or effectively evaluating performance in long-tail scenarios. To address these gaps, we introduce the Waymo Open Dataset for End-to-End Driving (WOD-E2E). WOD-E2E contains 4,021 driving segments (approximately 12 hours), specifically curated for challenging long-tail scenarios that that are rare in daily life with an occurring frequency of less than 0.03%. Concretely, each segment in WOD-E2E includes the high-level routing information, ego states, and 360-degree camera views from 8 surrounding cameras. To evaluate the E2E driving performance on these long-tail situations, we propose a novel open-loop evaluation metric: Rater Feedback Score (RFS). Unlike conventional metrics that measure the distance between predicted way points and the logs, RFS measures how closely the predicted trajectory matches rater-annotated trajectory preference labels. We have released rater preference labels for all WOD-E2E validation set segments, while the held out test set labels have been used for the 2025 WOD-E2E Challenge. Through our work, we aim to foster state of the art research into generalizable, robust, and safe end-to-end autonomous driving agents capable of handling complex real-world situations.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Beyond Synthetic Benchmarks: Evaluating LLM Performance on Real-World Class-Level Code Generation",
      "link": "https://arxiv.org/abs/2510.26130",
      "description": "arXiv:2510.26130v2 Announce Type: replace-cross \nAbstract: Large language models (LLMs) have demonstrated strong performance on function-level code generation benchmarks, yet real-world software development increasingly demands class-level implementations that integrate multiple methods, attributes, and dependencies within authentic project contexts. This gap between benchmark performance and practical utility raises critical questions about LLMs' readiness for production code assistance, particularly regarding their ability to generalize across familiar and novel codebases.\n  We introduce a benchmark derived from real-world open-source repositories, comprising classes divided into seen and unseen partitions to evaluate generalization under practical conditions. We systematically examine how input specification completeness and retrieval-augmented generation affect class-level correctness across multiple state-of-the-art LLMs.\n  Our evaluation reveals a substantial performance gap: while LLMs achieve 84 to 89% correctness on synthetic benchmarks, they attain only 25 to 34% on real-world class tasks, with minimal distinction between familiar and novel codebases. Comprehensive documentation provides marginal improvements (1 to 3%), whereas retrieval augmentation yields greater gains (4 to 7%) by supplying concrete implementation patterns. Error analysis identifies AttributeError, TypeError, and AssertionError as dominant failure modes, with distinct patterns between synthetic and real-world scenarios.\n  These findings provide actionable insights for enhancing context modelling, documentation strategies, and retrieval integration in production code assistance tools.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "On Improvisation and Open-Endedness: Insights for Experiential AI",
      "link": "https://arxiv.org/abs/2511.00529",
      "description": "arXiv:2511.00529v2 Announce Type: replace-cross \nAbstract: Improvisation-the art of spontaneous creation that unfolds moment-to-moment without a scripted outcome-requires practitioners to continuously sense, adapt, and create anew. It is a fundamental mode of human creativity spanning music, dance, and everyday life. The open-ended nature of improvisation produces a stream of novel, unrepeatable moments-an aspect highly valued in artistic creativity. In parallel, open-endedness (OE)-a system's capacity for unbounded novelty and endless \"interestingness\"-is exemplified in natural or cultural evolution and has been considered \"the last grand challenge\" in artificial life (ALife). The rise of generative AI now raises the question in computational creativity (CC) research: What makes a \"good\" improvisation for AI? Can AI learn to improvise in a genuinely open-ended way? In this work-in-progress paper, we report insights from in-depth interviews with 6 experts in improvisation across dance, music, and contact improvisation. We draw systemic connections between human improvisational arts and the design of future experiential AI agents that could improvise alone or alongside humans-or even with other AI agents-embodying qualities of improvisation drawn from practice: active listening (umwelt and awareness), being in the time (mindfulness and ephemerality), embracing the unknown (source of randomness and serendipity), non-judgmental flow (acceptance and dynamical stability, balancing structure and surprise (unpredictable criticality at edge of chaos), imaginative metaphor (synaesthesia and planning), empathy, trust, boundary, and care (mutual theory of mind), and playfulness and intrinsic motivation (maintaining interestingness).",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Quantifying truth and authenticity in AI-assisted candidate evaluation: A multi-domain pilot analysis",
      "link": "https://arxiv.org/abs/2511.00774",
      "description": "arXiv:2511.00774v2 Announce Type: replace-cross \nAbstract: This paper presents a retrospective analysis of anonymized candidate-evaluation data collected during pilot hiring campaigns conducted through AlteraSF, an AI-native resume-verification platform. The system evaluates resume claims, generates context-sensitive verification questions, and measures performance along quantitative axes of factual validity and job fit, complemented by qualitative integrity detection. Across six job families and 1,700 applications, the platform achieved a 90-95% reduction in screening time and detected measurable linguistic patterns consistent with AI-assisted or copied responses. The analysis demonstrates that candidate truthfulness can be assessed not only through factual accuracy but also through patterns of linguistic authenticity. The results suggest that a multi-dimensional verification framework can improve both hiring efficiency and trust in AI-mediated evaluation systems.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "HAFixAgent: History-Aware Automated Program Repair Agent",
      "link": "https://arxiv.org/abs/2511.01047",
      "description": "arXiv:2511.01047v2 Announce Type: replace-cross \nAbstract: Automated program repair (APR) has recently shifted toward large language models and agent-based systems, yet most systems rely on local snapshot context, overlooking repository history. Prior work shows that repository history helps repair single-line bugs, since the last commit touching the buggy line is often the bug-introducing one. In this paper, we investigate whether repository history can also improve agentic APR systems at scale, especially for complex multi-hunk bugs. We present HAFixAgent, a History-Aware Bug-Fixing Agent that injects blame-derived repository heuristics into its repair loop. A preliminary study of all 854 real-world bugs from Defects4J motivates our design, showing that bug-relevant history is both widely available and highly concentrated. Empirical comparison of HAFixAgent with two state-of-the-art baselines shows: (1) Effectiveness: HAFixAgent significantly improves over the agent-based baseline (by 212.3%) and the multi-hunk baseline (by 29.9%). (2) Efficiency: history does not significantly increase agent steps and keeps token costs comparable, with notably lower median costs for complex multi-file-multi-hunk bugs. (3) Practicality: combining different historical heuristics repairs more bugs, offering a clear cost-benefit trade-off. HAFixAgent offers a practical recipe for history-aware agentic APR: ground the agent in version control history, prioritize diff-based historical context, and integrate complementary heuristics when needed.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "AI for Requirements Engineering: Industry adoption and Practitioner perspectives",
      "link": "https://arxiv.org/abs/2511.01324",
      "description": "arXiv:2511.01324v3 Announce Type: replace-cross \nAbstract: The integration of AI for Requirements Engineering (RE) presents significant benefits but also poses real challenges. Although RE is fundamental to software engineering, limited research has examined AI adoption in RE. We surveyed 55 software practitioners to map AI usage across four RE phases: Elicitation, Analysis, Specification, and Validation, and four approaches for decision making: human-only decisions, AI validation, Human AI Collaboration (HAIC), and full AI automation. Participants also shared their perceptions, challenges, and opportunities when applying AI for RE tasks. Our data show that 58.2% of respondents already use AI in RE, and 69.1% view its impact as positive or very positive. HAIC dominates practice, accounting for 54.4% of all RE techniques, while full AI automation remains minimal at 5.4%. Passive AI validation (4.4 to 6.2%) lags even further behind, indicating that practitioners value AI's active support over passive oversight. These findings suggest that AI is most effective when positioned as a collaborative partner rather than a replacement for human expertise. It also highlights the need for RE-specific HAIC frameworks along with robust and responsible AI governance as AI adoption in RE grows.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Reg-DPO: SFT-Regularized Direct Preference Optimization with GT-Pair for Improving Video Generation",
      "link": "https://arxiv.org/abs/2511.01450",
      "description": "arXiv:2511.01450v2 Announce Type: replace-cross \nAbstract: Recent studies have identified Direct Preference Optimization (DPO) as an efficient and reward-free approach to improving video generation quality. However, existing methods largely follow image-domain paradigms and are mainly developed on small-scale models (approximately 2B parameters), limiting their ability to address the unique challenges of video tasks, such as costly data construction, unstable training, and heavy memory consumption. To overcome these limitations, we introduce a GT-Pair that automatically builds high-quality preference pairs by using real videos as positives and model-generated videos as negatives, eliminating the need for any external annotation. We further present Reg-DPO, which incorporates the SFT loss as a regularization term into the DPO loss to enhance training stability and generation fidelity. Additionally, by combining the FSDP framework with multiple memory optimization techniques, our approach achieves nearly three times higher training capacity than using FSDP alone. Extensive experiments on both I2V and T2V tasks across multiple datasets demonstrate that our method consistently outperforms existing approaches, delivering superior video generation quality.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "CudaForge: An Agent Framework with Hardware Feedback for CUDA Kernel Optimization",
      "link": "https://arxiv.org/abs/2511.01884",
      "description": "arXiv:2511.01884v2 Announce Type: replace-cross \nAbstract: Developing efficient CUDA kernels is increasingly critical for AI applications such as large-scale LLM training. However, manual kernel design is both costly and time-consuming, motivating automatic approaches that leverage LLMs for code generation. Existing methods for automatic kernel generation, however, often produce low-efficiency kernels, incur high computational overhead, and fail to generalize across settings. In this work, we propose CudaForge, a training-free multi-agent workflow for CUDA kernel generation and optimization. Our workflow is inspired by the iterative workflow of human experts, which contains steps such as developing initial kernels, testing correctness, analyzing hardware feedback, and iterative improvement. More specifically, CudaForge employs two LLM agents: a Coder and a Judge, that iteratively generate, correct, and optimize CUDA kernels, while integrating hardware feedback such as Nsight Compute (NCU) metrics. In extensive evaluations, we show that CudaForge, by leveraging base models like OpenAI-o3, achieves 97.6\\% correctness of generated kernels and an average 1.68$\\times$ speedup over PyTorch baselines, substantially surpassing state-of-the-art models including OpenAI-o3 and Kevin on KernelBench.Beyond accuracy and speed, CudaForge demonstrates strong generalization across GPUs (A100, RTX 6000, 4090, 3090) and base models (OpenAI-o3, GPT-5, gpt-oss-120B, Claude-Sonnet-4, QwQ-32B), while maintaining high efficiency. In particular, generating an optimized kernel takes about 26.5 minutes on one RTX6000 and incurs about \\$ 0.3 API cost, which is significantly cheaper than existing agentic work that costs 6 H100 hours and \\$ 5 API cost per kernel. Our results highlight that multi-agent, training-free workflows can enable cost-effective, generalizable, and high-performance CUDA kernel optimization. Code available at https://github.com/OptimAI-Lab/CudaForge",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "LA-MARRVEL: A Knowledge-Grounded and Language-Aware LLM Reranker for AI-MARRVEL in Rare Disease Diagnosis",
      "link": "https://arxiv.org/abs/2511.02263",
      "description": "arXiv:2511.02263v2 Announce Type: replace-cross \nAbstract: Diagnosing rare diseases often requires connecting variant-bearing genes to evidence that is written as unstructured clinical prose, which the current established pipelines still leave for clinicians to reconcile manually. To this end, we introduce LA-MARRVEL, a knowledge-grounded and language-aware reranking layer that operates on top of AI-MARRVEL: it supplies expert-engineered context, queries a large language model multiple times, and aggregates the resulting partial rankings with a ranked voting method to produce a stable, explainable gene ranking. Evaluated on three real-world cohorts (BG, DDD, UDN), LA-MARRVEL consistently improves Recall@K over AI-MARRVEL and established phenotype-driven tools such as Exomiser and LIRICAL, with especially large gains on cases where the first-stage ranker placed the causal gene lower. Each ranked gene is accompanied by LLM-generated reasoning that integrates phenotypic, inheritance, and variant-level evidence, thereby making the output more interpretable and facilitating clinical review.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "In Situ Training of Implicit Neural Compressors for Scientific Simulations via Sketch-Based Regularization",
      "link": "https://arxiv.org/abs/2511.02659",
      "description": "arXiv:2511.02659v2 Announce Type: replace-cross \nAbstract: Focusing on implicit neural representations, we present a novel in situ training protocol that employs limited memory buffers of full and sketched data samples, where the sketched data are leveraged to prevent catastrophic forgetting. The theoretical motivation for our use of sketching as a regularizer is presented via a simple Johnson-Lindenstrauss-informed result. While our methods may be of wider interest in the field of continual learning, we specifically target in situ neural compression using implicit neural representation-based hypernetworks. We evaluate our method on a variety of complex simulation data in two and three dimensions, over long time horizons, and across unstructured grids and non-Cartesian geometries. On these tasks, we show strong reconstruction performance at high compression rates. Most importantly, we demonstrate that sketching enables the presented in situ scheme to approximately match the performance of the equivalent offline method.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Scalable Evaluation and Neural Models for Compositional Generalization",
      "link": "https://arxiv.org/abs/2511.02667",
      "description": "arXiv:2511.02667v2 Announce Type: replace-cross \nAbstract: Compositional generalization-a key open challenge in modern machine learning-requires models to predict unknown combinations of known concepts. However, assessing compositional generalization remains a fundamental challenge due to the lack of standardized evaluation protocols and the limitations of current benchmarks, which often favor efficiency over rigor. At the same time, general-purpose vision architectures lack the necessary inductive biases, and existing approaches to endow them compromise scalability. As a remedy, this paper introduces: 1) a rigorous evaluation framework that unifies and extends previous approaches while reducing computational requirements from combinatorial to constant; 2) an extensive and modern evaluation on the status of compositional generalization in supervised vision backbones, training more than 5000 models; 3) Attribute Invariant Networks, a class of models establishing a new Pareto frontier in compositional generalization, achieving a 23.43% accuracy improvement over baselines while reducing parameter overhead from 600% to 16% compared to fully disentangled counterparts. Our code is available at https://github.com/IBM/scalable-compositional-generalization.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "TabTune: A Unified Library for Inference and Fine-Tuning Tabular Foundation Models",
      "link": "https://arxiv.org/abs/2511.02802",
      "description": "arXiv:2511.02802v2 Announce Type: replace-cross \nAbstract: Tabular foundation models represent a growing paradigm in structured data learning, extending the benefits of large-scale pretraining to tabular domains. However, their adoption remains limited due to heterogeneous preprocessing pipelines, fragmented APIs, inconsistent fine-tuning procedures, and the absence of standardized evaluation for deployment-oriented metrics such as calibration and fairness. We present TabTune, a unified library that standardizes the complete workflow for tabular foundation models through a single interface. TabTune provides consistent access to seven state-of-the-art models supporting multiple adaptation strategies, including zero-shot inference, meta-learning, supervised fine-tuning (SFT), and parameter-efficient fine-tuning (PEFT). The framework automates model-aware preprocessing, manages architectural heterogeneity internally, and integrates evaluation modules for performance, calibration, and fairness. Designed for extensibility and reproducibility, TabTune enables consistent benchmarking of adaptation strategies of tabular foundation models.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv AI",
      "sourceUrl": "http://export.arxiv.org/rss/cs.AI",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Improving Structured Outputs in the Gemini API",
      "link": "https://blog.google/technology/developers/gemini-api-structured-outputs/",
      "description": "Today, we're announcing enhancements to Structured Outputs in the Gemini API.",
      "pubDate": "Wed, 05 Nov 2025 17:00:00 +0000",
      "source": "Google AI Blog",
      "sourceUrl": "https://blog.google/technology/ai/rss/",
      "credibility": 0.95,
      "category": "company_official"
    }
  ],
  "ai_chips": [
    {
      "title": "Balancing Leakage Reduction with Correctness Preservation in RTL Code Generation (Univ. of Central Florida)",
      "link": "https://semiengineering.com/balancing-leakage-reduction-with-correctness-preservation-in-rtl-code-generation-univ-of-central-florida/",
      "description": "A new technical paper titled â€œCircuitGuard: Mitigating LLM Memorization in RTL Code Generation Against IP Leakageâ€ was published by researchers at University of Central Florida. Abstract â€œLarge Language Models (LLMs) have achieved remarkable success in generative tasks, including register-transfer level (RTL) hardware synthesis. However, their tendency to memorize training data poses critical risks when proprietary... Â» read more\nThe post Balancing Leakage Reduction with Correctness Preservation in RTL Code Generation (Univ. of Central Florida) appeared first on Semiconductor Engineering.",
      "pubDate": "Fri, 07 Nov 2025 01:00:08 +0000",
      "source": "Semiconductor Engineering",
      "sourceUrl": "https://semiengineering.com/feed/",
      "credibility": 0.85,
      "category": "tech_news"
    },
    {
      "title": "GTA 6 delayed again, this time to November 2026 â€” Rockstar says extra time needed to deliver quality that players have 'come to expect and deserve'",
      "link": "https://www.tomshardware.com/video-games/gta-6-delayed-again-this-time-to-november-2026-rockstar-says-extra-time-needed-to-deliver-quality-that-players-have-come-to-expect-and-deserve",
      "description": "Rockstar has just announced that perhaps the most highly anticipated game in history has been delayed once again. Originally set to release on May 26, 2026, GTA VI will now hit consoles on November 19, 2026 â€” marking a six-month delay, similar to initial pushback from Fall 2025.",
      "pubDate": "Thu, 06 Nov 2025 21:44:13 +0000",
      "source": "Tom's Hardware",
      "sourceUrl": "https://www.tomshardware.com/feeds/all",
      "credibility": 0.85,
      "category": "tech_news"
    },
    {
      "title": "AI RTL Generation versus AI RTL Verification",
      "link": "https://semiwiki.com/artificial-intelligence/362941-ai-rtl-generation-versus-ai-rtl-verification/",
      "description": "I should admit up front that I donâ€™t have a scientific answer to this comparison, but I do have a reasonably informed gut feel, at least for the near-term. The reason I ask the question is that automated RTL generation grabs headlines with visions of designing chips through natural language prompts, making design widely accessible.â€¦ Read More \nThe post AI RTL Generation versus AI RTL Verification appeared first on SemiWiki.",
      "pubDate": "Thu, 06 Nov 2025 18:00:08 +0000",
      "source": "SemiWiki",
      "sourceUrl": "https://semiwiki.com/feed/",
      "credibility": 0.85,
      "category": "tech_news"
    },
    {
      "title": "Apple's latest MacBook Air, MacBook Pro, and Mac Mini are on sale ahead of Black Friday â€” several models drop to all-time low prices",
      "link": "https://www.tomshardware.com/laptops/macbooks/apples-latest-macbook-air-macbook-pro-and-mac-mini-are-on-sale-ahead-of-black-friday-several-models-drop-to-all-time-low-prices",
      "description": "Is now the time to pick up a new MacBook with an M4 inside? Apple's iconic laptops are at their lowest-ever prices ahead of Black Friday",
      "pubDate": "Thu, 06 Nov 2025 17:52:59 +0000",
      "source": "Tom's Hardware",
      "sourceUrl": "https://www.tomshardware.com/feeds/all",
      "credibility": 0.85,
      "category": "tech_news"
    },
    {
      "title": "This $234.99 Intel Arc B580 is a brilliant budget-friendly graphics card upgrade â€“ 12GB of VRAM unlocks 1440p gaming, with Battlefield 6 thrown in for free",
      "link": "https://www.tomshardware.com/pc-components/gpus/this-usd234-99-intel-arc-b580-is-a-brilliant-budget-friendly-graphics-card-upgrade-12gb-of-vram-unlocks-1440p-gaming-with-battlefield-6-thrown-in-for-free",
      "description": "Save 25% on this Intel Arc B580 graphics card, fit for 1440p gaming on a budget.",
      "pubDate": "Thu, 06 Nov 2025 16:56:08 +0000",
      "source": "Tom's Hardware",
      "sourceUrl": "https://www.tomshardware.com/feeds/all",
      "credibility": 0.85,
      "category": "tech_news"
    },
    {
      "title": "AMD's budget Ryzen 5 7500X3D leaks out in early benchmarks, scores hint at performance on par with existing 7600X3D â€” budget offering could pack a punch in both single and multi-core tests",
      "link": "https://www.tomshardware.com/pc-components/cpus/amds-budget-ryzen-5-7500x3d-leaks-out-in-early-benchmarks-scores-hint-at-performance-on-par-with-existing-7600x3d-budget-offering-could-pack-a-punch-in-both-single-and-multi-core-tests",
      "description": "AMD's rumored upcoming budget CPU, the Ryzen 5 7500X3D, has just been spotted on GeekBench with solid single-core and multi-core results that put it on par with the 7600X3D, while having the same 96 MB of L3 cache. Unfortunately, for value-conscious gamers, the price of DDR5 right now might turn you away from the cheapest X3D chip on the AM5 platform.",
      "pubDate": "Thu, 06 Nov 2025 16:08:27 +0000",
      "source": "Tom's Hardware",
      "sourceUrl": "https://www.tomshardware.com/feeds/all",
      "credibility": 0.85,
      "category": "tech_news"
    },
    {
      "title": "As expected, Nvidia's $3,999 mini AI supercomputer is terrible for gaming â€” DGX Spark struggles to hit 50 FPS at 1080p on medium settings in Cyberpunk 2077",
      "link": "https://www.tomshardware.com/video-games/pc-gaming/as-expected-nvidias-usd3-999-mini-ai-supercomputer-is-terrible-for-gaming-dgx-spark-struggles-to-hit-50-fps-at-1080p-on-medium-settings-in-cyberpunk-2077",
      "description": "Redditor puts the DGX Spark to the test in gaming, specifically CD Projekt Red's Cyberpunk 2077.",
      "pubDate": "Thu, 06 Nov 2025 15:59:21 +0000",
      "source": "Tom's Hardware",
      "sourceUrl": "https://www.tomshardware.com/feeds/all",
      "credibility": 0.85,
      "category": "tech_news"
    },
    {
      "title": "Google deploys new Axion CPUs and seventh-gen Ironwood TPU â€” training and inferencing pods beat Nvidia GB300 and shape 'AI Hypercomputer' model",
      "link": "https://www.tomshardware.com/tech-industry/artificial-intelligence/google-deploys-new-axion-cpus-and-seventh-gen-ironwood-tpu-training-and-inferencing-pods-beat-nvidia-gb300-and-shape-ai-hypercomputer-model",
      "description": "Google Cloud has launched new Axion CPU and Ironwood TPU instances that combine Arm-based general-purpose computing with 7th-generation AI acceleration. Ironwood-based pods with up to 9,216 chips and 42.5 FP8 ExaFLOPS per pod vastly surpass Nvidia's GB300 systems and form the foundation of Google's AI Hypercomputer for large-scale model training and inference.",
      "pubDate": "Thu, 06 Nov 2025 15:03:18 +0000",
      "source": "Tom's Hardware",
      "sourceUrl": "https://www.tomshardware.com/feeds/all",
      "credibility": 0.85,
      "category": "tech_news"
    },
    {
      "title": "$5,000 Corsair pre-built keeps on frying Intel CPUs due to lack of BIOS update, tech alleges â€” kills three Intel Core i9 chips because latest version still doesnâ€™t have fix for Intel crashing issues",
      "link": "https://www.tomshardware.com/pc-components/cpus/usd5-000-corsair-pre-built-keeps-on-frying-intel-cpus-due-to-lack-of-bios-update-tech-alleges-kills-three-intel-core-i9-chips-because-latest-version-still-doesnt-have-fix-for-intel-crashing-issues",
      "description": "Corsair still hasn't released the latest 0x12F Vmin instability fix for its pre-built computers, leading to multiple, repeated failures for some of its customers.",
      "pubDate": "Thu, 06 Nov 2025 14:45:02 +0000",
      "source": "Tom's Hardware",
      "sourceUrl": "https://www.tomshardware.com/feeds/all",
      "credibility": 0.85,
      "category": "tech_news"
    },
    {
      "title": "US govt committee slams Nvidia over shared campus with banned Huawei affiliate â€” says China has been in Nvidiaâ€™s backyard for a decade, literally",
      "link": "https://www.tomshardware.com/tech-industry/us-govt-committee-slams-nvidia-over-shared-campus-with-banned-huawei-affiliate-says-china-has-been-in-nvidias-backyard-for-a-decade-literally",
      "description": "Former Huawei affiliate Futurewei allegedly maintained an office in Nvidia's Santa Clara campus for around a decade until 2024.",
      "pubDate": "Thu, 06 Nov 2025 14:07:36 +0000",
      "source": "Tom's Hardware",
      "sourceUrl": "https://www.tomshardware.com/feeds/all",
      "credibility": 0.85,
      "category": "tech_news"
    },
    {
      "title": "Memory Matters: The State of Embedded NVM (eNVM) 2025",
      "link": "https://semiwiki.com/ip/362594-memory-matters-the-state-of-embedded-nvm-envm-2025/",
      "description": "Make a difference and take this short survey. It asks about your experience with embedded non-volatile memory technologies. The survey is anonymous, and the results will be shared in aggregate to help the industry better understand trends: 2025 Embedded Non-Volatile Memory Survey.\nWe are now in the AI era where data is the lifebloodâ€¦ Read More \nThe post Memory Matters: The State of Embedded NVM (eNVM) 2025 appeared first on SemiWiki.",
      "pubDate": "Thu, 06 Nov 2025 14:00:39 +0000",
      "source": "SemiWiki",
      "sourceUrl": "https://semiwiki.com/feed/",
      "credibility": 0.85,
      "category": "tech_news"
    },
    {
      "title": "Fall Into Gaming With 20+ Titles Joining GeForce NOW in November",
      "link": "https://blogs.nvidia.com/blog/geforce-now-thursday-november-2025-games/",
      "description": "Editorâ€™s note: This blog has been updated to reflect the correct launch date for â€˜Call of Duty: Black Ops 7â€™, November 14.Â Â  A crisp chillâ€™s in the air â€” and so is the action. GeForce NOW is packing November with 23 games hitting the cloud, including the launch of the highly anticipated Call of Duty:\t\n\t\tRead Article",
      "pubDate": "Thu, 06 Nov 2025 14:00:37 +0000",
      "source": "NVIDIA Blog",
      "sourceUrl": "https://blogs.nvidia.com/feed/",
      "credibility": 0.95,
      "category": "company_official"
    },
    {
      "title": "MSI MPG271QR X50 27-inch 500 Hz QHD QD-OLED gaming monitor review: Fast and colorful with premium cred",
      "link": "https://www.tomshardware.com/monitors/gaming-monitors/msi-mpg271qr-x50-27-inch-500-hz-qhd-qd-oled-gaming-monitor-review",
      "description": "MSI brings another choice to the 500 Hz QHD OLED genre with its MPG271QR X50. Itâ€™s a 27-inch panel with Adaptive-Sync, MPRT, HDR 500 and wide gamut color. It delivers smooth, responsive and colorful gaming.",
      "pubDate": "Thu, 06 Nov 2025 14:00:00 +0000",
      "source": "Tom's Hardware",
      "sourceUrl": "https://www.tomshardware.com/feeds/all",
      "credibility": 0.85,
      "category": "tech_news"
    },
    {
      "title": "Hands-on with Lian Li's Lancool 217 INF: RGB and infinity mirror are a drastically different take on the wood-clad original 217",
      "link": "https://www.tomshardware.com/pc-components/pc-cases/hands-on-with-its-rgb-and-infinity-mirror-lian-lis-lancool-217-inf-is-a-drastically-different-take-on-the-wood-clad-original-217",
      "description": "Lian Liâ€™s latest case is an RGB-and-mirror-clad take on the companyâ€™s excellent Lancool 217. It sports large 170mm intake fans for impressive airflow, sports an infinity mirror up front, and looks like a prop from a Tron reboot.",
      "pubDate": "Thu, 06 Nov 2025 14:00:00 +0000",
      "source": "Tom's Hardware",
      "sourceUrl": "https://www.tomshardware.com/feeds/all",
      "credibility": 0.85,
      "category": "tech_news"
    },
    {
      "title": "Nvidia wants China's market share to secure the future of CUDA in the region â€” America's trade war threatens Huang's influence, and could bolster competition",
      "link": "https://www.tomshardware.com/tech-industry/nvidia-wants-chinas-market-share-to-secure-the-future-of-cuda-in-the-region-americas-trade-war-threatens-huangs-influence-and-could-bolster-competition",
      "description": "The United States and Nvidia have come to a head over trading the latest GPU technology to China. While the US wants to retain a technological lead and strengthen supply chains, Nvidia is concerned of its waning influence in the region, which could lose out to domestic suppliers in the near future.",
      "pubDate": "Thu, 06 Nov 2025 13:16:43 +0000",
      "source": "Tom's Hardware",
      "sourceUrl": "https://www.tomshardware.com/feeds/all",
      "credibility": 0.85,
      "category": "tech_news"
    },
    {
      "title": "Sony reportedly working on 'Cross-Buy' feature for unified game ownership between PlayStation 5 and PC â€” Leaked icons indicate Sony's answer to Xbox Play Anywhere is coming",
      "link": "https://www.tomshardware.com/video-games/playstation/sony-reportedly-working-on-cross-buy-feature-for-unified-game-ownership-between-playstation-5-and-pc-leaked-icons-indicate-sonys-answer-to-xbox-play-anywhere-is-coming",
      "description": "You might be able to buy PlayStation games once and own them across PC and PS5 together, similar to Microsoft's Xbox Play Anywhere. A new \"Cross-Buy\" feature has been datamined inside PlayStation Store, added only a few months ago, hinting that it's been in the works for quite some time and might be releasing soon.",
      "pubDate": "Thu, 06 Nov 2025 13:00:00 +0000",
      "source": "Tom's Hardware",
      "sourceUrl": "https://www.tomshardware.com/feeds/all",
      "credibility": 0.85,
      "category": "tech_news"
    },
    {
      "title": "How to rip your DVDs with Handbrake â€” preserve your DVD library before bit rot claims another victim",
      "link": "https://www.tomshardware.com/software/how-to-rip-your-dvds-with-handbrake-preserve-your-dvd-library-before-bit-rot-claims-another-victim",
      "description": "Algorithms are telling us what to watch, but we can fight back by curating our old DVD collection into a digital archive that we can use on our modern devices.",
      "pubDate": "Thu, 06 Nov 2025 13:00:00 +0000",
      "source": "Tom's Hardware",
      "sourceUrl": "https://www.tomshardware.com/feeds/all",
      "credibility": 0.85,
      "category": "tech_news"
    },
    {
      "title": "Razer BlackShark V3 Pro Review: Highly connected, but you'll sound worse",
      "link": "https://www.tomshardware.com/peripherals/gaming-headsets/razer-blackshark-v3-pro-review",
      "description": "Razer's new BlackShark V3 Pro gaming headset features simultaneous dual wireless audio mixing, active noise cancellation, and a new microphone. But it's struggling to compete with its predecessor.",
      "pubDate": "Thu, 06 Nov 2025 13:00:00 +0000",
      "source": "Tom's Hardware",
      "sourceUrl": "https://www.tomshardware.com/feeds/all",
      "credibility": 0.85,
      "category": "tech_news"
    },
    {
      "title": "PC modder installs a working train set on top of their GPU â€” All aboard the 'PCI Express' to Gamesville",
      "link": "https://www.tomshardware.com/desktops/pc-building/pc-modder-installs-a-working-train-set-on-top-of-their-gpu-all-aboard-the-pci-express-to-gamesville",
      "description": "An enthusiast has shared their first designs, melding the disparate worlds of PC DIY and model railroading.",
      "pubDate": "Thu, 06 Nov 2025 12:41:18 +0000",
      "source": "Tom's Hardware",
      "sourceUrl": "https://www.tomshardware.com/feeds/all",
      "credibility": 0.85,
      "category": "tech_news"
    },
    {
      "title": "NVIDIA Founder and CEO Jensen Huang and Chief Scientist Bill Dally Awarded Prestigious Queen Elizabeth Prize for Engineering",
      "link": "https://blogs.nvidia.com/blog/nvidia-founder-and-ceo-jensen-huang-and-chief-scientist-bill-dally-awarded-prestigious-queen-elizabeth-prize-for-engineering/",
      "description": "NVIDIA founder and CEO Jensen Huang and chief scientist Bill Dally were honored this week in the U.K. for their foundational work in AI and machine learning. They were among the seven recipients of the 2025 Queen Elizabeth Prize for Engineering, recognized for their contributions to modern machine learning. Presented by His Majesty King Charles\t\n\t\tRead Article",
      "pubDate": "Thu, 06 Nov 2025 12:40:19 +0000",
      "source": "NVIDIA Blog",
      "sourceUrl": "https://blogs.nvidia.com/feed/",
      "credibility": 0.95,
      "category": "company_official"
    },
    {
      "title": "Windows security update triggers BitLocker recovery in some systems â€” bug mostly impacts Intel PCs with Modern Standby support",
      "link": "https://www.tomshardware.com/software/windows/windows-security-update-triggers-bitlocker-recovery-in-some-systems-bug-mostly-impacts-intel-pcs-with-modern-standby-support",
      "description": "A Windows security update had some systems unexpectedly asking for BitLocker passwords.",
      "pubDate": "Thu, 06 Nov 2025 12:40:00 +0000",
      "source": "Tom's Hardware",
      "sourceUrl": "https://www.tomshardware.com/feeds/all",
      "credibility": 0.85,
      "category": "tech_news"
    },
    {
      "title": "OpenAI walks back statement it wants a government 'backstop' for its massive loans â€” company says government 'playing its part' critical for industrial AI capacity increases",
      "link": "https://www.tomshardware.com/tech-industry/openai-walks-back-statement-it-wants-a-government-backstop-for-its-massive-loans-company-says-government-playing-its-part-critical-for-industrial-ai-capacity-increases",
      "description": "OpenAI has walked back claims its CFO made in an interview that it was looking for a governmental backstop for its investments, providing a guarantee against its loans that would allow it to borrow more and reduce the cost of that financing.",
      "pubDate": "Thu, 06 Nov 2025 12:36:41 +0000",
      "source": "Tom's Hardware",
      "sourceUrl": "https://www.tomshardware.com/feeds/all",
      "credibility": 0.85,
      "category": "tech_news"
    },
    {
      "title": "Apple's 13-inch MacBook Air slashed to $749 â€” a new all-time low price on Amazon for this M4-powered laptop",
      "link": "https://www.tomshardware.com/laptops/macbooks/apples-13-inch-macbook-air-slashed-to-usd749-a-new-all-time-low-price-on-amazon-for-this-m4-powered-laptop",
      "description": "Save $50 on this MacBook Air's previous low price. Apple's 2025 MacBook Air with M4 is only $749 at Amazon",
      "pubDate": "Thu, 06 Nov 2025 12:29:26 +0000",
      "source": "Tom's Hardware",
      "sourceUrl": "https://www.tomshardware.com/feeds/all",
      "credibility": 0.85,
      "category": "tech_news"
    },
    {
      "title": "3D-printed PS5 mod squeezes console into tiny 6-liter case, drastically improves performance â€” custom cooling solution slashes temps and power draw",
      "link": "https://www.tomshardware.com/video-games/playstation/3d-printed-ps5-mod-squeezes-console-into-tiny-6-liter-case-drastically-improves-performance-custom-cooling-solution-slashes-temps-and-power-draw",
      "description": "YouTuber transforms Sony's PlayStation 5 console into a tiny mini-PC with a twist.",
      "pubDate": "Thu, 06 Nov 2025 12:20:00 +0000",
      "source": "Tom's Hardware",
      "sourceUrl": "https://www.tomshardware.com/feeds/all",
      "credibility": 0.85,
      "category": "tech_news"
    },
    {
      "title": "Legendary Windows Pinball developer rescues 200lb magnetic disc drive from the 1980s â€“ requires a scissor lift to move it, only has 622 MB of storage",
      "link": "https://www.tomshardware.com/tech-industry/legendary-windows-pinball-developer-rescues-200lb-magnetic-disc-drive-from-the-1980s-requires-a-scissor-lift-to-move-it-only-has-622-mb-of-storage",
      "description": "Ex-Windows developer Dave Plummer rescues a nearly 200 lb magnetic disc drive with just 622 MB of storage from the 1980s",
      "pubDate": "Thu, 06 Nov 2025 12:17:43 +0000",
      "source": "Tom's Hardware",
      "sourceUrl": "https://www.tomshardware.com/feeds/all",
      "credibility": 0.85,
      "category": "tech_news"
    },
    {
      "title": "â€˜China is going to win the AI raceâ€™ â€” Nvidia CEO Jensen Huang decries the price of electricity in the US, contrasts it with China's subsidized pricing",
      "link": "https://www.tomshardware.com/tech-industry/artificial-intelligence/china-is-going-to-win-the-ai-race-nvidia-ceo-jensen-huang-decries-the-price-of-electricity-in-the-us-contrasts-it-with-chinas-subsidized-pricing",
      "description": "Nvidia CEO Jensen Huang said that China will win the AI race because of its abundance of power and the fact that the U.S. is losing out on the chance for its hardware to become the standard tool that Chinese AI developers use.",
      "pubDate": "Thu, 06 Nov 2025 12:10:41 +0000",
      "source": "Tom's Hardware",
      "sourceUrl": "https://www.tomshardware.com/feeds/all",
      "credibility": 0.85,
      "category": "tech_news"
    },
    {
      "title": "EufyMake E1 review: Dimensional UV printer",
      "link": "https://www.tomshardware.com/3d-printing/eufymake-e1-uv-printer-review",
      "description": "EufyMakeâ€™s not quite 3D UV printer is excitingâ€¦but is it worth it?",
      "pubDate": "Thu, 06 Nov 2025 12:00:00 +0000",
      "source": "Tom's Hardware",
      "sourceUrl": "https://www.tomshardware.com/feeds/all",
      "credibility": 0.85,
      "category": "tech_news"
    },
    {
      "title": "Crypto fraud and laundering ring that stole $689 million busted by European authorities â€” 9 arrests made across multiple countries, perps face a decade behind bars and huge fines",
      "link": "https://www.tomshardware.com/tech-industry/cryptocurrency/crypto-fraud-and-laundering-ring-that-stole-eur600m-usd689m-busted-by-european-authorities-9-arrests-made-across-multiple-countries-perps-face-a-decade-behind-bars-and-huge-fines",
      "description": "Crypto fraud and laundering ring that stole â‚¬600m ($689m) busted by European authorities â€” 9 arrests made across multiple countries",
      "pubDate": "Thu, 06 Nov 2025 12:00:00 +0000",
      "source": "Tom's Hardware",
      "sourceUrl": "https://www.tomshardware.com/feeds/all",
      "credibility": 0.85,
      "category": "tech_news"
    },
    {
      "title": "Louvre heist reveals museum used â€˜LOUVREâ€™ as password for its video surveillance, still has workstations with Windows 2000 - glaring security weaknesses revealed in previous report",
      "link": "https://www.tomshardware.com/tech-industry/cyber-security/louvre-heist-reveals-glaring-security-weaknesses-previous-reports-say-museum-used-louvre-as-password-for-its-video-surveillance-still-has-workstations-with-windows-2000",
      "description": "Is the Louvre's weak cybersecurity a deeper symptom?",
      "pubDate": "Thu, 06 Nov 2025 11:40:00 +0000",
      "source": "Tom's Hardware",
      "sourceUrl": "https://www.tomshardware.com/feeds/all",
      "credibility": 0.85,
      "category": "tech_news"
    },
    {
      "title": "Integrating Design Verification To Approach Zero Defects",
      "link": "https://semiengineering.com/integrating-design-verification-to-approach-zero-defects/",
      "description": "Combine functional fault grading with conventional structural testing to improve fault coverage.\nThe post Integrating Design Verification To Approach Zero Defects appeared first on Semiconductor Engineering.",
      "pubDate": "Thu, 06 Nov 2025 08:09:20 +0000",
      "source": "Semiconductor Engineering",
      "sourceUrl": "https://semiengineering.com/feed/",
      "credibility": 0.85,
      "category": "tech_news"
    },
    {
      "title": "Confidential Computing To Secure AI Workloads",
      "link": "https://semiengineering.com/confidential-computing-to-secure-ai-workloads/",
      "description": "Safeguarding data during computation using hardware-protected enclaves that isolate code and data from untrusted software.\nThe post Confidential Computing To Secure AI Workloads appeared first on Semiconductor Engineering.",
      "pubDate": "Thu, 06 Nov 2025 08:08:46 +0000",
      "source": "Semiconductor Engineering",
      "sourceUrl": "https://semiengineering.com/feed/",
      "credibility": 0.85,
      "category": "tech_news"
    },
    {
      "title": "Building And Validating FreeRTOS-Based Virtual ECUs: A Comprehensive Approach",
      "link": "https://semiengineering.com/building-and-validating-freertos-based-virtual-ecus-a-comprehensive-approach/",
      "description": "Automotive OEMs and suppliers are increasingly investigating lightweight alternatives to AUTOSAR.\nThe post Building And Validating FreeRTOS-Based Virtual ECUs: A Comprehensive Approach appeared first on Semiconductor Engineering.",
      "pubDate": "Thu, 06 Nov 2025 08:07:00 +0000",
      "source": "Semiconductor Engineering",
      "sourceUrl": "https://semiengineering.com/feed/",
      "credibility": 0.85,
      "category": "tech_news"
    },
    {
      "title": "LLMs Add Safety Risks To Physical AI",
      "link": "https://semiengineering.com/llms-add-safety-risks-to-physical-ai/",
      "description": "Extra measures are needed to avoid accidents and bias with robots and drones.\nThe post LLMs Add Safety Risks To Physical AI appeared first on Semiconductor Engineering.",
      "pubDate": "Thu, 06 Nov 2025 08:05:34 +0000",
      "source": "Semiconductor Engineering",
      "sourceUrl": "https://semiengineering.com/feed/",
      "credibility": 0.85,
      "category": "tech_news"
    },
    {
      "title": "Why Openness Matters For AI At The Edge",
      "link": "https://semiengineering.com/why-openness-matters-for-ai-at-the-edge/",
      "description": "Openness across software, standards, and silicon is critical for ensuring interoperability, flexibility, and growth. \nThe post Why Openness Matters For AI At The Edge appeared first on Semiconductor Engineering.",
      "pubDate": "Thu, 06 Nov 2025 08:05:12 +0000",
      "source": "Semiconductor Engineering",
      "sourceUrl": "https://semiengineering.com/feed/",
      "credibility": 0.85,
      "category": "tech_news"
    },
    {
      "title": "AIxCC 2025: What It Means For Device Security",
      "link": "https://semiengineering.com/aixcc-2025-what-it-means-for-device-security/",
      "description": "Competition shows it is possible to discover and patch vulnerabilities in open-source programs without human aid.\nThe post AIxCC 2025: What It Means For Device Security appeared first on Semiconductor Engineering.",
      "pubDate": "Thu, 06 Nov 2025 08:04:55 +0000",
      "source": "Semiconductor Engineering",
      "sourceUrl": "https://semiengineering.com/feed/",
      "credibility": 0.85,
      "category": "tech_news"
    },
    {
      "title": "Breaking The Compromise: Low Power And High Performance For The Intelligent Edge",
      "link": "https://semiengineering.com/breaking-the-compromise-low-power-and-high-performance-for-the-intelligent-edge/",
      "description": "Adding intelligence within the tight energy budgets of today's embedded designs.\nThe post Breaking The Compromise: Low Power And High Performance For The Intelligent Edge appeared first on Semiconductor Engineering.",
      "pubDate": "Thu, 06 Nov 2025 08:03:49 +0000",
      "source": "Semiconductor Engineering",
      "sourceUrl": "https://semiengineering.com/feed/",
      "credibility": 0.85,
      "category": "tech_news"
    },
    {
      "title": "Moving AI Workloads To The Edge",
      "link": "https://semiengineering.com/moving-ai-workloads-to-the-edge/",
      "description": "There are benefits and challenges of processing AI workloads on-device to enhance performance, reduce costs, and ensure data privacy.\nThe post Moving AI Workloads To The Edge appeared first on Semiconductor Engineering.",
      "pubDate": "Thu, 06 Nov 2025 08:03:18 +0000",
      "source": "Semiconductor Engineering",
      "sourceUrl": "https://semiengineering.com/feed/",
      "credibility": 0.85,
      "category": "tech_news"
    },
    {
      "title": "How Fast Can Germany Shift To Software-Defined Vehicles?",
      "link": "https://semiengineering.com/how-fast-can-germany-shift-to-software-defined-vehicles/",
      "description": "Design, manufacturing, and business overhaul is beginning, but threat from low-cost, feature-rich vehicles from China continues to grow.\nThe post How Fast Can Germany Shift To Software-Defined Vehicles? appeared first on Semiconductor Engineering.",
      "pubDate": "Thu, 06 Nov 2025 08:02:53 +0000",
      "source": "Semiconductor Engineering",
      "sourceUrl": "https://semiengineering.com/feed/",
      "credibility": 0.85,
      "category": "tech_news"
    },
    {
      "title": "Ryzen 7 9800X3D sales help AMD hit record-breaking $2.8 billion in client revenue for Q3 2025",
      "link": "https://www.tomshardware.com/tech-industry/ryzen-7-9800x3d-sales-help-amd-hit-record-breaking-usd2-8-billion-in-client-revenue-for-q3-2025",
      "description": "With the help of complete CPU domination in the consumer/gaming marketplace thanks to chips such as the Ryzen 7 9800X3D, AMD has achieved a record breaking $2.8 billion in client revenue.",
      "pubDate": "Wed, 05 Nov 2025 19:35:19 +0000",
      "source": "Tom's Hardware",
      "sourceUrl": "https://www.tomshardware.com/feeds/all",
      "credibility": 0.85,
      "category": "tech_news"
    },
    {
      "title": "5 Lessons the Semiconductor Industry Can Learn from Gaming",
      "link": "https://semiwiki.com/eda/perforce/363450-5-lessons-the-semiconductor-industry-can-learn-from-gaming/",
      "description": "By Kamal Khan\nRead More \nThe post 5 Lessons the Semiconductor Industry Can Learn from Gaming appeared first on SemiWiki.",
      "pubDate": "Wed, 05 Nov 2025 18:00:21 +0000",
      "source": "SemiWiki",
      "sourceUrl": "https://semiwiki.com/feed/",
      "credibility": 0.85,
      "category": "tech_news"
    },
    {
      "title": "China bans foreign AI chips from state-funded data centers, report claims â€” crackdown would include removing Nvidia, AMD, and Intel chips from builds in early stages",
      "link": "https://www.tomshardware.com/tech-industry/semiconductors/china-bans-foreign-ai-chips-from-state-funded-data-centers",
      "description": "China has reportedly issued a sweeping ban on foreign AI chips in any data center backed by government money, applying retroactively to early-stage builds.",
      "pubDate": "Wed, 05 Nov 2025 16:32:53 +0000",
      "source": "Tom's Hardware",
      "sourceUrl": "https://www.tomshardware.com/feeds/all",
      "credibility": 0.85,
      "category": "tech_news"
    },
    {
      "title": "Razer's BlackShark V2 X gaming headset drops to less than Â£29 â€” a huge 53% saving makes them the ideal audio upgrade on a budget",
      "link": "https://www.tomshardware.com/peripherals/gaming-headsets/razers-blackshark-v2-x-gaming-headset-drops-to-less-than-gbp29-a-huge-53-percent-saving-makes-them-the-ideal-audio-upgrade-on-a-budget",
      "description": "Save 53% on this lightweight Razer wired gaming headset with 7.1 surround sound support.",
      "pubDate": "Wed, 05 Nov 2025 15:55:42 +0000",
      "source": "Tom's Hardware",
      "sourceUrl": "https://www.tomshardware.com/feeds/all",
      "credibility": 0.85,
      "category": "tech_news"
    },
    {
      "title": "Cooling system for a single Nvidia Blackwell Ultra NVL72 rack costs a staggering $50,000 â€” set to increase to $56,000 with next-generation NVL144 racks",
      "link": "https://www.tomshardware.com/pc-components/cooling/cooling-system-for-a-single-nvidia-blackwell-ultra-nvl72-rack-costs-a-staggering-usd50-000-set-to-increase-to-usd56-000-with-next-generation-nvl144-racks",
      "description": "Morgan Stanley estimates that the cooling cost per rack will rise from $49,860 for Nvidiaâ€™s GN300 NVL72 to $55,710 for the Vera Rubin NVL144 due to hotter Vera CPUs and Rubin GPUs that require pricier $400 cold plates and more advanced liquid cooling systems.",
      "pubDate": "Wed, 05 Nov 2025 15:45:00 +0000",
      "source": "Tom's Hardware",
      "sourceUrl": "https://www.tomshardware.com/feeds/all",
      "credibility": 0.85,
      "category": "tech_news"
    },
    {
      "title": "Steam Deck introduces screen-off downloads, helps preserve OLED screen life â€” users can now download large titles with the handheld in low-power mode and have a new game ready to go when they turn it on",
      "link": "https://www.tomshardware.com/video-games/handheld-gaming/steam-deck-introduces-screen-off-downloads-helps-preserve-oled-screen-life-users-can-now-download-large-titles-with-the-handheld-in-low-power-mode-and-have-a-new-game-ready-to-go-when-they-turn-it-on",
      "description": "The Steam Deck will now download games even while the screen is turned off.",
      "pubDate": "Wed, 05 Nov 2025 15:39:15 +0000",
      "source": "Tom's Hardware",
      "sourceUrl": "https://www.tomshardware.com/feeds/all",
      "credibility": 0.85,
      "category": "tech_news"
    },
    {
      "title": "Neuralink implant patient gets 'second pair of eyes' thanks to motorized AI webcam â€” Insta360 Link 2 connected to MacBook gives father with ALS new lease on life",
      "link": "https://www.tomshardware.com/peripherals/webcams/neuralink-implant-patient-gets-second-pair-of-eyes-thanks-to-motorized-ai-webcam-insta360-link-2-connected-to-macbook-gives-father-with-als-new-lease-on-life",
      "description": "A new documentary shows ALS and Neuralink implant patient Brad Smith expanding his family connection with a motorized AI webcam.",
      "pubDate": "Wed, 05 Nov 2025 15:05:34 +0000",
      "source": "Tom's Hardware",
      "sourceUrl": "https://www.tomshardware.com/feeds/all",
      "credibility": 0.85,
      "category": "tech_news"
    },
    {
      "title": "A Compelling Differentiator in OEM Product Design",
      "link": "https://semiwiki.com/artificial-intelligence/362678-a-compelling-differentiator-in-oem-product-design/",
      "description": "Jennifer, an OEM hardware designer, is planning a product around a microcontroller she thinks will meet her needs and wants to supply power from a 3V coin cell battery which she must connect though a boost controller. Jennifer searches a rough description of the part she needs, generating a long list of component manufacturers â€¦ Read More \nThe post A Compelling Differentiator in OEM Product Design appeared first on SemiWiki.",
      "pubDate": "Wed, 05 Nov 2025 14:00:01 +0000",
      "source": "SemiWiki",
      "sourceUrl": "https://semiwiki.com/feed/",
      "credibility": 0.85,
      "category": "tech_news"
    },
    {
      "title": "TP-Link TL-WR3602BE Wi-Fi 7 Travel Router Review: Compact and packed with features, but average performance",
      "link": "https://www.tomshardware.com/networking/routers/tp-link-tl-wr3602be-wi-fi-7-travel-router-review",
      "description": "The TL-WR3602BE is a well-rounded Wi-Fi travel companion, but it compromises on performance.",
      "pubDate": "Wed, 05 Nov 2025 14:00:00 +0000",
      "source": "Tom's Hardware",
      "sourceUrl": "https://www.tomshardware.com/feeds/all",
      "credibility": 0.85,
      "category": "tech_news"
    },
    {
      "title": "SK hynix reveals DRAM development roadmap through 2031 â€” DDR6, GDDR8, LPDDR6, and 3D DRAM incoming",
      "link": "https://www.tomshardware.com/pc-components/dram/sk-hynix-reveals-dram-development-roadmap-through-2031-ddr6-gddr8-lpddr6-and-3d-dram-incoming",
      "description": "SK hynixâ€™s DRAM roadmap presented at the SK AI Summit 2025 outlines the companyâ€™s AI-focused memory strategy through 2031 that includes DDR6, LPDDR6, GDDR8, and next generations of HBM solutions. HBF is not expected to hit the market before 2030.",
      "pubDate": "Wed, 05 Nov 2025 13:20:00 +0000",
      "source": "Tom's Hardware",
      "sourceUrl": "https://www.tomshardware.com/feeds/all",
      "credibility": 0.85,
      "category": "tech_news"
    },
    {
      "title": "AMD reportedly prepping new X3D chip with higher clock speeds â€“ Ryzen 7 9700X3D spotted in benchmarks that rival Ryzen 7 9800X3D, new Strix Halo chip also unearthed",
      "link": "https://www.tomshardware.com/pc-components/cpus/amd-prepping-new-x3d-chip-with-higher-clock-speeds-ryzen-7-9700x3d-spotted-in-benchmarks-that-rival-ryzen-7-9800x3d-new-strix-halo-chip-also-unearthed",
      "description": "AMDâ€™s unreleased Ryzen 7 9700X3D appears on PassMark, hinting at a lower-cost alternative to the 9800X3D for gamers.",
      "pubDate": "Wed, 05 Nov 2025 13:06:50 +0000",
      "source": "Tom's Hardware",
      "sourceUrl": "https://www.tomshardware.com/feeds/all",
      "credibility": 0.85,
      "category": "tech_news"
    },
    {
      "title": "Vectrex Mini retro console shatters Kickstarter goal within minutes â€” it will now cost $173, up from $115 and $150, to secure yours.",
      "link": "https://www.tomshardware.com/video-games/retro-gaming/vectrex-mini-retro-console-shatters-kickstarter-goal-within-minutes-it-will-now-cost-usd173-up-from-usd115-and-usd150-to-secure-yours",
      "description": "The Vectrex Mini vector retrogaming console blasted past its â‚¬100,000 funding goal within just 15 minutes",
      "pubDate": "Wed, 05 Nov 2025 13:00:00 +0000",
      "source": "Tom's Hardware",
      "sourceUrl": "https://www.tomshardware.com/feeds/all",
      "credibility": 0.85,
      "category": "tech_news"
    },
    {
      "title": "How to check and enable Secure Boot on your Windows PC",
      "link": "https://www.tomshardware.com/software/windows/how-to-check-and-enable-secure-boot-on-your-windows-pc",
      "description": "If you plan to migrate to Windows 11, you will need to enable Secure Boot to ensure your install is protected. We show you how to enable Secure Boot with no drama.",
      "pubDate": "Wed, 05 Nov 2025 13:00:00 +0000",
      "source": "Tom's Hardware",
      "sourceUrl": "https://www.tomshardware.com/feeds/all",
      "credibility": 0.85,
      "category": "tech_news"
    },
    {
      "title": "Google exploring putting AI data centers in space â€” Project Suncatcher wants to harness in-orbit solar power to scale AI compute",
      "link": "https://www.tomshardware.com/tech-industry/artificial-intelligence/google-exploring-putting-ai-data-centers-in-space-project-suncatcher-wants-to-harness-in-orbit-solar-power-to-scale-ai-compute",
      "description": "Google is looking into putting AI data centers into orbit, but it still needs to solve a ton of engineering and cost challenges.",
      "pubDate": "Wed, 05 Nov 2025 12:56:20 +0000",
      "source": "Tom's Hardware",
      "sourceUrl": "https://www.tomshardware.com/feeds/all",
      "credibility": 0.85,
      "category": "tech_news"
    },
    {
      "title": "Just $109 nets you a 1000W power supply from ASRock, knocking almost 1/3 off the price  â€” the Steel Legend SL-1000G has plenty of power to throw at the latest and greatest GPUs",
      "link": "https://www.tomshardware.com/pc-components/just-usd109-nets-you-a-1000w-power-supply-from-asrock-knocking-almost-1-3-off-the-price-the-steel-legend-sl-1000g-has-plenty-of-power-to-throw-at-the-latest-and-greatest-gpus",
      "description": "Capable of powering an RTX 5090, the ASRock Steel Legend SL-1000G has had its price slashed by 1/3 to just $109.",
      "pubDate": "Wed, 05 Nov 2025 12:47:55 +0000",
      "source": "Tom's Hardware",
      "sourceUrl": "https://www.tomshardware.com/feeds/all",
      "credibility": 0.85,
      "category": "tech_news"
    },
    {
      "title": "Membrane evaporative cooling tech achieves record-breaking results, could be solution for next-generation AI server cooling â€” clocks 800 watts of heat flux per square centimeter",
      "link": "https://www.tomshardware.com/pc-components/cooling/membrane-evaporative-cooling-technology-achieves-record-breaking-results-could-be-a-solution-for-next-generation-ai-server-cooling-clocks-800-watts-of-heat-flux-per-square-centimeter",
      "description": "Engineers from the University of California, San Diego, have developed a new membrane evaporative cooling solution that has achieved record-breaking cooling performance. It could be a key solution for cooling Nvidia's future Feynman GPUs rated at over 4,000 watts of power consumption.",
      "pubDate": "Wed, 05 Nov 2025 12:40:00 +0000",
      "source": "Tom's Hardware",
      "sourceUrl": "https://www.tomshardware.com/feeds/all",
      "credibility": 0.85,
      "category": "tech_news"
    },
    {
      "title": "Custom Intel motherboards with a whopping 36 USB ports spotted online â€” extravagant connectivity offering fuels bot farm speculation",
      "link": "https://www.tomshardware.com/pc-components/motherboards/custom-intel-motherboards-with-a-whopping-36-usb-ports-spotted-online-extravagant-connectivity-offering-fuels-bot-farm-speculation",
      "description": "A user on Reddit has shared a collection of customized LGA 1151 motherboards with 36 USB ports stacked atop and next to each other. Even the most overkill models from the factory usually only support up to 20, and they're meant for special applications. A commenter suggests bot farming as the use for this odd motherboard.",
      "pubDate": "Wed, 05 Nov 2025 12:20:00 +0000",
      "source": "Tom's Hardware",
      "sourceUrl": "https://www.tomshardware.com/feeds/all",
      "credibility": 0.85,
      "category": "tech_news"
    },
    {
      "title": "Quantum diamond scanner delivers non-invasive 3D imaging of semiconductors â€” EuQlid Qu-MRI could â€˜save chip foundries billions of dollarsâ€™",
      "link": "https://www.tomshardware.com/tech-industry/semiconductors/quantum-diamond-scanner-delivers-non-invasive-3d-imaging-of-semiconductors-euqlid-qu-mri-could-save-chip-foundries-billions-of-dollars",
      "description": "EuQlid reveals a breakthrough diamond-based quantum sensing device for the non-invasive 3D imaging of semiconductors.",
      "pubDate": "Wed, 05 Nov 2025 12:00:30 +0000",
      "source": "Tom's Hardware",
      "sourceUrl": "https://www.tomshardware.com/feeds/all",
      "credibility": 0.85,
      "category": "tech_news"
    },
    {
      "title": "Creality Falcon A1 Pro 20 Watt review: Enclosed diode laser for prosumers",
      "link": "https://www.tomshardware.com/maker-stem/creality-falcon-a1-pro-20-watt-review",
      "description": "The Creality Falcon A1 Pro is a 20W laser engraver that offers automatic material focus, an integrated camera, and user-friendly software at a sub-$1,000 base price. With an upgrade path that includes a 2W IR laser for engraving metal and glass, a rotary engraver, and an air filter, the Falcon A1 Pro is a prosumer product that can grow with users over time.",
      "pubDate": "Wed, 05 Nov 2025 12:00:00 +0000",
      "source": "Tom's Hardware",
      "sourceUrl": "https://www.tomshardware.com/feeds/all",
      "credibility": 0.85,
      "category": "tech_news"
    },
    {
      "title": "Amazon's new armored undersea cable is fast enough to stream 12.5 million HD films simultaneously between the US and Ireland, repels attacks â€” Fastnet to deliver 320+ terabits per second across the Atlantic",
      "link": "https://www.tomshardware.com/networking/amazons-new-armored-undersea-cable-is-fast-enough-to-stream-12-5-million-hd-films-simultaneously-between-the-us-and-ireland-fastnet-to-deliver-320-terabits-per-second-across-the-atlantic",
      "description": "Amazon's Fastnet 320 Tb/s armored cable connects U.S. and Ireland.",
      "pubDate": "Wed, 05 Nov 2025 12:00:00 +0000",
      "source": "Tom's Hardware",
      "sourceUrl": "https://www.tomshardware.com/feeds/all",
      "credibility": 0.85,
      "category": "tech_news"
    },
    {
      "title": "China and America's AI war isn't just about compute, it's about energy  â€” energy subsidies promote homegrown chip push, amid data center energy squeeze",
      "link": "https://www.tomshardware.com/tech-industry/china-and-americas-ai-war-isnt-just-about-compute-its-about-energy-energy-subsidies-promote-homegrown-chip-push-amid-data-center-energy-squeeze",
      "description": "Local Chinese governments are issuing attractive energy subsidies to companies which use domestically produced chips in their data centers and \"AI factories,\" in order to discourage use of international CPUs and GPUs from the likes of Nvidia.",
      "pubDate": "Wed, 05 Nov 2025 11:41:49 +0000",
      "source": "Tom's Hardware",
      "sourceUrl": "https://www.tomshardware.com/feeds/all",
      "credibility": 0.85,
      "category": "tech_news"
    }
  ],
  "quantum_computing": [
    {
      "title": "The electron double-slit experiment from an ISP perspective",
      "link": "https://arxiv.org/abs/2511.02863",
      "description": "arXiv:2511.02863v1 Announce Type: new \nAbstract: This paper presents a pedagogical model, and accompanying R code, of the electron double-slit experiment using the perspective of indivisible stochastic processes. The approach offers an alternative lens on quantum probability and coherence phenomena, emphasizing a statistical rather than purely wave-mechanical interpretation.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv Quantum Physics",
      "sourceUrl": "http://export.arxiv.org/rss/quant-ph",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Clifford Hierarchy Stabilizer Codes: Transversal Non-Clifford Gates and Magic",
      "link": "https://arxiv.org/abs/2511.02900",
      "description": "arXiv:2511.02900v1 Announce Type: new \nAbstract: A fundamental problem in fault-tolerant quantum computation is the tradeoff between universality and dimensionality, exemplified by the the Bravyi-K\\\"onig bound for $n$-dimensional topological stabilizer codes. In this work, we extend topological Pauli stabilizer codes to a broad class of $n$-dimensional Clifford hierarchy stabilizer codes. These codes correspond to the $(n+1)$D Dijkgraaf-Witten gauge theories with non-Abelian topological order. We construct transversal non-Clifford gates through automorphism symmetries represented by cup products. In 2D, we obtain the first transversal non-Clifford logical gates including T and CS for Clifford stabilizer codes, using the automorphism of the twisted $\\mathbb{Z}_2^3$ gauge theory (equivalent to $\\mathbb{D}_4$ topological order). We also combine it with the just-in-time decoder to fault-tolerantly prepare the logical T magic state in $O(d)$ rounds via code switching. In 3D, we construct a transversal logical $\\sqrt{\\text{T}}$ gate in a non-Clifford stabilizer code at the third level of the Clifford hierarchy, located on a tetrahedron corresponding to a twisted $\\mathbb{Z}_2^4$ gauge theory. Due to the potential single-shot code-switching properties of these codes, one could achieve the 4th level of Clifford hierarchy with an $O(d^3)$ space-time overhead, avoiding the tradeoff observed in 2D. We propose a conjecture extending the Bravyi-K\\\"onig bound to Clifford hierarchy stabilizer codes, with our explicit constructions providing an upper bound of spatial dimension $(N-1)$ for achieving the logical gates in the $N^\\text{th}$-level of Clifford hierarchy.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv Quantum Physics",
      "sourceUrl": "http://export.arxiv.org/rss/quant-ph",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Zero-Noise Extrapolation via Cyclic Permutations of Quantum Circuit Layouts",
      "link": "https://arxiv.org/abs/2511.02901",
      "description": "arXiv:2511.02901v1 Announce Type: new \nAbstract: Increasing the utility of currently available Noisy Intermediate-Scale Quantum (NISQ) devices requires developing efficient methods to mitigate hardware errors, taking into account the constraints of these devices such as medium number of qubits and limited connectivity between them. In this work we propose a novel Cyclic Layout Permutations based Zero Noise Extrapolation (CLP-ZNE) protocol for such a task. The method leverages the inherent non-uniformity of gate errors in NISQ hardware and exploits symmetries of quantum circuits with one-dimensional connectivity to extrapolate the expectation value, averaged over cyclic circuit layout permutations, to the level of zero noise. In contrast to the previous layout permutation based approaches, for $n$ qubit circuit CLP-ZNE requires measurements of only $O(n)$ different circuit layouts to reconstruct the noiseless expected value. When benchmarked against noise channels modeling the IBM Torino quantum computer, the method reduces a typical expectation value error by an order of magnitude, depending on the protocol specifications. By employing a noise model derived from real hardware specifications, including both depolarizing and $T_1/T_2$ relaxation processes, these results give evidence for the applicability of CLP-ZNE to present-day NISQ processors.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv Quantum Physics",
      "sourceUrl": "http://export.arxiv.org/rss/quant-ph",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Classical shadows for sample-efficient measurements of gauge-invariant observables",
      "link": "https://arxiv.org/abs/2511.02904",
      "description": "arXiv:2511.02904v1 Announce Type: new \nAbstract: Classical shadows provide a versatile framework for estimating many properties of quantum states from repeated, randomly chosen measurements without requiring full quantum state tomography. When prior information is available, such as knowledge of symmetries of states and operators, this knowledge can be exploited to significantly improve sample efficiency. In this work, we develop three classical shadow protocols tailored to systems with local (or gauge) symmetries to enable efficient prediction of gauge-invariant observables in lattice gauge theory models which are currently at the forefront of quantum simulation efforts. For such models, our approaches can offer exponential improvements in sample complexity over symmetry-agnostic methods, albeit at the cost of increased circuit complexity. We demonstrate these trade-offs using a $\\mathbb{Z}_2$ lattice gauge theory, where a dual formulation enables a rigorous analysis of resource requirements, including both circuit depth and sample complexity.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv Quantum Physics",
      "sourceUrl": "http://export.arxiv.org/rss/quant-ph",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Analytically Continuing the Randomized Measurement Toolbox",
      "link": "https://arxiv.org/abs/2511.02912",
      "description": "arXiv:2511.02912v1 Announce Type: new \nAbstract: We develop a framework for extracting non-polynomial analytic functions of density matrices in randomized measurement experiments by a method of analytical continuation. A central advantage of this approach, dubbed stabilized analytic continuation (SAC), is its robustness to statistical noise arising from finite repetitions of a quantum experiment, making it well-suited to realistic quantum hardware. As a demonstration, we use SAC to estimate the von Neumann entanglement entropy of a numerically simulated quenched N\\'eel state from R\\'enyi entropies estimated via the randomized measurement protocol. We then apply the method to experimental R\\'enyi data from a trapped-ion quantum simulator, extracting subsystem von Neumann entropies at different evolution times. Finally, we briefly note that the SAC framework is readily generalizable to obtain other nonlinear diagnostics, such as the logarithmic negativity and R\\'enyi relative entropies.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv Quantum Physics",
      "sourceUrl": "http://export.arxiv.org/rss/quant-ph",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Correlation Self-Testing of Quantum Theory against Generalised Probabilistic Theories with Restricted Relabelling Symmetry",
      "link": "https://arxiv.org/abs/2511.02914",
      "description": "arXiv:2511.02914v1 Announce Type: new \nAbstract: Correlation self-testing of quantum theory involves identifying a task or set of tasks whose optimal performance can be achieved only by theories that can realise the same set of correlations as quantum theory in every causal structure. Following this approach, previous work has ruled out various classes of generalised probabilistic theories whose joint state spaces have a certain regularity in the sense of a (discrete) rotation symmetry of the bipartite state spaces. Here we consider theories whose bipartite state spaces lack this regularity. We form them by taking the convex hull of all the local states and a finite number of non-local states. We show that a criterion of compositional consistency is needed in such theories: for a measurement effect to be valid, there must exist at least one measurement that it is part of. This goes beyond previous consistency criteria and corresponds to a strengthening of the no-restriction hypothesis. We show that quantum theory outperforms these theories in a task called the adaptive CHSH game, which shows that they can be ruled out experimentally. We further show a connection between compositional consistency and Tsirelson's bound.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv Quantum Physics",
      "sourceUrl": "http://export.arxiv.org/rss/quant-ph",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Motional entanglement in low-energy collisions near shape resonances",
      "link": "https://arxiv.org/abs/2511.02925",
      "description": "arXiv:2511.02925v1 Announce Type: new \nAbstract: Einstein, Podolsky, and Rosen discussed their paradox in terms of measuring the positions or momenta of two particles. These can become entangled upon scattering, but how much entanglement can be created in this process? Here we address this question with fully coherent calculations of bipartite scattering in three-dimensional space, quantifying entanglement by the inverse of the single particle purity. We show that the standard plane-wave description of scattering fails to capture the entanglement properties, due to the essential role of quantum uncertainty in the initial state. For a more realistic description of a scattering setup and narrow initial momentum dispersion, we find the entanglement to scale linearly with the scattering cross section, including strong enhancement close to shape resonances. We discuss how the generation of motional entanglement can be detected in experiments. Our results open the way to probing and eventually using entanglement in quantum collisions.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv Quantum Physics",
      "sourceUrl": "http://export.arxiv.org/rss/quant-ph",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "SWAP-Network Routing and Spectral Qubit Ordering for MPS Imaginary-Time Optimization",
      "link": "https://arxiv.org/abs/2511.02980",
      "description": "arXiv:2511.02980v1 Announce Type: new \nAbstract: We propose a quantum-inspired combinatorial solver that performs imaginary-time evolution (ITE) on a matrix product state (MPS), incorporating non-local couplings through structured SWAP networks and spectral qubit mapping of logical qubits. The SWAP networks, composed exclusively of local two-qubit gates, effectively mediate non-local qubit interactions. We investigate two distinct network architectures based on rectangular and triangular meshes of SWAP gates and analyze their performance in combination with spectral qubit ordering, which maps logical qubits to MPS sites based on the Laplacian of the logical qubit connectivity graph. The proposed framework is evaluated on synthetic MaxCut instances with varying graph connectivity, as well as on a dynamic portfolio optimization problem based on real historical asset data involving 180 qubits. On certain problem configurations, we observe an over 20$\\times$ reduction in error when combining spectral ordering and triangular SWAP networks compared to optimization with shuffled qubit ordering. Furthermore, an analysis of the entanglement entropy during portfolio optimization reveals that spectral qubit ordering not only improves solution quality but also enhances the total and spatially distributed entanglement within the MPS. These findings demonstrate that exploiting problem structure through spectral mapping and efficient routing networks can substantially enhance the performance of tensor-network-based optimization algorithms.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv Quantum Physics",
      "sourceUrl": "http://export.arxiv.org/rss/quant-ph",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "D2-UC: A Distributed-Distributed Quantum-Classical Framework for Unit Commitment",
      "link": "https://arxiv.org/abs/2511.03104",
      "description": "arXiv:2511.03104v1 Announce Type: new \nAbstract: This paper introduces D2-UC, a quantum-ready framework for the unit commitment (UC) problem that prepares UC for near-term hybrid quantum-classical solvers by combining distributed classical decomposition with distributed quantum execution. We reformulate deterministic and stochastic UC into a three-block alternating direction method of multipliers (ADMM): (i) a convex quadratic subproblem for dispatch and reserves, (ii) a binary subproblem expressed as a quadratic unconstrained binary optimization (QUBO), and (iii) a proximal slack update for consensus. The core contributions are fivefold. First, we demonstrate how the full UC problem can be expressed as a single monolithic QUBO, establishing a direct interface to quantum solvers. Second, we decompose this large binary block into three type-specific QUBOs for commitment, startup, and shutdown, making the problem more tractable but revealing slower ADMM convergence. Third, we restore local logical couplings through per-unit-time micro-QUBOs, which accelerate convergence. Fourth, we batch micro-QUBOs into K non-overlapping block-diagonal problems, reducing many subproblems to a fixed number of solver-ready QUBOs per iteration, compatible with distributed variational quantum eigensolvers (DVQE). Fifth, we integrate an accept-if-better safeguard with DVQE to stabilize hybrid updates and prevent oscillations. Case studies confirm that the proposed methods deliver feasible schedules, faster convergence, and QUBO sizes aligned with current and near-term quantum hardware capabilities. All detailed data, codes, and parameter values are available at https://github.com/LSU-RAISE-LAB/3B-ADMM-UC-DVQE .",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv Quantum Physics",
      "sourceUrl": "http://export.arxiv.org/rss/quant-ph",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Frequency- and Amplitude-Modulated Gates for Universal Quantum Control",
      "link": "https://arxiv.org/abs/2511.03164",
      "description": "arXiv:2511.03164v1 Announce Type: new \nAbstract: Achieving high-fidelity single- and two-qubit gates is essential for executing arbitrary digital quantum algorithms and for building error-corrected quantum computers. We propose a theoretical framework for implementing quantum gates using frequency- and amplitude-modulated microwave control, which extends conventional amplitude modulation by introducing frequency modulation as an additional degree of control. Our approach operates on fixed-frequency qubits, converting the need for qubit frequency tunability into drive frequency modulation. Using Floquet theory, we analyze and design these drives for optimal fidelity within specified criteria. Our framework spans adiabatic to nonadiabatic gates within the Floquet framework, ensuring broad applicability across gate types and control schemes. Using typical transmon qubit parameters in numerical simulations, we demonstrate a universal gate set-including the X, Hadamard, phase, and CZ gates-with control error well below 0.1% and gate times of 25-40 ns for single-qubit operations and 125-135 ns for two-qubit operations. Furthermore, we show an always-on CZ gate tailored for driven qubits, which has gate times of 80-90 ns.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv Quantum Physics",
      "sourceUrl": "http://export.arxiv.org/rss/quant-ph",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Heralded Induced-Coherence Interferometry in a Noisy Environment",
      "link": "https://arxiv.org/abs/2511.03176",
      "description": "arXiv:2511.03176v1 Announce Type: new \nAbstract: Induced-coherence interferometry, first introduced in the Zou-Wang-Mandel (ZWM) setup, enables retrieval of object information from the interference pattern of light that never interacted with the object. This scheme relies on two identically correlated photon pairs and the absence of \"which-way\" information about the photons illuminating the object to induce coherence in their companions. In previous studies, the effect of thermal background on the ZWM interferometer was considered; here we explicitly include background noise and analyze the interference visibility in both low- and high-gain regimes, revealing how thermal photons introduce an incoherent offset that lowers the observed interference contrast. We show that the visibility can be restored either by optimal attenuation or by extending the geometry to a three-SPDC configuration. Furthermore, we demonstrate that introducing heralded detection removes the detrimental effect of thermal background noise, restoring high-contrast interference fringes.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv Quantum Physics",
      "sourceUrl": "http://export.arxiv.org/rss/quant-ph",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Quantum Sensing of Copper-Phthalocyanine Electron Spins via NV Relaxometry",
      "link": "https://arxiv.org/abs/2511.03200",
      "description": "arXiv:2511.03200v1 Announce Type: new \nAbstract: Molecular spin systems are promising candidates for quantum information processing and nanoscale sensing, yet their characterization at room temperature remains challenging due to fast spin decoherence. In this work, we use $T_1$ relaxometry of shallow nitrogen-vacancy (NV) centers in diamond to probe the electron spin ensemble of a polycrystalline copper phthalocyanine (CuPc) thin film. In addition to unequivocally identifying the NV-CuPc interaction thanks to its hyperfine spectrum, we further extract key parameters of the CuPc spin ensemble, including its correlation time and local lattice orientation, that cannot be measured in bulk electron resonance experiments. The analysis of our experimental results confirms that electron-electron interactions dominate the decoherence dynamics of CuPc at room temperature. Additionally, we demonstrate that the CuPc-enhanced NV relaxometry can serve as a robust method to estimate the NV depth with $\\sim1$~nm precision. Our results establish NV centers as powerful probes for molecular spin systems, providing insights into molecular qubits, spin bath engineering, and hybrid quantum materials, and offering a potential pathway toward their applications such as molecular-scale quantum processors and spin-based quantum networks.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv Quantum Physics",
      "sourceUrl": "http://export.arxiv.org/rss/quant-ph",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Quantum properties of superpositions of oppositely squeezed states",
      "link": "https://arxiv.org/abs/2511.03204",
      "description": "arXiv:2511.03204v1 Announce Type: new \nAbstract: We investigate the quantum properties of superpositions of oppositely squeezed states, which can be regarded as Schrodinger cat states. Compared with conventional coherent-state cat states, these states exhibit distinct photon-number structures and enhanced nonclassical features. We analyze their Wigner function and quantify the entanglement generated when they are injected into a 50:50 beam splitter. For small squeezing parameters, the resulting two-mode states possess higher entanglement than pure two-mode squeezed vacuum states. We also propose a linear-optical heralding scheme that approximates this superposition of oppositely squeezed states without requiring strong Kerr nonlinearities. Our results indicate that such states are promising resources for continuous-variable quantum information processing, particularly in regimes where high non-Gaussianity and strong entanglement are desirable.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv Quantum Physics",
      "sourceUrl": "http://export.arxiv.org/rss/quant-ph",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Quantum phase transition in the anisotropic Rabi model induced by parametric amplification",
      "link": "https://arxiv.org/abs/2511.03207",
      "description": "arXiv:2511.03207v1 Announce Type: new \nAbstract: In this manuscript, we analyze the mechanism of the superradiant phase transition in the anisotropic Rabi model under the classical oscillator limit using the pattern picture. By expanding the anisotropic Rabi model Hamiltonian in operator space, we obtained three patterns, and we find that the phase transition arises from the competition between patterns. The difficulty in achieving the classical oscillator limit motivates our investigation into the quantum phase transition within a parametrically-driven Jaynes-Cummings model. This parametrically-driven Jaynes-Cummings model can reproduce the dynamics of a ultrastrong-coupling anisotropic Rabi model in a squeezed-light frame. According to the eigenenergies and eigenstates of the normal and superradiant phases of this equivalent anisotropic Rabi model, we find that the excitation energy of the normal phase and the superradiant phase vanishes at the critical point. The photon number becomes infinite beyond the critical point. These results indicate that the system undergoes a superradiant phase transition at the critical point.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv Quantum Physics",
      "sourceUrl": "http://export.arxiv.org/rss/quant-ph",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Quantum-classical hybrid algorithm using quantum annealing for multi-objective job shop scheduling",
      "link": "https://arxiv.org/abs/2511.03257",
      "description": "arXiv:2511.03257v1 Announce Type: new \nAbstract: Efficient production planning is essential in modern manufacturing to improve performance indicators such as lead time and to reduce reliance on human intuition. While mathematical optimization approaches, formulated as job shop scheduling problems, have been applied to automate this process, solving large-scale production planning problems remains computationally demanding. Moreover, many practical scenarios involve conflicting objectives, making traditional scalarization techniques ineffective in finding diverse and useful Pareto-optimal solutions. To address these challenges, we developed a quantum-classical hybrid algorithm that decomposes the problem into two subproblems: resource allocation and task scheduling. Resource allocation is formulated as a quadratic unconstrained binary optimization problem and solved using annealing-based methods that efficiently explore complex solutions. Task scheduling is modeled as a mixed-integer linear programming problem and solved using conventional solvers to satisfy detailed scheduling constraints. We validated the proposed method using benchmark instances based on foundry production scenarios. Experimental results demonstrate that our hybrid approach achieves superior solution quality and computational efficiency compared to traditional monolithic methods. This work offers a promising direction for high-speed, multi-objective scheduling in industrial applications.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv Quantum Physics",
      "sourceUrl": "http://export.arxiv.org/rss/quant-ph",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Thermodynamic Probes of Multipartite Entanglement in Strongly Interacting Quantum Systems",
      "link": "https://arxiv.org/abs/2511.03266",
      "description": "arXiv:2511.03266v1 Announce Type: new \nAbstract: Quantifying multipartite entanglement in quantum many-body systems and hybrid quantum computing architectures is a fundamental yet challenging task. In recent years, thermodynamic quantities such as the maximum extractable work from an isolated system (the ergotropy) have allowed for entanglement measures that are operationally more accessible. However, these measures can be restrictive when applied to systems governed by Hamiltonians with strong collective or interparticle interactions. Motivated by advances in quantum simulators, we propose a framework that circumvents these restrictions by evaluating global and local ergotropy either through controlled quenching of interactions or by measuring suitable local observables only. We show that this formalism allows us to correctly estimate genuine multipartite entanglement in both stationary and time-evolved states of systems with strong interactions, including parametrized quantum states simulated on a quantum circuit with varying circuit depth and noise. We demonstrate its applicability to realistic physical models, namely, the Tavis-Cummings model, the three-level Dicke model, and the transverse-field Ising model, highlighting its potential as a versatile tool for characterizing entanglement in near-term quantum simulators.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv Quantum Physics",
      "sourceUrl": "http://export.arxiv.org/rss/quant-ph",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Influence of Data Dimensionality Reduction Methods on the Effectiveness of Quantum Machine Learning Models",
      "link": "https://arxiv.org/abs/2511.03320",
      "description": "arXiv:2511.03320v1 Announce Type: new \nAbstract: Data dimensionality reduction techniques are often utilized in the implementation of Quantum Machine Learning models to address two significant issues: the constraints of NISQ quantum devices, which are characterized by noise and a limited number of qubits, and the challenge of simulating a large number of qubits on classical devices. It also raises concerns over the scalability of these approaches, as dimensionality reduction methods are slow to adapt to large datasets. In this article, we analyze how data reduction methods affect different QML models. We conduct this experiment over several generated datasets, quantum machine algorithms, quantum data encoding methods, and data reduction methods. All these models were evaluated on the performance metrics like accuracy, precision, recall, and F1 score. Our findings have led us to conclude that the usage of data dimensionality reduction methods results in skewed performance metric values, which results in wrongly estimating the actual performance of quantum machine learning models. There are several factors, along with data dimensionality reduction methods, that worsen this problem, such as characteristics of the datasets, classical to quantum information embedding methods, percentage of feature reduction, classical components associated with quantum models, and structure of quantum machine learning models. We consistently observed the difference in the accuracy range of 14% to 48% amongst these models, using data reduction and not using it. Apart from this, our observations have shown that some data reduction methods tend to perform better for some specific data embedding methodologies and ansatz constructions.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv Quantum Physics",
      "sourceUrl": "http://export.arxiv.org/rss/quant-ph",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Exploring Topologies in Quantum Annealing: A Hardware-Aware Perspective",
      "link": "https://arxiv.org/abs/2511.03327",
      "description": "arXiv:2511.03327v1 Announce Type: new \nAbstract: Quantum Annealing (QA) offers a promising framework for solving NP-hard optimization problems, but its effectiveness is constrained by the topology of the underlying quantum hardware. Solving an optimization problem $P$ via QA involves a hardware-aware circuit compilation which requires representing $P$ as a graph $G_P$ and embedding it into the hardware connectivity graph $G_Q$ that defines how qubits connect to each other in a QA-based quantum processing unit (QPU).\n  Minor Embedding (ME) is a possible operational form of this hardware-aware compilation. ME heuristically builds a map that associates each node of $G_P$ -- the logical variables of $P$ -- to a chain of adjacent nodes in $G_Q$ by means of one of its minors, so that the arcs of $G_P$ are preserved as physical connections among qubits in $G_Q$.\n  The static topology of hardwired qubits can clearly lead to inefficient compilations because $G_Q$ cannot be a clique, currently. We propose a methodology and a set of criteria to evaluate how the hardware topology $G_Q$ can negatively affect the embedded problem, thus making the quantum optimization more sensible to noise.\n  We evaluate the result of ME across two QPU topologies: Zephyr graphs (used in current D-Wave systems) and Havel-Hakimi graphs, which allow controlled variation of the average node degree. This enables us to study how the ratio `number of nodes/number of incident arcs per node' affects ME success rates to map $G_P$ into a minor of $G_Q$.\n  Our findings, obtained through ME executed on classical, i.e. non-quantum, architectures, suggest that Havel-Hakimi-based topologies, on average, require shorter qubit chains in the minor of $G_P$, exhibiting smoother scaling of the largest embeddable $G_P$ as the QPU size increases. These characteristics indicate their potential as alternative designs for QA-based QPUs.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv Quantum Physics",
      "sourceUrl": "http://export.arxiv.org/rss/quant-ph",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Dynamical discontinuities in repeated weak measurements revealed by complex weak values",
      "link": "https://arxiv.org/abs/2511.03352",
      "description": "arXiv:2511.03352v1 Announce Type: new \nAbstract: Critical phenomena reveal universal behavior in complex systems, and uncovering analogous effects in quantum weak measurement protocols with post-selection provides new insight into how measurement backaction can shape quantum dynamics. This work investigates dynamical discontinuities that arise when the post-selected polar angle is used as a control parameter. The system evolves under repeated applications of a weak measurement protocol with post-selection, in which the meter state is retained after each iteration while the system is renewed. The emergence of these discontinuities is shown to be determined by the structure of the weak value: when the weak value has a nonzero imaginary component, a discontinuity appears in the expectation value of meter observables precisely at the point where the imaginary part of the weak value vanishes as a function of the post-selection polar angle. In contrast, no discontinuities occur when the weak value remains real for all post-selection angles. The phenomenon originates from the eigenstructure of the protocol's Kraus operator, with the stability of fixed points changing at the critical point where the discontinuity arises. Remarkably, the associated critical exponent is 1, independent of system parameters. These results open new perspectives for engineering non-analytic behavior in measurement-based quantum control and for probing criticality in post-selected quantum dynamics using weak measurements with weak values.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv Quantum Physics",
      "sourceUrl": "http://export.arxiv.org/rss/quant-ph",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Universal Quantum Simulation of 50 Qubits on Europe`s First Exascale Supercomputer Harnessing Its Heterogeneous CPU-GPU Architecture",
      "link": "https://arxiv.org/abs/2511.03359",
      "description": "arXiv:2511.03359v1 Announce Type: new \nAbstract: We have developed a new version of the high-performance J\\\"ulich universal quantum computer simulator (JUQCS-50) that leverages key features of the GH200 superchips as used in the JUPITER supercomputer, enabling simulations of a 50-qubit universal quantum computer for the first time. JUQCS-50 achieves this through three key innovations: (1) extending usable memory beyond GPU limits via high-bandwidth CPU-GPU interconnects and LPDDR5 memory; (2) adaptive data encoding to reduce memory footprint with acceptable trade-offs in precision and compute effort; and (3) an on-the-fly network traffic optimizer. These advances result in an 11.4-fold speedup over the previous 48-qubit record on the K computer.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv Quantum Physics",
      "sourceUrl": "http://export.arxiv.org/rss/quant-ph",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Integration of quantum dots at the tips of single plasmonic bipyramid nanoantennas for strong coupling at room temperature",
      "link": "https://arxiv.org/abs/2511.03409",
      "description": "arXiv:2511.03409v1 Announce Type: new \nAbstract: Achieving strong coupling between excitons of colloidal semiconductor quantum dots (QDs) and localized surface plasmon polaritons (LSPs) is critical for advanced room-temperature quantum emitter and sensing applications. A key challenge is to have precise control of the emitters position with respect to an individual plasmonic nanostructure. Here, we present room temperature strong coupling between QDs and a single gold nano-bipyramid (BPs). The selection of the bipyramid plasmonic nanocavity offers access to a single hotspot with a very small mode volume. The localization of QDs at a single hotspot is achieved via plasmon-triggered two-photon polymerization. This technique exploits the enhanced electric field at the BP tip to selectively polymerize a photosensitive QD-containing formulation. Room-temperature scattering spectra of a 3-QD-BP system reveal Rabi splitting of 349.3 meV and a coupling strength of 175.68 meV. The with distinct anti-crossing behavior is confirmed by simulations. This approach simplifies QD integration for strong coupling systems compared to previous methods. These results indicate a scalable platform for solid-state quantum technologies with colloidal QDs, enabling explora-tion of exciton-plasmon interactions and further advance-ment of applications in quantum optics and quantum sensing under ambient conditions.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv Quantum Physics",
      "sourceUrl": "http://export.arxiv.org/rss/quant-ph",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Quantum-elevated Chiral Discrimination for Bio-molecules",
      "link": "https://arxiv.org/abs/2511.03412",
      "description": "arXiv:2511.03412v1 Announce Type: new \nAbstract: Chiral discrimination of enantiomeric biomolecules is vital in chemistry, biology, and medicine. Conventional methods, relying on circularly polarized light, face weak chiroptical signals and potential photodamage. Despite extensive efforts to improve sensitivity under low-photon exposure, classical chiral probes remain fundamentally bounded by the shot-noise limit due to quantum fluctuations. To beat these limitations, we demonstrate quantum-elevated chiral discrimination using continuous-variable polarization-entangled states as moderate-photon-flux, high-sensitivity, quantum-noise-squeezed chiral probes. We achieve a 5 dB improvement beyond the SNL in distinguishing L- and D-amino acids in liquid phase. This non-destructive, biocompatible protocol enables high-sensitivity chiral analysis, with broad implications for drug development, biochemical research, environmental monitoring, and asymmetric synthesis.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv Quantum Physics",
      "sourceUrl": "http://export.arxiv.org/rss/quant-ph",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Universal first-passage time statistics for quantum diffusion",
      "link": "https://arxiv.org/abs/2511.03455",
      "description": "arXiv:2511.03455v1 Announce Type: new \nAbstract: First-passage phenomena play a fundamental role in classical stochastic processes. We here exactly solve a quantum first-passage time problem for quantum diffusion driven by measurement noise, a generalization of classical Brownian motion. Such continuous monitoring may trap the measured quantum system in a decoherence-free subspace, a fraction of the available state space that is isolated from the surroundings, and thus plays an important role in quantum information science. We analytically determine the first-passage time distribution, whose form neither depends on the system Hamiltonian nor on the measurement operator, and is therefore universal. These results provide a general framework to investigate the first-passage statistics of diffusive quantum trajectories.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv Quantum Physics",
      "sourceUrl": "http://export.arxiv.org/rss/quant-ph",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Mutually Unbiased Bases and Orthogonal Latin Squares",
      "link": "https://arxiv.org/abs/2511.03537",
      "description": "arXiv:2511.03537v1 Announce Type: new \nAbstract: In this paper, we prove that the existence of a complete set of mutually unbiased bases (MUBs) in N-dimensional Hilbert space implies the existence of a complete set of mutually orthogonal Latin squares (MOLSs) of order N. In particular, we prove that a complete set of MUBs does not exist in dimension six (the first dimension which is not a power of prime).",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv Quantum Physics",
      "sourceUrl": "http://export.arxiv.org/rss/quant-ph",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "The Converse Madelung Question",
      "link": "https://arxiv.org/abs/2511.03552",
      "description": "arXiv:2511.03552v1 Announce Type: new \nAbstract: We pose the converse Madelung question: not whether Fisher information can reproduce quantum mechanics, but whether it is necessary. We work with minimal, physically motivated axioms on density and phase: locality, probability conservation, Euclidean invariance with a global phase symmetry, reversibility, and convex regularity. Within the resulting class of first order local Hamiltonian field theories, these axioms single out the canonical Poisson bracket on density and phase under the Dubrovin and Novikov assumptions for local hydrodynamic brackets. Using a pointwise, gauge covariant complex change of variables that maps density and phase to a single complex field, we show that the only convex, rotationally invariant, first derivative local functional of the density whose Euler Lagrange term yields a reversible completion that is exactly projectively linear is the Fisher functional. When its coefficient equals Planck constant squared divided by twice the mass, the dynamics reduce to the linear Schrodinger equation. For many body systems, a single local complex structure across sectors enforces the same relation species by species, fixing a single Planck constant. Galilean covariance appears through the Bargmann central extension, with the usual superselection consequences. Comparison with the Doebner and Goldin family identifies the reversible zero diffusion corner with linear Schrodinger dynamics. We provide operational falsifiers via residual diagnostics for the continuity and Hamilton Jacobi equations and report numerical minima at the Fisher scale that are invariant under Galilean boosts. In this setting, quantum mechanics emerges as a reversible fixed point of Fisher regularised information hydrodynamics. A code archive enables direct numerical checks, including a superposition stress test that preserves exact projective linearity within our axioms.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv Quantum Physics",
      "sourceUrl": "http://export.arxiv.org/rss/quant-ph",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Quantum error mitigation using energy sampling and extrapolation enhanced Clifford data regression",
      "link": "https://arxiv.org/abs/2511.03556",
      "description": "arXiv:2511.03556v1 Announce Type: new \nAbstract: Error mitigation is essential for the practical implementation of quantum algorithms on noisy intermediate-scale quantum (NISQ) devices. This work explores and extends Clifford Data Regression (CDR) to mitigate noise in quantum chemistry simulations using the Variational Quantum Eigensolver (VQE). Using the H$_4$ molecule with the tiled Unitary Product State (tUPS) ansatz, we perform noisy simulations with the ibm torino noise model to investigate in detail the effect of various hyperparameters in CDR on the error mitigation quality. Building on these insights, two improvements to the CDR framework are proposed. The first, Energy Sampling (ES), improves performance by selecting only the lowest-energy training circuits for regression, thereby further biasing the sample energies toward the target state. The second, Non-Clifford Extrapolation (NCE), enhances the regression model by including the number of non-Clifford parameters as an additional input, enabling the model to learn how the noisy-ideal mapping evolves as the circuit approaches the optimal one. Our numerical results demonstrate that both strategies outperform the original CDR.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv Quantum Physics",
      "sourceUrl": "http://export.arxiv.org/rss/quant-ph",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Atom-Field Non-Markovian Dynamics in Open and Dissipative Systems: An Efficient Memory-Kernel Approach Linked to Dyadic Greens Function and CEM Treatments",
      "link": "https://arxiv.org/abs/2511.03561",
      "description": "arXiv:2511.03561v1 Announce Type: new \nAbstract: In this work, we present a numerical framework for modeling single photon emission from a two level system in open and dissipative systems beyond the Markovian approximation. The method can be readily integrated into standard computational electromagnetic (CEM) solvers such as finite difference time domain (FDTD) and finite element method (FEM). We numerically verify the completeness of boundary and medium assisted modes in the modified Langevin noise formalism by reconstructing the imaginary part of the dyadic Greens function through modal expansion in three dimensions. This reconstruction enables a first principles description of atom field interaction via the multi mode Jaynes Cummings model in open and dissipative environments. Within the single excitation manifold, we show that the memory kernel of a two level system is determined by the imaginary part of the Greens function, implying that radiative modes alone govern the relevant dynamics. The proposed framework thus provides a Greens function based approach for describing atomic population and single photon dynamics, directly compatible with Maxwell solvers. We then present concrete strategies for implementing our method in both FDTD and FEM frameworks, demonstrating its practical applicability. We further verify numerical results for a lossy Lorentz Drude type mirror, including both the case of a TLS near a finite sized metallic mirror and that of a TLS centered in a Fabry Perot cavity. This work establishes a rigorous foundation for incorporating quantum emitter dynamics into computational electromagnetics, thereby extending classical solvers toward quantum light matter interactions.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv Quantum Physics",
      "sourceUrl": "http://export.arxiv.org/rss/quant-ph",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Spontaneous symmetry breaking in nonlinear superradiance",
      "link": "https://arxiv.org/abs/2511.03590",
      "description": "arXiv:2511.03590v1 Announce Type: new \nAbstract: Creation and manipulation of non-classical states of light is rapidly becoming the focus of modern attosecond science. Here, we demonstrate numerically how such states can arise by considering a modification of the well-known problem of superradiance encountered already by Dicke. Similarly to him, we investigate photon emission by ensembles of indistinguishable atoms. In contrast to him, however, we leverage symmetry-based selection rules to suppress emission of single photons by single atoms. A steady state is therefore only reached following a spontaneous transition into a collective symmetry-broken state of atoms and photonic modes. The novel non-Markovian, non-perturbative method applied allows us to observe a large quantum state of light form and exhibit drastically non-classical statistics once the system undergoes a symmetry-breaking transition.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv Quantum Physics",
      "sourceUrl": "http://export.arxiv.org/rss/quant-ph",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Directional quantum walks of two bosons on the Hatano-Nelson lattice",
      "link": "https://arxiv.org/abs/2511.03613",
      "description": "arXiv:2511.03613v1 Announce Type: new \nAbstract: We theoretically investigate the interplay of interactions and non-Hermiticity in the dynamics of two bosons on the one-dimensional Hatano-Nelson lattice with non-reciprocal tunneling. We find that the non-reciprocity in the tunneling leads to the formation of an asymmetric density cone during the time-evolution of the system; the degree of asymmetry can be tuned by tuning the non-reciprocity parameter, $\\delta$. Next, we analyze the dynamics of this system in the presence of a static external force and demonstrate that non-Hermiticity leads to asymmetric two-particle Bloch oscillations. Interestingly, when $F=0$ ($F \\ne 0$), strong interactions leads to the formation of an inner density-cone (density-hourglass) structure; this inner structure also becomes asymmetric in the presence of non-Hermiticity. We further analyze the spatial correlations and establish that the system exhibits non-reciprocal bunching (anti-bunching) in the presence of weak (strong) interactions. Finally, we examine the growth of the Quantum Fisher Information, $F_Q$, with time, and demonstrate that $F_Q \\propto t^{\\alpha}$ where $\\alpha \\sim 3$. This feature persists for both one- and two-particle walks, thereby demonstrating that this system can be employed as a quantum-enhanced sensor for detecting weak forces.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv Quantum Physics",
      "sourceUrl": "http://export.arxiv.org/rss/quant-ph",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Annual-modulation fingerprint of the axion wind induced sideband triplet in quantum dot spin qubit sensors",
      "link": "https://arxiv.org/abs/2511.03630",
      "description": "arXiv:2511.03630v1 Announce Type: new \nAbstract: We propose a phase-coherent, narrowband magnetometer for searching couplings between axions or axion-like particles (ALPs) and electron spins, using gate-defined silicon quantum-dot spin qubits. With repeated Ramsey echo sequences and dispersive readout, the qubit precession response can be tracked with sub-Hz spectral resolution. The accessible axion mass window is determined using a series of filtering protocols that take into account sensing noise, including readout errors and $1/f$ noise. We demonstrate clear evidence of sidereal modulation of the signal due to Earth's rotation, while Earth's orbital motion produces an annual amplitude envelope that generates sidebands at fixed frequency spacing $\\pm \\Omega_\\oplus$ around the sidereal component. For axion masses between $1$-$10~\\mu{\\rm eV}$, the proposed method covers axion-electron coupling strengths $g_{ae}$ ranging from $10^{-14}$ to $10^{-10}$. Including both daily and annual modulation patterns in the likelihood analysis enhances the rejection of stationary or instrumental noise. Our results indicate that spin-qubit magnetometry can achieve sensitivities approaching those suggested by astrophysical considerations, providing a complementary, laboratory-based probe of axion-electron interactions. Although we focus on silicon spin-qubit architectures, the approach is broadly applicable to spin-based quantum sensors.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv Quantum Physics",
      "sourceUrl": "http://export.arxiv.org/rss/quant-ph",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Correlation-Powered Work: Equivalence in Peak Yield, Differences in Robustness",
      "link": "https://arxiv.org/abs/2511.03679",
      "description": "arXiv:2511.03679v1 Announce Type: new \nAbstract: Initial system-environment correlations are a thermodynamic resource, enabling work extraction via their erasure. We compare the work potential of classical, quantum, and hypothetical stronger-than-quantum correlations as a function of measurement misalignment. While all models can yield a peak extractable work of k_B T ln 2, corresponding to a mutual information of ln 2, their value as a resource differs critically in its robustness. The classical resource is fragile, decaying linearly with misalignment, whereas the quantum resource is robust, decaying only quadratically. Thus, the degree of nonlocality maps not to the maximum energetic value of a correlation, but to its operational robustness as a thermodynamic fuel.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv Quantum Physics",
      "sourceUrl": "http://export.arxiv.org/rss/quant-ph",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Certified randomness amplification by dynamically probing remote random quantum states",
      "link": "https://arxiv.org/abs/2511.03686",
      "description": "arXiv:2511.03686v1 Announce Type: new \nAbstract: Cryptography depends on truly unpredictable numbers, but physical sources emit biased or correlated bits. Quantum mechanics enables the amplification of imperfect randomness into nearly perfect randomness, but prior demonstrations have required physically co-located, loophole-free Bell tests, constraining the feasibility of remote operation. Here we realize certified randomness amplification across a network by dynamically probing large, entangled quantum states on Quantinuum's 98-qubit Helios trapped-ion quantum processor. Our protocol is secure even if the remote device acts maliciously or is compromised by an intercepting adversary, provided the samples are generated quickly enough to preclude classical simulation of the quantum circuits. We stream quantum gates in real time to the quantum processor, maintain quantum state coherence for $\\approx 0.9$ seconds, and then reveal the measurement bases to the quantum processor only milliseconds before measurement. This limits the time for classical spoofing to 30 ms and constrains the location of hypothetical adversaries to a $4{,}500$ km radius. We achieve a fidelity of 0.586 on random circuits with 64 qubits and 276 two-qubit gates, enabling the amplification of realistic imperfect randomness with a low entropy rate into nearly perfect randomness.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv Quantum Physics",
      "sourceUrl": "http://export.arxiv.org/rss/quant-ph",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Frequency shifts as a reflection of ground state squeezing and entanglement in two coupled harmonic oscillators",
      "link": "https://arxiv.org/abs/2511.03687",
      "description": "arXiv:2511.03687v1 Announce Type: new \nAbstract: It is often argued that two coupled quantum harmonic oscillators, even when cooled to their ground state, display no inherently quantum features beyond quantized energy levels. Here, we challenge this view by showing that their classical observables can encode genuinely quantum features. In particular, we demonstrate that the characteristic frequency shifts observed in such systems act as a signature of non-classical correlations and ground-state entanglement at zero temperature, specifically two-mode squeezing between the uncoupled modes. From a complementary perspective, these two effects -- frequency shifts and squeezing -- represent the same underlying phenomenon expressed in different mode bases. What appears as a spectral renormalization in one description manifests as entanglement in another. These shifts therefore can serve as an entanglement witness accessible via standard frequency measurements. Furthermore, we show that this underlying squeezing, although not directly measurable, can be exploited to enhance the signal-to-noise ratio in precision frequency measurements of individual oscillators without requiring squeezed quantum noise. Our results uncover a new route to quantum-enhanced sensing within systems traditionally regarded as classical, offering fresh insight into how signatures of quantumness persist across the quantum-to-classical boundary.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv Quantum Physics",
      "sourceUrl": "http://export.arxiv.org/rss/quant-ph",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Realization of a Quantum Streaming Algorithm on Long-lived Trapped-ion Qubits",
      "link": "https://arxiv.org/abs/2511.03689",
      "description": "arXiv:2511.03689v1 Announce Type: new \nAbstract: Large classical datasets are often processed in the streaming model, with data arriving one item at a time. In this model, quantum algorithms have been shown to offer an unconditional exponential advantage in space. However, experimentally implementing such streaming algorithms requires qubits that remain coherent while interacting with an external data stream. In this work, we realize such a data-streaming model using Quantinuum Helios trapped-ion quantum computer with long-lived qubits that communicate with an external server. We implement a quantum pair sketch, which is the primitive underlying many quantum streaming algorithms, and use it to solve Hidden Matching, a problem known to exhibit a theoretical exponential quantum advantage in space. Furthermore, we compile the quantum streaming algorithm to fault-tolerant quantum architectures based on surface and bivariate bicycle codes and show that the quantum space advantage persists even with the overheads of fault-tolerance.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv Quantum Physics",
      "sourceUrl": "http://export.arxiv.org/rss/quant-ph",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "A Transferable Machine Learning Approach to Predict Quantum Circuit Parameters for Electronic Structure Problems",
      "link": "https://arxiv.org/abs/2511.03726",
      "description": "arXiv:2511.03726v1 Announce Type: new \nAbstract: The individual optimization of quantum circuit parameters is currently one of the main practical bottlenecks in variational quantum eigensolvers for electronic systems. To this end, several machine learning approaches have been proposed to mitigate the problem. However, such method predominantly aims at training and predicting parameters tailored to individual molecules: either a specific structure, or several structures of the same molecule with varying bond lengths. This work explores machine learning based modeling strategies to include transferability between different molecules. We use a well investigated quantum circuit design and apply it to model properties of hydrogenic systems where we show parameter prediction that is systematically transferable to instances significantly larger than the training instances.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv Quantum Physics",
      "sourceUrl": "http://export.arxiv.org/rss/quant-ph",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Universal breathing mode scaling in harmonically trapped Fermi gases",
      "link": "https://arxiv.org/abs/2510.20719",
      "description": "arXiv:2510.20719v1 Announce Type: cross \nAbstract: We derive universal, experiment ready analytic laws for the breathing (monopole) mode of harmonically trapped Fermi gases. Within a fixed hyperangular channel $s>0$, contact-weighted products of associated Laguerre polynomials reduce to an elementary gamma ratio, yielding: (i) a level resolved fractional breathing mode shift with scaling $\\delta\\omega/(2\\omega)\\propto Q^{-1}$, where $Q\\equiv 2q+s+1$, with $q$ the radial quantum number; (ii) a first order quantum anomaly correction involving exactly two intermediate states, producing a $Q^{-2}$ falloff of the leaked monopole strength with an explicit prefactor; and (iii) a closed form finite temperature average exhibiting a low-$T$ plateau and a $1/T$ high-$T$ tail. We also obtain a mixed anomaly\\nobreakdash-quartic correction for weak anharmonicity. All expressions become parameter free after a single per-channel calibration of the Tan contact $\\lambda_s$ at $q=0$.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv Quantum Physics",
      "sourceUrl": "http://export.arxiv.org/rss/quant-ph",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Enhancing Federated Learning Privacy with QUBO",
      "link": "https://arxiv.org/abs/2511.02785",
      "description": "arXiv:2511.02785v1 Announce Type: cross \nAbstract: Federated learning (FL) is a widely used method for training machine learning (ML) models in a scalable way while preserving privacy (i.e., without centralizing raw data). Prior research shows that the risk of exposing sensitive data increases cumulatively as the number of iterations where a client's updates are included in the aggregated model increase. Attackers can launch membership inference attacks (MIA; deciding whether a sample or client participated), property inference attacks (PIA; inferring attributes of a client's data), and model inversion attacks (MI; reconstructing inputs), thereby inferring client-specific attributes and, in some cases, reconstructing inputs. In this paper, we mitigate risk by substantially reducing per client exposure using a quantum computing-inspired quadratic unconstrained binary optimization (QUBO) formulation that selects a small subset of client updates most relevant for each training round. In this work, we focus on two threat vectors: (i) information leakage by clients during training and (ii) adversaries who can query or obtain the global model. We assume a trusted central server and do not model server compromise. This method also assumes that the server has access to a validation/test set with global data distribution. Experiments on the MNIST dataset with 300 clients in 20 rounds showed a 95.2% per-round and 49% cumulative privacy exposure reduction, with 147 clients' updates never being used during training while maintaining in general the full-aggregation accuracy or even better. The method proved to be efficient at lower scale and more complex model as well. A CINIC-10 dataset-based experiment with 30 clients resulted in 82% per-round privacy improvement and 33% cumulative privacy.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv Quantum Physics",
      "sourceUrl": "http://export.arxiv.org/rss/quant-ph",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Heisenberg's S-matrix program and Feynman's divergence problem",
      "link": "https://arxiv.org/abs/2511.02847",
      "description": "arXiv:2511.02847v1 Announce Type: cross \nAbstract: In the present article, we assume that the first approximation of the scattering operator is given and that it has the logarithmic divergence. This first approximation allows us to construct the so called deviation factor. Using the deviation factor, we regularize all terms of the scattering operator's approximations. The infrared and ultraviolet cases as well as concrete examples are considered. Thus, for a wide range of cases, we provide a positive answer to the well-known problem of J. R. Oppenheimer regarding scattering operators in QED: ``Can the procedure be freed of the expansion in $\\varepsilon$ and carried out rigorously?\"",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv Quantum Physics",
      "sourceUrl": "http://export.arxiv.org/rss/quant-ph",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Structure and interactions of atoms and diatomic molecules: from ultracold gases to doped solids",
      "link": "https://arxiv.org/abs/2511.02890",
      "description": "arXiv:2511.02890v1 Announce Type: cross \nAbstract: This is the manuscript of my \"Habilitation \\`a diriger des recherches\", where I present the research work that I have done after my PhD, defended in 2009. The manuscript is divided in two parts. The first one is dedicated to atomic-structure calculations with neutral and trivalent lanthanides, in the contexts of ultracold gases and rare-earth doped solids. The second part deals with long-range interactions in ultracold gases of alkali-metal atoms and diatomic molecules, as well as lanthanide atoms. The detailed description of long-range interactions serves to characterize ultralow-temperature phenomena, like photoassociation and collisional shielding.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv Quantum Physics",
      "sourceUrl": "http://export.arxiv.org/rss/quant-ph",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Revisiting Nishimori multicriticality through the lens of information measures",
      "link": "https://arxiv.org/abs/2511.02907",
      "description": "arXiv:2511.02907v1 Announce Type: cross \nAbstract: The quantum error correction threshold is closely related to the Nishimori physics of random statistical models. We extend quantum information measures such as coherent information beyond the Nishimori line and establish them as sharp indicators of phase transitions. We derive exact inequalities for several generalized measures, demonstrating that each attains its extremum along the Nishimori line. Using a fermionic transfer matrix method, we compute these quantities in the 2d $\\pm J$ random-bond Ising model-corresponding to a surface code under bit-flip noise-on system sizes up to $512$ and over $10^7$ disorder realizations. All critical points extracted from statistical and information-theoretic indicators coincide with high precision at $p_c=0.1092212(4)$, with the coherent information exhibiting the smallest finite-size effects. We further analyze the domain-wall free energy distribution and confirm its scale invariance at the multicritical point.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv Quantum Physics",
      "sourceUrl": "http://export.arxiv.org/rss/quant-ph",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Automorphisms with growing generators",
      "link": "https://arxiv.org/abs/2511.02941",
      "description": "arXiv:2511.02941v1 Announce Type: cross \nAbstract: We prove global existence and uniqueness of Heisenberg dynamics on the quasi-local algebra of an extended quantum lattice system for spatially growing generators. Existing results assume that the local terms of the generator decay fast enough and are bounded uniformly in space and time. We show, in analogy to global existence results for first order ODEs, that global existence and uniqueness still hold true if the local terms grow at most linearly in space. Moreover, we obtain Lieb-Robinson bounds with exponential light cones for the generated dynamics.\n  For the proof, we mainly assume Lieb-Robinson bounds with linear light cones for dynamics generated by uniformly bounded local terms. These are known to hold for example if the local terms are exponentially localized.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv Quantum Physics",
      "sourceUrl": "http://export.arxiv.org/rss/quant-ph",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Improving the Energy and Angular Resolutions of X-ray Telescopes with Nitrogen-Vacancy Centers in Diamond",
      "link": "https://arxiv.org/abs/2511.02961",
      "description": "arXiv:2511.02961v1 Announce Type: cross \nAbstract: We introduce a focal-plane detector for advancing the energy and angular resolutions of current X-ray telescopes. The architecture integrates a metallic magnetic microcalorimeter (MMC) array of paramagnetic absorber pads with a thin layer of nitrogen-vacancy (NV) centers in diamond for simultaneous optical readout. An impinging X-ray photon induces a temperature transient in an absorber pad, kept at ~35 mK. This time- and temperature-dependent magnetic field transient is then optically imaged by diamond NV centers, kept at 4 K and positioned directly below the pad. For a 10 $\\mu$m absorber length used with a 12 m focal length telescope, our design yields an optimal angular resolution of ~0.17 arcseconds and energy resolution of ~0.70 eV. Our NV-MMC design improves upon current transition-edge sensors (TES) or MMCs read-out by superconducting quantum interference devices (SQUID) by enabling simultaneous optical readout of the entire MMC array. Because no additional cryogenic multiplexing electronics are required, our approach scales naturally to larger and finer arrays, supporting finer angular resolutions and wider fields of view.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv Quantum Physics",
      "sourceUrl": "http://export.arxiv.org/rss/quant-ph",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Exploring the mechanisms of transverse relaxation of copper(II)-phthalocyanine spin qubits",
      "link": "https://arxiv.org/abs/2511.03199",
      "description": "arXiv:2511.03199v1 Announce Type: cross \nAbstract: Molecular spin qubits are promising candidates for quantum technologies, but their performance is limited by decoherence arising from diverse mechanisms. The complexity of the environment makes it challenging to identify the main source of noise and target it for mitigation. Here we present a systematic experimental and theoretical framework for analyzing the mechanisms of transverse relaxation in copper(II) phthalocyanine (CuPc) diluted into diamagnetic phthalocyanine hosts. Using pulsed EPR spectroscopy together with first-principles cluster correlation expansion simulations, we quantitatively separate the contributions from hyperfine-coupled nuclear spins, spin--lattice relaxation, and electron--electron dipolar interactions. Our detailed modeling shows that both strongly and weakly coupled nuclei contribute negligibly to $T_2$, while longitudinal dipolar interactions with electronic spins, through instantaneous and spectral diffusion, constitute the main decoherence channel even at moderate spin densities. This conclusion is validated by direct comparison between simulated spin-echo dynamics and experimental data. By providing a robust modeling and experimental approach, our work identifies favorable values of the electron spin density for quantum applications, and provides a transferable methodology for predicting ensemble coherence times. These insights will guide the design and optimization of molecular spin qubits for scalable quantum devices.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv Quantum Physics",
      "sourceUrl": "http://export.arxiv.org/rss/quant-ph",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Bistability and Exact Reflectionless States in Nonlinear Scattering of a Bose--Einstein Condensate",
      "link": "https://arxiv.org/abs/2511.03233",
      "description": "arXiv:2511.03233v1 Announce Type: cross \nAbstract: We investigate the mean-field scattering dynamics of a quasi-one-dimensional Bose--Einstein condensate interacting with a Rosen--Morse potential. For specific potential and nonlinearity parameters, we derive analytically exact, degenerate scattering states (doubly or triply degenerate) exhibiting perfect transmission. Using the Bogoliubov--de Gennes approach, we analyze the stability of these reflectionless degenerate states, demonstrating that only one solution within each degenerate manifold is dynamically stable. Furthermore, we study a configuration with spatially localized nonlinearity, identifying an exact reflectionless state under specific conditions. Numerical analysis shows that this state marks the system's transition from monostability to bistability as the incident wave amplitude increases. Our work establishes an analytic framework for these multistable transmission phenomena, directly relevant to coherent matter-wave transport in ultracold atomic systems and optical propagation in engineered photonic lattices.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv Quantum Physics",
      "sourceUrl": "http://export.arxiv.org/rss/quant-ph",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Quantum Error Correction-like Noise Mitigation for Wave-like Dark Matter Searches with Quantum Sensors",
      "link": "https://arxiv.org/abs/2511.03253",
      "description": "arXiv:2511.03253v1 Announce Type: cross \nAbstract: We propose a quantum error correction-like noise mitigation protocol for enhancing the sensitivity of wave-like dark matter searches with quantum sensors. Our protocol uses multiple sensors to mitigate the noise affecting each sensor individually, allowing for the suppression of excitation noise that is parallel to the dark matter signal. We demonstrate that our protocol can improve the sensitivity to dark matter signals by a factor of $\\sqrt{N}$, where $N$ is the number of sensors used. Furthermore, we find that our protocol achieves the same performance as the standard quantum limit by the ideal measurement, which is impossible to achieve due to the unknown phase of the dark matter field. Our work can be widely applied to various types of signals with unknown phases, and has the potential to enhance the sensitivity of quantum sensors such as arrays of resonant cavities.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv Quantum Physics",
      "sourceUrl": "http://export.arxiv.org/rss/quant-ph",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Isolated quantum-state networks in ultracold molecules",
      "link": "https://arxiv.org/abs/2511.03324",
      "description": "arXiv:2511.03324v1 Announce Type: cross \nAbstract: Precise control over rotational angular momentum is at the heart of recent advances in quantum chemistry, quantum simulation, and quantum computation with ultracold bialkali molecules. Each rotational state comprises a rich manifold of hyperfine states arising from combinations of rotation and nuclear spins; this often yields hundreds of transitions available between a given pair of rotational states, and the efficient navigation of this complex space is a current challenge for experiments. Here, we describe a general approach based on a simple heuristic and graph theory to quickly identify optimal sets of states in ultracold bialkali molecules. We explain how to find pathways through the many available transitions to prepare the molecule in a specific state with maximum speed for any desired fidelity. We then examine networks of states where multiple couplings are present at the same time. As example applications, we first identify a closed loop of four states in the RbCs molecule where there is minimal population leakage out of the loop during simultaneous microwave coupling; we then extend the optimisation procedure to account for decoherence induced by magnetic-field noise and obtain an optimal set of 3 states for quantum computation applications.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv Quantum Physics",
      "sourceUrl": "http://export.arxiv.org/rss/quant-ph",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Integrability of a family of clean SYK models from the critical Ising chain",
      "link": "https://arxiv.org/abs/2511.03460",
      "description": "arXiv:2511.03460v1 Announce Type: cross \nAbstract: We establish the integrability of a family of SYK models with uniform $p$-body interactions. We derive the R-matrix and mutually commuting transfer matrices that generate the Hamiltonians of these models, and obtain their exact eigenspectra and eigenstates. Remarkably, the R-matrix is that of the critical transverse-field Ising chain. This work reveals an unexpected connection between the SYK model, central to many-body quantum chaos, and the critical Ising chain, a cornerstone of statistical mechanics.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv Quantum Physics",
      "sourceUrl": "http://export.arxiv.org/rss/quant-ph",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Inertial Repulsion from Quantum Geometry",
      "link": "https://arxiv.org/abs/2511.03510",
      "description": "arXiv:2511.03510v1 Announce Type: cross \nAbstract: We derive a repulsive, charge-dipole-like interaction for a Dirac particle in a rotating frame, arising from a geometric $U(1)$ gauge symmetry associated with the Berry phase. The Lagrangian of this system includes a non-inertial correction due to centrifugal field coupling. By imposing gauge symmetry and treating it as a full gauge theory, the Lagrangian is extended to include Berry connection and curvature terms. Upon integrating out the geometric gauge field, the effective action is obtained. This leads to the emergence of a repulsive, long-range effective interaction in the Lagrangian. Explicitly, in the non-inertial frame of the observer, the geometric gauge invariance effectively leads to a repulsive Coulomb-interaction in momentum space. In real space, the inertial repulsion manifests in a $1/\\vert r\\vert^{2}$ potential, which is symmetric about the origin of rotation and mirrors charge-dipole interaction.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv Quantum Physics",
      "sourceUrl": "http://export.arxiv.org/rss/quant-ph",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Quantum effects in the magnon spectrum of 2D altermagnets via continuous similarity transformations",
      "link": "https://arxiv.org/abs/2511.03528",
      "description": "arXiv:2511.03528v1 Announce Type: cross \nAbstract: We investigate quantum effects on magnon excitations in a minimal spin-1/2 Heisenberg model for 2D altermagnets on the square lattice. A continuous similarity transformation is applied in momentum space to derive an effective Hamiltonian that conserves the number of magnon excitations. This allows us to quantitatively calculate the one-magnon dispersion, the effects of magnon-magnon interactions, and the dynamic structure factor in a certain range of parameters. In particular, we focus on the altermagnetic spin splitting of the magnon bands and the size of the roton minimum. We further map out divergencies of the continuous similarity transformation for different types of generators, which signal either the breakdown of the N\\'eel-ordered phase or the presence of significant magnon decay.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv Quantum Physics",
      "sourceUrl": "http://export.arxiv.org/rss/quant-ph",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Encoding electronic ground-state information with variational even-tempered basis sets",
      "link": "https://arxiv.org/abs/2511.03579",
      "description": "arXiv:2511.03579v1 Announce Type: cross \nAbstract: We propose a system-oriented basis-set design based on even-tempered basis functions to variationally encode electronic ground-state information into molecular orbitals. First, we introduce a reduced formalism of concentric even-tempered orbitals that achieves hydrogen energy accuracy on par with the conventional formalism, with lower optimization cost and improved scalability. Second, we propose a symmetry-adapted, even-tempered formalism specifically designed for molecular systems. It requires only primitive S-subshell Gaussian-type orbitals and uses two parameters to characterize all exponent coefficients. In the case of the diatomic hydrogen molecule, the basis set generated by this formalism produces a dissociation curve more consistent with cc-pV5Z than cc-pVTZ at the size of aug-cc-pVDZ. Finally, we test our even-tempered formalism against several types of tetra-atomic hydrogen molecules for ground-state computation and point out its current limitations and potential improvements.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv Quantum Physics",
      "sourceUrl": "http://export.arxiv.org/rss/quant-ph",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Stability of the Quantum Coherent Superradiant States in Relation to Exciton-Phonon Interactions and the Fundamental Soliton in Hybrid Perovskites",
      "link": "https://arxiv.org/abs/2511.03600",
      "description": "arXiv:2511.03600v1 Announce Type: cross \nAbstract: The use of macroscopic coherent quantum states at room temperature is crucial in modern quantum technologies. In light of recent experiments demonstrating high-temperature superfluorescence in hybrid perovskite thin films, in this work we investigate the stability of the superradiant state concerning exciton-phonon interactions. We focused on a quasi-2D Wannier exciton interacting with longitudinal optical (LO) phonons in polar crystals, as well as with acoustic phonons. Our study leads to the derivation of nonlinear equations in the coordinate space that govern the exciton wavefunction's coefficient in the single-exciton basis for the lowest exciton state, which translates to the complex-valued polarization. The resulting equations take the form of a 2D nonlocal nonlinear Schrodinger (NLS) equation. We perform a linear stability analysis of the plane wave solutions for the equations in question, which allows us to establish stability criteria. This analysis is particularly important for evaluating the stability of the superradiant state in the considered quasi-2D structures, as the superradiant state represents a specific case of the plane wave solution. Our findings indicate that, when the exciton interacts with LO phonons, a plane wave solution is modulationally stable, provided that the square of its amplitude does not exceed a critical intensity value defined by the exciton-LO phonon interaction parameters. Furthermore, interactions between the exciton and acoustic phonons reduce the intensity of modulationally stable waves compared to the case without such interactions. Our analytical results are corroborated by numerical calculations. We also numerically solve the 2D nonlocal NLS equation in the polar coordinates and obtain its fundamental soliton solution, which is stable.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv Quantum Physics",
      "sourceUrl": "http://export.arxiv.org/rss/quant-ph",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Introducing Quantum Computing into Statistical Physics: Random Walks and the Ising Model with Qiskit",
      "link": "https://arxiv.org/abs/2511.03696",
      "description": "arXiv:2511.03696v1 Announce Type: cross \nAbstract: Quantum computing offers a powerful new perspective on probabilistic and collective behaviors traditionally taught in statistical physics. This paper presents two classroom-ready modules that integrate quantum computing into the undergraduate curriculum using Qiskit: the quantum random walk and the Ising model. Both modules allow students to simulate and contrast classical and quantum systems, deepening their understanding of concepts such as superposition, interference, and statistical distributions. We outline the quantum circuits involved, provide sample code and student activities, and discuss how each example can be used to enhance student engagement with statistical physics. These modules are suitable for integration into courses in statistical mechanics, modern physics, or as part of an introductory unit on quantum computing.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv Quantum Physics",
      "sourceUrl": "http://export.arxiv.org/rss/quant-ph",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Superresolving collective quantum measurements",
      "link": "https://arxiv.org/abs/2110.00986",
      "description": "arXiv:2110.00986v5 Announce Type: replace \nAbstract: We demonstrate a method for super-resolving signals encoded as finite mixtures of bosonic modes using collective measurements that exploit permutation symmetry. Specifically, we use multiple copies of the state $\\rho$ of the finite mixture to extract an estimate for the purity of $\\rho$ via a spectrum measurement, the weak Schur-sampling measurement. Depending on the outcome we then further fine-grain the measurement to optimally extract an estimate of the relative intensity between the two incoherent mixtures. Our protocol furnishes simultaneous estimates for both the relative intensity and the separation of incoherent signals saturating the multi-parameter Cram\\'{e}r-Rao bound, and is robust against misalignment errors. We also provide viable experimental avenues for implementing such collective measurements in different set-ups.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv Quantum Physics",
      "sourceUrl": "http://export.arxiv.org/rss/quant-ph",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Experimental high-dimensional entanglement certification and quantum steering with time-energy measurements",
      "link": "https://arxiv.org/abs/2310.20694",
      "description": "arXiv:2310.20694v3 Announce Type: replace \nAbstract: High-dimensional entanglement provides unique ways of transcending the limitations of current approaches in quantum information processing, quantum communications based on qubits. The generation of time-frequency qudit states offer significantly increased quantum capacities while keeping the number of photons constant, but pose significant challenges regarding the possible measurements for certification of entanglement. Here, we develop a new scheme and experimentally demonstrate the certification of 24-dimensional entanglement and a 9-dimensional quantum steering. We then subject our photon-pairs to dispersion conditions equivalent to the transmission through 600-km of fiber and still certify 21-dimensional entanglement. Furthermore, we use a steering inequality to prove 7-dimensional entanglement in a semi-device independent manner, proving that large chromatic dispersion is not an obstacle in distributing and certifying high-dimensional entanglement and quantum steering. Our approach, leveraging intrinsic large-alphabet nature of telecom-band photons, enables scalable, commercially viable, and field-deployable entangled and steerable quantum sources, providing a pathway towards fully scalable quantum information processer and high-dimensional quantum communication networks.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv Quantum Physics",
      "sourceUrl": "http://export.arxiv.org/rss/quant-ph",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Optimal finite-dimensional probe states for quantum phase estimation",
      "link": "https://arxiv.org/abs/2312.01965",
      "description": "arXiv:2312.01965v4 Announce Type: replace \nAbstract: Phase estimation is a major mission in quantum metrology, especially in quantum interferometry. A full phase estimation scheme usually includes the optimal probe state and measurement. For the finite-dimensional states in Fock basis, the N00N state ceases to be optimal when the average particle number is fixed yet not equal to the Fock dimension (Fock number of the highest occupied Fock state of one mode), and what is the true optimal finite-dimensional probe state in this case is still undiscovered. Hereby we present several theorems to answer this question and provide a complete optimal scheme to realize the ultimate precision limit in practice. These optimal finite-dimensional probe states reveal an important fact that the Fock dimension could be treated as a metrological resource, and the given scheme is particularly useful in scenarios where weak light or limited particle number is demanded.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv Quantum Physics",
      "sourceUrl": "http://export.arxiv.org/rss/quant-ph",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Multi-qubit DC gates over an inhomogeneous array of quantum dots",
      "link": "https://arxiv.org/abs/2403.06894",
      "description": "arXiv:2403.06894v3 Announce Type: replace \nAbstract: The prospect of large-scale quantum computation with an integrated chip of spin qubits is imminent as technology improves. This invites us to think beyond the traditional 2-qubit-gate framework and consider a naturally supported ``instruction set'' of multi-qubit gates. In this work, we systematically study such a family of multi-qubit gates implementable over an array of quantum dots by DC evolution. A useful representation of the computational Hamiltonian is proposed for a dot-array with strong spin-orbit coupling effects, distinctive $g$-factor tensors and varying interdot couplings. Adopting a perturbative treatment, we model a multi-qubit DC gate by the first-order dynamics in the qubit frame and develop a detailed formalism for decomposing the resulting gate, estimating and optimizing the coherent gate errors with appropriate local phase shifts for arbitrary array connectivity. Examples of such multi-qubit gates and their applications in quantum error correction and quantum algorithms are also explored, demonstrating their potential advantage in accelerating complex tasks and reducing overall errors.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv Quantum Physics",
      "sourceUrl": "http://export.arxiv.org/rss/quant-ph",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "A Cross-Platform Execution Engine for the Quantum Intermediate Representation",
      "link": "https://arxiv.org/abs/2404.14299",
      "description": "arXiv:2404.14299v2 Announce Type: replace \nAbstract: Hybrid languages like the quantum intermediate representation (QIR) are essential for programming systems that mix quantum and conventional computing models, while execution of these programs is often deferred to a system-specific implementation. Here, we develop the QIR Execution Engine (QIR-EE) for parsing, interpreting, and executing QIR across multiple hardware platforms. QIR-EE uses LLVM to execute hybrid instructions specifying quantum programs and, by design, presents extension points that support customized runtime and hardware environments. We demonstrate an implementation that uses the XACC quantum hardware-accelerator library to dispatch prototypical quantum programs on different commercial quantum platforms and numerical simulators, and we validate execution of QIR-EE on IonQ, Quantinuum, and IBM hardware. Our results highlight the efficiency of hybrid executable architectures for handling mixed instructions, managing mixed data, and integrating with quantum computing frameworks to realize cross-platform execution.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv Quantum Physics",
      "sourceUrl": "http://export.arxiv.org/rss/quant-ph",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Contraction of Private Quantum Channels and Private Quantum Hypothesis Testing",
      "link": "https://arxiv.org/abs/2406.18651",
      "description": "arXiv:2406.18651v3 Announce Type: replace \nAbstract: A quantum generalized divergence by definition satisfies the data-processing inequality; as such, the relative decrease in such a divergence under the action of a quantum channel is at most one. This relative decrease is formally known as the contraction coefficient of the channel and the divergence. Interestingly, there exist combinations of channels and divergences for which the contraction coefficient is strictly less than one. Furthermore, understanding the contraction coefficient is fundamental for the study of statistical tasks under privacy constraints. To this end, here we establish upper bounds on contraction coefficients for the hockey-stick divergence under privacy constraints, where privacy is quantified with respect to the quantum local differential privacy (QLDP) framework, and we fully characterize the contraction coefficient for the trace distance under privacy constraints. With the machinery developed, we also determine an upper bound on the contraction of both the Bures distance and quantum relative entropy relative to the normalized trace distance, under QLDP constraints. Next, we apply our findings to establish bounds on the sample complexity of quantum hypothesis testing under privacy constraints. Furthermore, we study various scenarios in which the sample complexity bounds are tight, while providing order-optimal quantum channels that achieve those bounds. Lastly, we show how private quantum channels provide fairness and Holevo information stability in quantum learning settings.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv Quantum Physics",
      "sourceUrl": "http://export.arxiv.org/rss/quant-ph",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Demonstration of magic state power of $\\mathbf{D}(\\mathbf{S}_{3})$ anyons with two qudits",
      "link": "https://arxiv.org/abs/2408.03377",
      "description": "arXiv:2408.03377v3 Announce Type: replace \nAbstract: We consider a lattice of $d=6$ qudits that supports $\\mathbf{D}(\\mathbf{S}_3)$ non-Abelian anyons. We present a method for implementing both braiding and fusion evolutions using only the operators that create and measure anyons, without requiring additional dynamical control. This provides a minimal protocol demonstrating that $\\mathbf{D}(\\mathbf{S}_3)$ anyons can generate magic states, thereby establishing their universality for quantum computation. Furthermore, we show that the entire scheme can be encoded in just two qudits, offering a compact blueprint that is inherently scalable and readily implementable in current quantum platforms.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv Quantum Physics",
      "sourceUrl": "http://export.arxiv.org/rss/quant-ph",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Fast gradient-free optimization of excitations in variational quantum eigensolvers",
      "link": "https://arxiv.org/abs/2409.05939",
      "description": "arXiv:2409.05939v3 Announce Type: replace \nAbstract: Finding molecular ground states and energies with variational quantum eigensolvers is central to chemistry applications on quantum computers. Physically motivated ans\\\"atze based on excitation operators respect physical symmetries, but existing quantum-aware optimizers, such as Rotosolve, have been limited to simpler operator types. To fill this gap, we introduce ExcitationSolve, a fast quantum-aware optimizer that is globally-informed, gradient-free, and hyperparameter-free. ExcitationSolve extends these optimizers to parameterized unitaries with generators $G$ of the form $G^3=G$ exhibited by excitation operators in approaches such as unitary coupled cluster. ExcitationSolve determines the global optimum along each variational parameter using the same quantum resources that gradient-based optimizers require for one update step. We provide optimization strategies for both fixed and adaptive variational ans\\\"atze, along with generalizations for simultaneously selecting and optimizing multiple excitations. On molecular ground state energy benchmarks, ExcitationSolve outperforms state-of-the-art optimizers by converging faster, achieving chemical accuracy for equilibrium geometries in a single parameter sweep, yielding shallower adaptive ans\\\"atze and remaining robust to real hardware noise. By uniting physical insight with efficient optimization, ExcitationSolve paves the way for scalable quantum chemistry calculations.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv Quantum Physics",
      "sourceUrl": "http://export.arxiv.org/rss/quant-ph",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Reconstructing thermal states using dimensionally limited probes : A Model for Limited Control & Memory in Quantum Thermodynamics",
      "link": "https://arxiv.org/abs/2410.18167",
      "description": "arXiv:2410.18167v2 Announce Type: replace \nAbstract: Whilst the complexity of acquiring knowledge of a quantum state has been extensively studied in the fields of quantum tomography and quantum learning, a physical understanding of its operational role and cost in quantum thermodynamics is lacking. Knowledge is central to thermodynamics, as exemplified by Maxwell's demon thought experiment, where a demonic agent is able to extract paradoxical amounts of work -- reconciled by the thermodynamic costs of acquiring this knowledge. In this work, we address this gap by extending unitary models of measurement to incorporate the resources available to an agent. We view an agent's knowledge of a quantum state as their ability to reconstruct it unitarily given access to states with partial knowledge of the true state. In our model, an agent correlates an unknown $d$-dimensional system, with copies of a $k$-dimensional probe ($k\\leq d$), which are then used to unitarily reconstruct an estimate state in $d$-dimensional memories. We find that this framework is a unitary representation of coarse-grained POVMs. As an application, we investigate the role of knowledge in an extended Szilard Engine scenario.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv Quantum Physics",
      "sourceUrl": "http://export.arxiv.org/rss/quant-ph",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Storage and retrieval of two unknown unitary channels",
      "link": "https://arxiv.org/abs/2410.23376",
      "description": "arXiv:2410.23376v2 Announce Type: replace \nAbstract: We address the fundamental task of converting $n$ uses of an unknown unitary transformation into a quantum state (i.e., storage) and later retrieval of the transformation. Specifically, we consider the case where the unknown unitary is selected with equal prior probability from two options. First, we prove that the optimal storage strategy involves the sequential application of the $n$ uses of the unknown unitary, and it produces the optimal state for discrimination between the two possible unitaries. Next, we show that incoherent \"measure-and-prepare\" retrieval achieves the maximum fidelity between the retrieved operation and the original (qubit) unitary. We then identify the retrieval strategy that maximizes the probability of successfully and perfectly retrieving the unknown transformation. In the regime in which the fidelity between the two possible unitaries is large the probability of success scales as $ P_{succ} = 1 - \\mathcal{O}(n^{-2} ) $, which is a quadratic improvement with respect to the case in which the unitaries are drawn from the entire unitary group $U(d)$ with uniform prior probability. Finally, we present an optical experiment for this approach and assess the storage and retrieval quality using quantum tomography of states and processes. The results are discussed in relation to non-optimal measure-and-prepare strategy, highlighting the advantages of our protocol.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv Quantum Physics",
      "sourceUrl": "http://export.arxiv.org/rss/quant-ph",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Credible-interval-based adaptive Bayesian quantum frequency estimation for entanglement-enhanced atomic clocks",
      "link": "https://arxiv.org/abs/2411.14944",
      "description": "arXiv:2411.14944v2 Announce Type: replace \nAbstract: Entanglement-enhanced quantum sensors encounter a fundamental trade-off: while entanglement improves precision to the Heisenberg limit, it restricts dynamic range. To address this trade-off, we present a credible-interval-based adaptive Bayesian quantum frequency estimation protocol for Greenberger-Horne-Zeilinger (GHZ)-state-based atomic clocks. Our method optimally integrates prior knowledge with new measurements and determines the interrogation time by correlating it with the period of the likelihood function, based on Bayesian credible intervals. Our protocol can be implemented using either individual or cascaded GHZ states, thereby extending the dynamic range without compromising Heisenberg-limited sensitivity. In parallel with the cascaded-GHZ-state protocol using fixed interrogation times, the dynamic range can be extended through an interferometry sequence that employs individual GHZ states with variable interrogation times. Furthermore, by varying the interrogation times, the dynamic range of the cascaded-GHZ-state protocol can be further extended. Crucially, our protocol enables dual Heisenberg-limited precision scaling $\\propto 1/(Nt)$ in both particle number $N$ and total interrogation time $t$, surpassing the hybrid scaling $\\propto 1/{(N\\sqrt {t}})$ of the conventional cascaded-GHZ-state protocol. While offering a wider dynamic range, the protocol is more stable against noise and more robust to dephasing than existing adaptive schemes. Beyond atomic clocks, our approach establishes a general framework for developing entanglement-enhanced quantum sensors that simultaneously achieve both high precision and broad dynamic range.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv Quantum Physics",
      "sourceUrl": "http://export.arxiv.org/rss/quant-ph",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Construction of Superposition States of Energy Eigenstates via Classically Emulated Digital Quantum Simulation: The Hydrogen Molecule as an Example",
      "link": "https://arxiv.org/abs/2412.20672",
      "description": "arXiv:2412.20672v2 Announce Type: replace \nAbstract: We construct superposition states of energy eigenstates of the hydrogen molecule using classically emulated digital quantum simulation. We generate the ground state and excited states of the system via the twirling operation method, and construct superposition states of the ground state and an excited state of the system by applying a controlled excitation unitary transformation on the ground state with one ancillary qubit as the control. To verify the correctness of the resulting superposition state, we calculate matrix elements of several physical observables.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv Quantum Physics",
      "sourceUrl": "http://export.arxiv.org/rss/quant-ph",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "The phase diagram of quantum chromodynamics in one dimension on a quantum computer",
      "link": "https://arxiv.org/abs/2501.00579",
      "description": "arXiv:2501.00579v2 Announce Type: replace \nAbstract: The quantum chromodynamics (QCD) phase diagram, which reveals the state of strongly interacting matter at different temperatures and densities, is key to answering open questions in physics, ranging from the behavior of particles in neutron stars to the conditions of the early universe. However, classical simulations of QCD face significant computational barriers, such as the sign problem at finite matter densities. Quantum computing offers a promising solution to overcome these challenges. Here, we take an important step toward exploring the QCD phase diagram with quantum devices by preparing thermal states in one-dimensional non-Abelian gauge theories. We experimentally simulate the thermal states of SU(2) and SU(3) gauge theories at finite densities on a trapped-ion quantum computer using a variational method. This is achieved by introducing two features: Firstly, we add motional ancillae to the existing qubit register to efficiently prepare thermal probability distributions. Secondly, we introduce charge-singlet measurements to enforce color-neutrality constraints. This work marks the first lattice gauge theory quantum simulation of QCD at finite density and temperature for two and three colors, laying the foundation to explore QCD phenomena on quantum platforms.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv Quantum Physics",
      "sourceUrl": "http://export.arxiv.org/rss/quant-ph",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Is there a conflict between causality and diamagnetism?",
      "link": "https://arxiv.org/abs/2501.13098",
      "description": "arXiv:2501.13098v3 Announce Type: replace \nAbstract: There is a long-standing apparent conflict between the existence of diamagnetism and causality as expressed through the Kramers-Kronig relations. In essence, using causality arguments, along with a small number of seemingly well-justified assumptions, one can show that diamagnetism is impossible. However, experiments show diamagnetic responses from magnetic media. We present a resolution to this issue, which also explains the absence of observed dia-electric responses in media. In the process, we expose some of the short-comings in earlier analyses that have kept the paradox alive.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv Quantum Physics",
      "sourceUrl": "http://export.arxiv.org/rss/quant-ph",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Online Learning of Pure States is as Hard as Mixed States",
      "link": "https://arxiv.org/abs/2502.00823",
      "description": "arXiv:2502.00823v3 Announce Type: replace \nAbstract: Quantum state tomography, the task of learning an unknown quantum state, is a fundamental problem in quantum information. In standard settings, the complexity of this problem depends significantly on the type of quantum state that one is trying to learn, with pure states being substantially easier to learn than general mixed states. A natural question is whether this separation holds for any quantum state learning setting. In this work, we consider the online learning framework and prove the surprising result that learning pure states in this setting is as hard as learning mixed states. More specifically, we show that both classes share almost the same sequential fat-shattering dimension, leading to identical regret scaling. We also generalize previous results on full quantum state tomography in the online setting to (i) the $\\epsilon$-realizable setting and (ii) learning the density matrix only partially, using smoothed analysis.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv Quantum Physics",
      "sourceUrl": "http://export.arxiv.org/rss/quant-ph",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Dicke subsystems are entangled",
      "link": "https://arxiv.org/abs/2502.18574",
      "description": "arXiv:2502.18574v3 Announce Type: replace \nAbstract: We show that all reduced states of nonproduct symmetric Dicke states of arbitrary number of qudits are genuinely multipartite entangled, and of nonpositive partial transpose with respect to any subsystem.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv Quantum Physics",
      "sourceUrl": "http://export.arxiv.org/rss/quant-ph",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Stabilizer Ranks, Barnes Wall Lattices and Magic Monotones",
      "link": "https://arxiv.org/abs/2503.04101",
      "description": "arXiv:2503.04101v2 Announce Type: replace \nAbstract: In 2024, Kliuchnikov and Sch\\\"onnenbeck showed a connection between the Barnes Wall lattices, stabilizer states and Clifford operations. In this work, we study their results and relate them to the problem of lower bounding stabilizer ranks. We show the first quantitative lower bound on stabilizer fidelity as a function of stabilizer ranks, which reproduces the linear-by-log lower bound for $\\chi_{\\delta}({|{H}\\rangle^{ \\otimes n}})$, i.e, on the approximate stabilizer rank of $|H\\rangle^{\\otimes n}$. In fact, we show that the lower bound holds even when the fidelity between the approximation and ${|H\\rangle}^{\\otimes n}$ is exponentially small, which is currently the best lower bound in this regime.\n  Next, we define a new magic monotone for pure states, the Barnes Wall norm, and its corresponding approximate variant. We upper bound these monotones by the $CS$-count of state preparation, and also by the stabilizer ranks. In particular, the upper bound given by the $CS$-count is tight, in the sense that we exhibit states that achieve the bound.\n  Apart from these results, we give a Fidelity Amplification algorithm, which provides a trade-off between approximation error and the stabilizer rank. As a corollary, it gives us a way to compose approximate stabilizer decompositions into approximate decompositions of their tensor products.\n  Finally, we provide an alternate, elementary proof of the existence and density of product states with maximal stabilizer ranks, which was first proven by Lovitz and Steffan (2022), where they used results from algebraic geometry.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv Quantum Physics",
      "sourceUrl": "http://export.arxiv.org/rss/quant-ph",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Quantization of nonlinear non-Hamiltonian systems",
      "link": "https://arxiv.org/abs/2503.06939",
      "description": "arXiv:2503.06939v3 Announce Type: replace \nAbstract: Several important dynamical systems are in $\\mathbb{R}^2$, defined by the pair of differential equations $(x',y')=(f(x,y),g(x,y))$. A question of fundamental importance is how such systems might behave quantum mechanically. In developing quantum theory, Dirac and others realized that classical Hamiltonian systems can be mapped to their quantum counterparts via canonical quantization. The resulting quantum dynamics is always physical, characterized by completely-positive and trace-preserving evolutions in the Schr\\\"{o}dinger picture. However, whether non-Hamiltonian systems can be quantized systematically while respecting the same physical requirements has remained a long-standing problem. Here we resolve this question when $f(x,y)$ and $g(x,y)$ are arbitrary polynomials. By leveraging open-systems theory, we prove constructively that every polynomial system admits a physical generator of time evolution in the form of a Lindbladian. We call our method cascade quantization, and demonstrate its power by analyzing several paradigmatic examples of nonlinear dynamics such as bifurcations, noise-activated spiking, and Li\\'{e}nard systems. In effect, our method can quantize any classical system whose $f(x,y)$ and $g(x,y)$ are analytic with arbitrary precision. More importantly, cascade quantization is exact. This means restrictive system properties usually assumed in the literature to facilitate quantization, such as weak nonlinearity, rotational symmetry, or semiclassical dynamics, can all be dispensed with by cascade quantization. We also highlight the advantages of cascade quantization over existing proposals, by weighing it against examples from the variational paradigm using Lagrangians, as well as non-variational approaches.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv Quantum Physics",
      "sourceUrl": "http://export.arxiv.org/rss/quant-ph",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "A quantum wire approach to weighted combinatorial graph optimisation problems",
      "link": "https://arxiv.org/abs/2503.17115",
      "description": "arXiv:2503.17115v2 Announce Type: replace \nAbstract: Neutral atom arrays provide a versatile platform to implement coherent quantum annealing as an approach to solving hard combinatorial optimization problems. Here we present and experimentally demonstrate an efficient encoding scheme based on chains of Rydberg-blockaded atoms, which we call quantum wires, to natively embed maximum weighted independent set (MWIS) and quadratic unconstrained binary optimization (QUBO) problems on a neutral atom architecture. For graphs with quasi-unit-disk connectivity, in which only a few long-range edges are required, our approach requires a significantly lower overhead in the number of ancilla qubits than previous proposals, facilitating the implementation on currently available hardware. To demonstrate the approach, we perform weighted-graph annealing on a programmable atom array using local light shifts to encode problem-specific weights across graphs of varying sizes. This approach successfully identifies the solutions to the original MWIS and QUBO graph instances. Our work expands the operational toolkit of near-term neutral atom arrays, enhancing their potential for scalable quantum optimization.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv Quantum Physics",
      "sourceUrl": "http://export.arxiv.org/rss/quant-ph",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Higher-Form Anomalies Imply Intrinsic Long-Range Entanglement",
      "link": "https://arxiv.org/abs/2504.10569",
      "description": "arXiv:2504.10569v2 Announce Type: replace \nAbstract: We show that generic gapped quantum many-body states which respect an anomalous finite higher-form symmetry have an exponentially small overlap with any short-range entangled (SRE) state. Hence, anomalies of higher-form symmetries enforce $intrinsic$ long-range entanglement, which is in contrast with anomalies of ordinary (0-form) symmetries which are compatible with symmetric SRE states (specifically, symmetric cat states). As an application, we show that the anomalies of strong higher-form symmetries provide a diagnostic for mixed-state topological order in $d \\geq 2$ spatial dimensions. We also identify a new (3+1)D intrinsic mixed-state topological order that does not obey remote-detectability by local decoherence of the (3+1)D Toric Code with fermionic loop excitations. This breakdown of remote detectability, as encoded in anomalies of strong higher-form symmetries, provides a partial characterization of intrinsically mixed-state topological order.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv Quantum Physics",
      "sourceUrl": "http://export.arxiv.org/rss/quant-ph",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Surmise for random matrices' level spacing distributions beyond nearest-neighbors",
      "link": "https://arxiv.org/abs/2504.20134",
      "description": "arXiv:2504.20134v2 Announce Type: replace \nAbstract: Correlations between energy levels can help distinguish whether a many-body system is of integrable or chaotic nature. The study of short-range and long-range spectral correlations generally involves quantities which are very different, unless one uses the $k$-th nearest neighbor ($k$NN) level spacing distributions. For nearest-neighbor (NN) spectral spacings, the distribution in random matrices is well captured by the Wigner surmise. This well-known approximation, derived exactly for a 2$\\times$2 matrix, is simple and satisfactorily describes the NN spacings of larger matrices. There have been attempts in the literature to generalize Wigner's surmise to further away neighbors. However, as we show, the current proposal in the literature fails to accurately capture numerical data. Using the known variance of the distributions from random matrix theory, we propose a corrected surmise for the $k$NN spectral distributions. This surmise better characterizes spectral correlations while retaining the simplicity of Wigner's surmise. We test the predictions against numerical results and show that the corrected surmise is systematically more accurate at capturing data from random matrices. Using the XXZ spin chain with random on-site disorder, we illustrate how these results can be used as a refined probe of many-body quantum chaos for both short- and long-range spectral correlations.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv Quantum Physics",
      "sourceUrl": "http://export.arxiv.org/rss/quant-ph",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Non-Markovianity in collision models with initial intra-environment correlations",
      "link": "https://arxiv.org/abs/2505.05433",
      "description": "arXiv:2505.05433v2 Announce Type: replace \nAbstract: Collision models (CMs) describe an open system interacting in sequence with elements of an environment, termed ancillas. They have been established as a useful tool for analyzing non-Markovian open quantum dynamics based on the ability to control the environmental memory through simple feedback mechanisms. In this work, we investigate how ancilla-ancilla entanglement can serve as a mechanism for controlling the non-Markovianity of an open system, focusing on an operational approach to generating correlations within the environment. To this end, we first demonstrate that the open dynamics of CMs with sequentially generated correlations between groups of ancillas can be mapped onto a composite CM, where the memory part of the environment is incorporated into an enlarged Markovian system. We then apply this framework to an all-qubit CM, and show that non-Markovian behavior emerges only when the next incoming pair of ancillas are entangled prior to colliding with the system. On the other hand, when system-ancilla collisions precede ancilla-ancilla entanglement, we find the open dynamics to always be Markovian. Our findings highlight how certain qualitative features of inter-ancilla correlations can strongly influence the onset of system non-Markovianity.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv Quantum Physics",
      "sourceUrl": "http://export.arxiv.org/rss/quant-ph",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "New aspects of quantum topological data analysis: Betti number estimation, and testing and tracking of homology and cohomology classes",
      "link": "https://arxiv.org/abs/2506.01432",
      "description": "arXiv:2506.01432v3 Announce Type: replace \nAbstract: We present new quantum algorithms for estimating homological invariants, specifically Betti and persistent Betti numbers, of a simplicial complex given through structured classical data. Our approach efficiently constructs block-encodings of (persistent) Laplacians, enabling estimation via stochastic rank methods with complexity polylogarithmic in the number of simplices across both sparse and dense regimes.\n  Unlike prior spectral algorithms that suffer when Betti numbers are small, we introduce homology tracking and property testing techniques achieving exponential speedups under natural sparsity and structure assumptions. We also formulate homology triviality and equivalence testing as property testing problems, giving nearly linear-time quantum algorithms when the boundary rank is large. A cohomological formulation further yields rank-independent testing and polylog-time manipulation of $r$-cocycles via block-encoded projections. These results open a new direction in quantum topological data analysis and demonstrate provable quantum advantages in computing topological invariants.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv Quantum Physics",
      "sourceUrl": "http://export.arxiv.org/rss/quant-ph",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Experimental memory control in continuous variable optical quantum reservoir computing",
      "link": "https://arxiv.org/abs/2506.07279",
      "description": "arXiv:2506.07279v3 Announce Type: replace \nAbstract: Quantum reservoir computing (QRC) offers a promising framework for online quantum-enhanced machine learning tailored to temporal tasks, yet practical implementations with native memory capabilities remain limited. Here, we demonstrate an optical QRC platform based on deterministically generated multimode squeezed states, exploiting both spectral and temporal multiplexing in a fully continuous-variable (CV) setting, and enabling controlled fading memory. Data is encoded via programmable phase shaping of the pump in an optical parametric process and retrieved through mode-selective homodyne detection. Real-time memory is achieved through feedback using electro-optic phase modulation, while long-term dependencies are achieved via spatial multiplexing. This architecture with minimal post-processing performs nonlinear temporal tasks, including parity checking and chaotic signal forecasting, with results corroborated by a high-fidelity Digital Twin. We show that leveraging the entangled multimode structure significantly enhances the expressivity and memory capacity of the quantum reservoir. This work establishes a scalable photonic platform for quantum machine learning, operating in CV encoding and supporting practical quantum-enhanced information processing.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv Quantum Physics",
      "sourceUrl": "http://export.arxiv.org/rss/quant-ph",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Learning-Optimized Qubit Mapping and Reuse to Minimize Inter-Core Communication in Modular Quantum Architectures",
      "link": "https://arxiv.org/abs/2506.09323",
      "description": "arXiv:2506.09323v3 Announce Type: replace \nAbstract: Modular quantum architectures have emerged as a promising approach for scaling quantum computing systems by connecting multiple Quantum Processing Units (QPUs). However, this approach introduces significant challenges due to costly inter-core operations between chips and quantum state transfers, which contribute to noise and quantum decoherence. This paper presents QARMA, a novel Qubit mapping using Attention-based deep Reinforcement learning (DRL) for Modular quantum Architectures, along with its extension QARMA-R that incorporates dynamic qubit reuse capabilities. Our approach combines an attention-based mechanism with Graph Neural Networks (GNN) to learn optimal qubit allocation, routing, and reuse strategies that minimize inter-core communications. We introduce two key innovations: (1) a transformer-based encoder that captures both the global circuit structure and local qubit interactions and (2) a dynamic qubit reuse compilation mechanism that leverages mid-circuit measurement and reset operations to reduce inter-operation and qubit requirements. Our experimental results show significant improvements over state-of-the-art approaches. Compared to highly-optimized Qiskit with modular architecture configuration, QARMA-R reduces inter-core communications by up to 100% (on average 86%), while QARMA maintains 15-40% improvement for larger circuits without reuse. Against traditional modular qubit mapping, our approach achieves 97-100% reduction in inter-core operation. The proposed methods advance quantum circuit compilation techniques and enable the execution of more extensive quantum algorithms on resource-constrained modular quantum systems, contributing to the growing body of research on scalable quantum computing architectures.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv Quantum Physics",
      "sourceUrl": "http://export.arxiv.org/rss/quant-ph",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "VQC-MLPNet: An Unconventional Hybrid Quantum-Classical Architecture for Scalable and Robust Quantum Machine Learning",
      "link": "https://arxiv.org/abs/2506.10275",
      "description": "arXiv:2506.10275v2 Announce Type: replace \nAbstract: Variational quantum circuits (VQCs) hold promise for quantum machine learning but face challenges in expressivity, trainability, and noise resilience. We propose VQC-MLPNet, a hybrid architecture where a VQC generates the first-layer weights of a classical multilayer perceptron during training, while inference is performed entirely classically. This design preserves scalability, reduces quantum resource demands, and enables practical deployment. We provide a theoretical analysis based on statistical learning and neural tangent kernel theory, establishing explicit risk bounds and demonstrating improved expressivity and trainability compared to purely quantum or existing hybrid approaches. These theoretical insights demonstrate exponential improvements in representation capacity relative to quantum circuit depth and the number of qubits, providing clear computational advantages over standalone quantum circuits and existing hybrid quantum architectures. Empirical results on diverse datasets, including quantum-dot classification and genomic sequence analysis, show that VQC-MLPNet achieves high accuracy and robustness under realistic noise models, outperforming classical and quantum baselines while using significantly fewer trainable parameters.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv Quantum Physics",
      "sourceUrl": "http://export.arxiv.org/rss/quant-ph",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Constant-Factor Improvements in Quantum Algorithms for Linear Differential Equations",
      "link": "https://arxiv.org/abs/2506.20760",
      "description": "arXiv:2506.20760v2 Announce Type: replace \nAbstract: Finding the solution to linear ordinary differential equations of the form $\\partial_t u(t) = -A(t)u(t)$ has been a promising theoretical avenue for \\textit{asymptotic} quantum speedups. However, despite the improvements to existing quantum differential equation solvers over the years, little is known about \\textit{constant factor} costs of such quantum algorithms. This makes it challenging to assess the prospects for using these algorithms in practice. In this work, we prove constant factor bounds for a promising new quantum differential equation solver, the linear combination of Hamiltonian simulation (LCHS) algorithm. Our bounds are formulated as the number of queries to a unitary $U_A$ that block encodes the generator $A$. In doing so, we make several algorithmic improvements such as tighter truncation and discretization bounds on the LCHS kernel integral, a more efficient quantum compilation scheme for the SELECT operator in LCHS, as well as use of a constant-factor bound for oblivious amplitude amplification, which may be of general interest. To the best of our knowledge, our new formulae improve over previous state of the art by at least two orders of magnitude, where the speedup can be far greater if state preparation has a significant cost. Accordingly, for any previous resource estimates of time-independent linear differential equations for the most general case whereby the dynamics are not \\textit{fast-forwardable}, these findings provide a 100-200x reduction in runtime costs. This analysis contributes towards establishing more promising applications for quantum computing.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv Quantum Physics",
      "sourceUrl": "http://export.arxiv.org/rss/quant-ph",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Perfect quantum state transfer through a chaotic spin chain via many-body scars",
      "link": "https://arxiv.org/abs/2506.22114",
      "description": "arXiv:2506.22114v2 Announce Type: replace \nAbstract: Quantum many-body scars (QMBS) offer a mechanism for weak ergodicity breaking, enabling non-thermal dynamics to persist in a chaotic many-body system. While most studies of QMBS focus on anomalous eigenstate properties or long-lived revivals of local observables, their potential for quantum information processing remains largely unexplored. In this work, we demonstrate that \\emph{perfect quantum state transfer} can be achieved in a strongly interacting, quantum chaotic spin chain by exploiting a sparse set of QMBS eigenstates embedded within an otherwise thermal spectrum. These results show that QMBS in chaotic many-body systems may be harnessed for information transport tasks typically associated with integrable models.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv Quantum Physics",
      "sourceUrl": "http://export.arxiv.org/rss/quant-ph",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Neural Importance Resampling: A Practical Sampling Strategy for Neural Quantum States",
      "link": "https://arxiv.org/abs/2507.20510",
      "description": "arXiv:2507.20510v2 Announce Type: replace \nAbstract: Neural quantum states (NQS) have emerged as powerful tools for simulating many-body quantum systems, but their practical use is often hindered by limitations of current sampling techniques. Markov chain Monte Carlo (MCMC) methods suffer from slow mixing and require manual tuning, while autoregressive NQS impose restrictive architectural constraints that complicate the enforcement of symmetries and the construction of determinant-based multi-state wave functions. In this work, we introduce Neural Importance Resampling (NIR), a new sampling algorithm that combines importance resampling with a separately trained autoregressive proposal network. This approach enables efficient and unbiased sampling without constraining the NQS architecture. We demonstrate that NIR supports stable and scalable training, including for multi-state NQS, and mitigates issues faced by MCMC and autoregressive approaches. Numerical experiments on the 2D transverse-field Ising and $J_1$-$J_2$ Heisenberg models show that NIR outperforms MCMC in challenging regimes and yields results competitive with state of the art methods. Our results establish NIR as a robust alternative for sampling in variational NQS algorithms.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv Quantum Physics",
      "sourceUrl": "http://export.arxiv.org/rss/quant-ph",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "TensorHyper-VQC: A Tensor-Train-Guided Hypernetwork for Robust and Scalable Variational Quantum Computing",
      "link": "https://arxiv.org/abs/2508.01116",
      "description": "arXiv:2508.01116v2 Announce Type: replace \nAbstract: Variational Quantum Computing (VQC) faces fundamental scalability barriers, primarily due to the presence of barren plateaus and its sensitivity to quantum noise. To address these challenges, we introduce TensorHyper-VQC, a novel tensor-train (TT)-guided hypernetwork framework that significantly improves the robustness and scalability of VQC. Our framework fully delegates the generation of quantum circuit parameters to a classical TT network, effectively decoupling optimization from quantum hardware. This innovative parameterization mitigates gradient vanishing, enhances noise resilience through structured low-rank representations, and facilitates efficient gradient propagation. Grounded in Neural Tangent Kernel and statistical learning theory, our rigorous theoretical analyses establish strong guarantees on approximation capability, optimization stability, and generalization performance. Extensive empirical results across quantum dot classification, Max-Cut optimization, and molecular quantum simulation tasks demonstrate that TensorHyper-VQC consistently achieves superior performance and robust noise tolerance, including hardware-level validation on a 156-qubit IBM Heron processor. These results position TensorHyper-VQC as a scalable and noise-resilient framework for advancing practical quantum machine learning on near-term devices.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv Quantum Physics",
      "sourceUrl": "http://export.arxiv.org/rss/quant-ph",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Excitonic Coupling and Photon Antibunching in Venus Yellow Fluorescent Protein Dimers: A Lindblad Master Equation Approach",
      "link": "https://arxiv.org/abs/2508.14233",
      "description": "arXiv:2508.14233v5 Announce Type: replace \nAbstract: Strong excitonic coupling and photon antibunching (AB) have been observed together in Venus yellow fluorescent protein dimers and currently lack a cohesive theoretical explanation. In 2019, Kim et al. demonstrated Davydov splitting in circular dichroism spectra, revealing strong J-like coupling, while antibunched fluorescence emission was confirmed by combined antibunching--fluorescence correlation spectroscopy (AB/FCS fingerprinting). To investigate the implications of this coexistence, Venus yellow fluorescent protein (YFP) dimer population dynamics are modeled within a Lindblad master equation framework, testing its ability to cope with typical, data-informed, Venus YFP dimer time and energy values. Simulations predict multiple-femtosecond (fs) decoherence, yielding bright/dark state mixtures consistent with antibunched fluorescence emission at room temperature. Thus, excitonic coupling and photon AB in Venus YFP dimers are reconciled without invoking long-lived quantum coherence. However, clear violations of several Lindblad approximation validity conditions appear imminent, calling for careful modifications to choices of standard system and bath definitions and parameter values.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv Quantum Physics",
      "sourceUrl": "http://export.arxiv.org/rss/quant-ph",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Towards quantum topological data analysis: torsion detection",
      "link": "https://arxiv.org/abs/2508.19943",
      "description": "arXiv:2508.19943v2 Announce Type: replace \nAbstract: Topological data analysis (TDA) has become an attractive area for the application of quantum computing. Recent advances have uncovered many interesting connections between the two fields. On one hand, complexity theoretic results show that estimating Betti numbers, a central task in TDA, is NP hard, indicating that a generic quantum speedup is unlikely. On the other hand, several recent studies have explored structured, less generic settings and demonstrated that quantum algorithms can still achieve significant speedups under certain conditions. To date, most of these efforts have focused on Betti numbers, which are topological invariants capturing the intrinsic connectivity and holes in a dataset. However, there is another important feature of topological spaces: torsion. Torsion represents a distinct component of homology that can reveal richer structural information. In this work, we introduce a quantum algorithm for torsion detection, that is, determining whether a given simplicial complex contains torsion. Our algorithm, assisted by a low complexity classical procedure, can succeed with high probability and potentially offer exponential speedup over the classical counterpart.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv Quantum Physics",
      "sourceUrl": "http://export.arxiv.org/rss/quant-ph",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Noise-Resilient Quantum Metrology with Quantum Computing",
      "link": "https://arxiv.org/abs/2509.00771",
      "description": "arXiv:2509.00771v2 Announce Type: replace \nAbstract: Quantum computing has made remarkable strides in recent years, as demonstrated by quantum supremacy experiments and the realization of high-fidelity, fault-tolerant gates. However, a major obstacle persists: practical real-world applications remain scarce, largely due to the inefficiency of loading classical data into quantum processors. Here, we propose an alternative strategy that shifts the focus from classical data encoding to directly processing quantum data. We target quantum metrology, a practical quantum technology whose precision is often constrained by realistic noise. We develop an experimentally feasible scheme in which a quantum computer optimizes information acquired from quantum metrology, thereby enhancing performance in noisy quantum metrology tasks and overcoming the classical-data-loading bottleneck. We demonstrate this approach through experimental implementation with nitrogen-vacancy centers in diamond and numerical simulations using models of distributed superconducting quantum processors. Our results show that this method improves the accuracy of sensing estimates and significantly boosts sensitivity, as quantified by the quantum Fisher information, thus offering a new pathway to harness near-term quantum computers for realistic quantum metrology.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv Quantum Physics",
      "sourceUrl": "http://export.arxiv.org/rss/quant-ph",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "A Deficiency-Based Framework for the Operational Interpretation of Quantum Resources with Applications",
      "link": "https://arxiv.org/abs/2509.03043",
      "description": "arXiv:2509.03043v3 Announce Type: replace \nAbstract: A fundamental challenge in quantum resource theory lies in establishing operational interpretations by quantifying the distinct advantages that quantum resources provide over classical resources in specific physical tasks. However, conventional quantum resource theories have inherent limitations in characterizing operational advantages for certain quantum tasks. To overcome these limitations, we propose a novel framework that defines the resource deficiency of a given state relative to the set of maximal resource states in physical tasks. This extension not only broadens the scope of quantum resource theories and provides more comprehensive operational interpretations, but also delivers crucial insights for classifying and interpreting mixed resource states -- specifically those with inactive resource properties in certain tasks -- that have remained uncharacterized in conventional quantum resource theories. Moreover, we further demonstrate that the proposed geometric measure satisfies the framework's requirements for both quantum coherence and entanglement, while also demonstrating its ability to characterize the operational disadvantage of arbitrary states compared to maximal resource states in subchannel discrimination tasks under specific conditions.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv Quantum Physics",
      "sourceUrl": "http://export.arxiv.org/rss/quant-ph",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Coherence-Driven Quantum Battery Charging via Autonomous Thermal Machines: Energy Transfer, Memory Effects, and Ergotropy Enhancement",
      "link": "https://arxiv.org/abs/2509.03766",
      "description": "arXiv:2509.03766v2 Announce Type: replace \nAbstract: In this work, we study a hybrid quantum system composed of a quantum battery and a coherence-driven charger interacting with a Quantum Autonomous Thermal Machine (QATM). The QATM, made of two qubits, each coupled to Markovian bosonic thermal reservoirs at different temperatures, acts as a structured environment that mediates energy and coherence transfer between the charger and the battery. By applying a coherent driving field on the charger, we investigate the coherence injection effect on the dynamics, including non-Markovianity, power of charging, coherence storage, and ergotropy. We show that the QATM effectively filters the decoherence induced by the thermal baths and induces non-Markovian memory effects due to correlation backflow. Our results demonstrate that coherence driving enhances the battery's ergotropy; coherence driving raises the maximum ergotropy by approximately 40% compared to the case without coherence driving, and the power of charging by preserving the internal energy of the charger and facilitating coherent energy transfer.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv Quantum Physics",
      "sourceUrl": "http://export.arxiv.org/rss/quant-ph",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Reaffirming a Challenge to Bohmian Mechanics",
      "link": "https://arxiv.org/abs/2509.06584",
      "description": "arXiv:2509.06584v3 Announce Type: replace \nAbstract: In our recent work (Sharoglazova et al., Nature 643, 67 (2025)), we reported the measurement of the speed of tunnelling particles using a coupled waveguide system. The measured speed was found to disagree with the standard guiding equation of Bohmian mechanics, which we regard as a challenge to that framework. In the present work, we provide a more detailed account of this issue. In particular, we argue that agreement or disagreement between standard quantum mechanics and Bohmian mechanics on quantities such as particle velocity, speed, tunnelling, and dwell times depends solely on the choice of guiding equation. If this choice is made based on observable spatio-temporal transformation behaviour of the particle density, the two theories agree on these phenomena.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv Quantum Physics",
      "sourceUrl": "http://export.arxiv.org/rss/quant-ph",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Persistent-DPO: A novel loss function and hybrid learning for generative quantum eigensolver",
      "link": "https://arxiv.org/abs/2509.08351",
      "description": "arXiv:2509.08351v2 Announce Type: replace \nAbstract: We study the generative quantum eigensolver (GQE)~\\cite{nakaji2024generative}, which trains a classical generative model to produce quantum circuits with desired properties such as describing molecular ground states. We introduce two methods to improve GQE. First, we identify a limitation of direct preference optimization (DPO) when used as the loss function in GQE, and propose Persistent-DPO (P-DPO) as a solution to this limitation. Second, as a method to improve the online learning during the training phase of GQE, we introduce a hybrid approach that combines online and offline learning. Using a transformer decoder implementation of GQE, we evaluate our methods through ground state search experiments on the $\\mathrm{BeH_2^{}}$ molecule and observe that P-DPO achieves lower energies than DPO. The hybrid approach further improves convergence and final energy values, particularly with P-DPO. A method for imposing upper constraints on the occurrences of specific gates is also presented, which serves to enhance the applicability of GQE.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv Quantum Physics",
      "sourceUrl": "http://export.arxiv.org/rss/quant-ph",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Spatial structure of multipartite entanglement at measurement induced phase transitions",
      "link": "https://arxiv.org/abs/2509.12109",
      "description": "arXiv:2509.12109v2 Announce Type: replace \nAbstract: We study multiparty entanglement near measurement induced phase transitions (MIPTs), which arise in ensembles of local quantum circuits built with unitaries and measurements. In contrast to equilibrium quantum critical transitions, where entanglement is short-ranged, MIPTs possess long-range k-party genuine multiparty entanglement (GME) characterized by an infinite hierarchy of entanglement exponents for k>=2. First, we represent the average spread of entanglement with \"entanglement clusters,\" and use them to conjecture general exponent relations: 1) classical dominance, 2) monotonicity, 3) subadditivity. We then introduce measure-weighted graphs to construct such clusters in general circuits. Second, we obtain the exact entanglement exponents for a 1d MIPT in a measurement-only circuit that maps to percolation by exploiting non-unitary conformal field theory. The exponents, which we numerically verify, obey the inequalities. We also extend the construction to a 2d MIPT that maps to classical 3d percolation, and numerically find the first entanglement exponents. Our results provide a firm ground to understand the multiparty entanglement of MIPTs, and more general ensembles of quantum circuits.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv Quantum Physics",
      "sourceUrl": "http://export.arxiv.org/rss/quant-ph",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "The quantum smooth label cover problem is undecidable",
      "link": "https://arxiv.org/abs/2510.03477",
      "description": "arXiv:2510.03477v3 Announce Type: replace \nAbstract: We show that the quantum smooth label cover problem is undecidable and RE-hard. This sharply contrasts the quantum unique label cover problem, which can be decided efficiently by a result of Kempe, Regev, and Toner (FOCS'08). On the other hand, our result aligns with the RE-hardness of the quantum label cover problem, which follows from the celebrated MIP* = RE result of Ji, Natarajan, Vidick, Wright, and Yuen (ACM'21). Additionally, we show that the quantum oracularized smooth label cover problem is RE-hard. Our second result fits with the alternative quantum unique games conjecture recently proposed by Mousavi and Spirig (ITCS'25) on the RE-hardness of the quantum oracularized unique label cover problem. Our proof techniques include a quantum version of Feige's reduction from 3SAT to 3SAT5 (STOC'96) for BCSMIP*-protocols, which may be of independent interest.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv Quantum Physics",
      "sourceUrl": "http://export.arxiv.org/rss/quant-ph",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Low Overhead Universal Quantum Computation with Triorthogonal Codes",
      "link": "https://arxiv.org/abs/2510.05708",
      "description": "arXiv:2510.05708v2 Announce Type: replace \nAbstract: We study the use of triorthogonal codes for universal fault-tolerant quantum computation and propose two methods to circumvent the Eastin-Knill theorem, which prohibits any single quantum error-correcting code from supporting both universality and a transversal gate set. We show that our methods reduce the resource overhead compared with existing fault-tolerant protocols. We develop a simple fault-tolerant implementation of the logical Hadamard gate for triorthogonal codes by exploiting the fact that they have transversal controlled-Z (CZ) gates, resulting in a circuit with reduced overhead. We also introduce a procedure for generating a symmetric Calderbank-Shor-Steane code paired with a triorthogonal code, which allows CNOT and CZ gate transversality across the pair of codes. In addition, we present logical state teleportation circuits that transfer encoded states between the two codes, allowing all logical operations to be performed transversally. Our methods can be integrated into the Steane error correction framework without incurring additional resource cost. Finally, using the 15-qubit code as an example, we demonstrate that our protocols significantly reduce the gate overhead compared with other existing methods. These results highlight the potential of combining distinct code structures to achieve low-overhead, universal fault-tolerant quantum computation.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv Quantum Physics",
      "sourceUrl": "http://export.arxiv.org/rss/quant-ph",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "HPQEA: A Scalable and High-Performance Quantum Emulator with High-Bandwidth Memory for Diverse Algorithms Support",
      "link": "https://arxiv.org/abs/2510.07110",
      "description": "arXiv:2510.07110v2 Announce Type: replace \nAbstract: In recent years, there has been a growing interest in the development of quantum emulation. However, existing studies often struggle to achieve broad applicability, high performance, and efficient resource and memory utilization. To address these challenges, we provide HPQEA, a quantum emulator based on the state-vector emulation approach. HPQEA includes three main features: a high-performance computing core, an optimized controlled-NOT gate computation strategy, and effective utilization of high-bandwidth memory. Verification and evaluation on the Alveo U280 board show that HPQEA can emulate quantum circuits with up to 30 qubits while maintaining high fidelity and low mean square error. It outperforms comparable FPGA-based systems by producing faster execution, supporting a wider range of algorithms, and requiring low hardware resources. Furthermore, it exceeds the Nvidia A100 in normalized gate speed for systems with up to 20 qubits. These results demonstrate the scalability and efficiency of HPQEA as a platform for emulating quantum algorithms.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv Quantum Physics",
      "sourceUrl": "http://export.arxiv.org/rss/quant-ph",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Proposals for experimentally realizing (mostly) quantum-autonomous gates",
      "link": "https://arxiv.org/abs/2510.07372",
      "description": "arXiv:2510.07372v2 Announce Type: replace \nAbstract: Autonomous quantum machines (AQMs) execute tasks without requiring time-dependent external control. Motivations for AQMs include the restrictions imposed by classical control on quantum machines' coherence times and geometries. Most AQM work is theoretical and abstract; yet an experiment recently demonstrated AQMs' usefulness in qubit reset, crucial to quantum computing. To further reduce quantum computing's classical control, we propose realizations of (fully and partially) quantum-autonomous gates on three platforms: Rydberg atoms, trapped ions, and superconducting qubits. First, we show that a Rydberg-blockade interaction or an ultrafast transition can quantum-autonomously effect entangling gates on Rydberg atoms. One can perform $Z$ or entangling gates on trapped ions mostly quantum-autonomously, by sculpting a linear Paul trap or leveraging a ring trap. Passive lasers control these gates, as well as the Rydberg-atom gates, quantum-autonomously. Finally, circuit quantum electrodynamics can enable quantum-autonomous $Z$ and $XY$ gates on superconducting qubits. The gates can serve as building blocks for (fully or partially) quantum-autonomous circuits, which may reduce classical-control burdens.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv Quantum Physics",
      "sourceUrl": "http://export.arxiv.org/rss/quant-ph",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Angular Geometry of Atomic Multipole Transitions",
      "link": "https://arxiv.org/abs/2510.07451",
      "description": "arXiv:2510.07451v3 Announce Type: replace \nAbstract: A simple way to calculate Rabi frequencies is outlined for interactions of atomic or nuclear multipole moments with laser fields that focuses on their relative geometry. The resulting expression takes the form of a dot product between the laser polarization and a vector spherical harmonic, thereby naturally connecting to the multipole's far-field spontaneous-emission pattern and providing a way to visualize the interaction. Since the vector spherical harmonics are not yet a standard tool in quantum science, their relevant properties are reviewed. This approach is illustrated in the calculation of a variety of beam effects, yielding both perturbative corrections and some nontrivial cases with non-vanishing coupling.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv Quantum Physics",
      "sourceUrl": "http://export.arxiv.org/rss/quant-ph",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Experimental verification of multi-copy activation of genuine multipartite entanglement",
      "link": "https://arxiv.org/abs/2510.12457",
      "description": "arXiv:2510.12457v3 Announce Type: replace \nAbstract: A central concept in quantum information processing is genuine multipartite entanglement (GME), a type of correlation beyond biseparability, that is, correlations that cannot be explained by statistical mixtures of partially separable states. GME is relevant for characterizing and benchmarking complex quantum systems, and it is an important resource for applications such as quantum communication. Remarkably, it has been found that GME can be activated from multiple copies of biseparable quantum states, which do not possess GME individually. Here, we experimentally demonstrate unambiguous evidence of such GME activation from two copies of a biseparable three-qubit state in a trapped-ion quantum processor. These results not only challenge notions of quantum resources but also highlight the potential of using multiple copies of quantum states to achieve tasks beyond the capabilities of the individual copies.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv Quantum Physics",
      "sourceUrl": "http://export.arxiv.org/rss/quant-ph",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Grid-Partitioned MWIS Solving with Neutral Atom Quantum Computing for QUBO Problems",
      "link": "https://arxiv.org/abs/2510.18540",
      "description": "arXiv:2510.18540v2 Announce Type: replace \nAbstract: Quadratic Unconstrained Binary Optimization (QUBO) problems are prevalent in real-world applications, such as portfolio optimization, but pose significant computational challenges for large-scale instances. We propose a hybrid quantum-classical framework that leverages neutral atom quantum computing to address QUBO problems by mapping them to the Maximum Weighted Independent Set (MWIS) problem on unit disk graphs. Our approach employs spatial grid partitioning to decompose the problem into manageable subgraphs, solves each subgraph using Analog Hamiltonian Simulation (AHS), and merges solutions greedily to approximate the global optimum. We evaluate the framework on a 50-asset portfolio optimization problem using historical S&P 500 data, benchmarking against classical simulated annealing. Results demonstrate competitive performance, highlighting the scalability and practical potential of our method in the Noisy Intermediate-Scale Quantum (NISQ) era. As neutral atom quantum hardware advances, our framework offers a promising path toward solving large-scale optimization problems efficiently.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv Quantum Physics",
      "sourceUrl": "http://export.arxiv.org/rss/quant-ph",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Transition-Aware Decomposition of Single-Qudit Gates",
      "link": "https://arxiv.org/abs/2510.25561",
      "description": "arXiv:2510.25561v2 Announce Type: replace \nAbstract: Quantum computation with $d$-level quantum systems, also known as qudits, benefits from the possibility to use a richer computational space compared to qubits. However, for arbitrary qudit-based hardware platform the issue is that a generic qudit operation has to be decomposed into the sequence of native operations $-$ pulses that are adjusted to the transitions between two levels in a qudit. Typically, not all levels in a qudit are simply connected to each other due to specific selection rules. Moreover, the number of pulses plays a significant role, since each pulse takes a certain execution time and may introduce error. In this paper, we propose a resource-efficient algorithm to decompose single-qudit operations into the sequence of pulses that are allowed by qudit selection rules. Using the developed algorithm, the number of pulses is at most $d(d{-}1)/2$ for an arbitrary single-qudit operation. For specific operations the algorithm could produce even fewer pulses. We provide a comparison of qudit decompositions for several types of trapped ions, specifically $^{171}\\text{Yb}^+$, $^{137}\\text{Ba}^+$ and $^{40}\\text{Ca}^+$ with different selection rules, and also decomposition for superconducting qudits.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv Quantum Physics",
      "sourceUrl": "http://export.arxiv.org/rss/quant-ph",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Entanglement-assisted circuit knitting",
      "link": "https://arxiv.org/abs/2510.26789",
      "description": "arXiv:2510.26789v2 Announce Type: replace \nAbstract: Distributed quantum computing (DQC) provides a promising route toward scalable quantum computation, where entanglement-assisted LOCC and circuit knitting represent two complementary approaches. The former deterministically realizes nonlocal operations but demands extensive entanglement resources, whereas the latter requires no entanglement yet suffers from exponential sampling overhead. Here, we propose a hybrid framework that integrates these two paradigms by performing circuit knitting assisted with a limited amount of entanglement. We establish a general theoretical formulation that yields lower bounds on the optimal sampling overhead and present a constructive protocol demonstrating that a single shared Bell pair can reduce the overhead to the asymptotic limit of standard circuit knitting without requiring classical communication. Furthermore, we extend the entanglement-assisted circuit knitting framework to the black-box setting, which can be applicable to the circuit knitting of quantum combs. This hybrid approach can be viewed as a form of hybrid classical-quantum computation, balancing the trade-off between sampling and entanglement efficiency, and enabling more resource-practical implementations of distributed quantum computing.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv Quantum Physics",
      "sourceUrl": "http://export.arxiv.org/rss/quant-ph",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "No, classical gravity does not entangle quantized matter fields",
      "link": "https://arxiv.org/abs/2511.00852",
      "description": "arXiv:2511.00852v2 Announce Type: replace \nAbstract: In their recent work [Nature,646,813(2025)], Aziz and Howl claim that classical (unquantized) gravity produces entanglement of quantized matter if matter is treated within quantum field theory which is, no doubt, our ultimate theory to use. However, an elementary quantum field re-calculation of the authors' example shows that there is no entangling effect.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv Quantum Physics",
      "sourceUrl": "http://export.arxiv.org/rss/quant-ph",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "EPR Revisited: Context-Indexed Elements of Reality and Operational Completeness",
      "link": "https://arxiv.org/abs/2511.01930",
      "description": "arXiv:2511.01930v2 Announce Type: replace \nAbstract: We reframe the EPR argument through an operational lens, replacing the notion of fixed \"elements of reality\" with context-indexed conditional states - what's often referred to as a measurement assemblage. This move deliberately sidesteps the assumption of context-independent values for incompatible observables. Our updated version of the Reality Criterion works like this: if Alice measures observable x and obtains outcome a, then Bob's system must adopt a conditional state that ensures the corresponding outcome for that specific context. Crucially, we also assume operational completeness - a condition that quantum mechanics satisfies when we're dealing with quantum-reachable assemblages. Now, in any theory where one party cannot signal to the other (so-called one-sided no-signaling theories), perfect predictions do support drawing context-indexed inferences. But - and this is key - they don't legitimize assigning fixed values across all contexts. We rigorously demonstrate this distinction. To ground the argument, we offer examples: the qubit singlet scenario using Pauli settings and CJWR thresholds, a continuous-variable case based on the Reid criteria, and a counterexample in the spirit of the PR box, which highlights the boundaries of what quantum theory can actually reach.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv Quantum Physics",
      "sourceUrl": "http://export.arxiv.org/rss/quant-ph",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Circular Rosenzweig-Porter random matrix ensemble",
      "link": "https://arxiv.org/abs/2111.08031",
      "description": "arXiv:2111.08031v3 Announce Type: replace-cross \nAbstract: The Rosenzweig-Porter random matrix ensemble serves as a qualitative phenomenological model for the level statistics and fractality of eigenstates across the many-body localization transition in static systems. We propose a unitary (circular) analogue of this ensemble, which similarly captures the phenomenology of many-body localization in periodically driven (Floquet) systems. We define this ensemble as the outcome of a Dyson Brownian motion process. We show numerical evidence that this ensemble shares some key statistical properties with the Rosenzweig-Porter ensemble for both the eigenvalues and the eigenstates.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv Quantum Physics",
      "sourceUrl": "http://export.arxiv.org/rss/quant-ph",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Operator growth and black hole formation",
      "link": "https://arxiv.org/abs/2304.14351",
      "description": "arXiv:2304.14351v3 Announce Type: replace-cross \nAbstract: When two particles collide in an asymptotically AdS spacetime with high enough energy and small enough impact parameter, they can form a black hole. Motivated by dual quantum circuit considerations, we propose a threshold condition for black hole formation. Intuitively the condition can be understood as the onset of overlap of the butterfly cones describing the ballistic spread of the effect of the perturbations on the boundary systems. We verify the correctness of the condition in three bulk dimensions. We describe a six-point correlation function that can diagnose this condition and compute it in two-dimensional CFTs using eikonal resummation.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv Quantum Physics",
      "sourceUrl": "http://export.arxiv.org/rss/quant-ph",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Mean-field theory of 1+1D $\\mathbb{Z}_2$ lattice gauge theory with matter",
      "link": "https://arxiv.org/abs/2404.02890",
      "description": "arXiv:2404.02890v2 Announce Type: replace-cross \nAbstract: Lattice gauge theories (LGTs) provide valuable insights into problems in strongly correlated many-body systems. Confinement which arises when matter is coupled to gauge fields is just one of the open problems, where LGT formalism can explain the underlying mechanism. However, coupling gauge fields to dynamical charges complicates the theoretical and experimental treatment of the problem. Developing a simplified mean-field theory is thus one of the ways to gain new insights into these complicated systems. Here we develop a mean-field theory of a paradigmatic 1+1D $\\mathbb{Z}_2$ lattice gauge theory with superconducting pairing term, the gauged Kitaev chain, by decoupling charge and $\\mathbb{Z}_2$ fields while enforcing the Gauss law on the mean-field level. We first determine the phase diagram of the original model in the context of confinement, which allows us to identify the symmetry-protected topological transition in the Kitaev chain as a confinement transition. We then compute the phase diagram of the effective mean-field theory, which correctly captures the main features of the original LGT. This is furthermore confirmed by the Green's function results and a direct comparison of the ground state energy. This simple LGT can be implemented in state-of-the art cold atom experiments. We thus also consider string-length histograms and the electric field polarization, which are easily accessible quantities in experimental setups and show that they reliably capture the various phases.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv Quantum Physics",
      "sourceUrl": "http://export.arxiv.org/rss/quant-ph",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Quantised helicity in optical media",
      "link": "https://arxiv.org/abs/2405.08086",
      "description": "arXiv:2405.08086v2 Announce Type: replace-cross \nAbstract: We present a new approach to the definition of optical helicity in a medium. Our approach resolves the problem that duality transformations which simultaneously combine $\\mathbf{E}$ with $\\mathbf{H}$ and $\\mathbf{D}$ with $\\mathbf{B}$ are incompatible with linear constitutive relations. We find that the helicity density in a medium, as the conserved quantity associated with duality transforms, must contain an explicit contribution associated with the polarisation and magnetisation of the matter, and that it can be expressed naturally in terms of the elementary polarised excitations of the system. In media for which the helicity is conserved, each circular excitation carries a well-defined helicity. However, in a medium for which the helicity is not conserved, we find that the time-varying helicity can be viewed in terms of oscillations between different helicity eigenstates, analogous to neutrino oscillations. Here we explicitly study the helicity in homogeneous and lossless media but we believe that, differently to other choices, this helicity is readily generalisable to media that may be inhomogeneous, lossy, chiral or nonreciprocal.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv Quantum Physics",
      "sourceUrl": "http://export.arxiv.org/rss/quant-ph",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Direct and mediated dipole-dipole interactions in a reconfigurable array of optical traps",
      "link": "https://arxiv.org/abs/2408.06256",
      "description": "arXiv:2408.06256v3 Announce Type: replace-cross \nAbstract: Optically levitated nanoparticles in vacuum experience both electrostatic and light-induced dipole-dipole interactions, offering a versatile platform to explore mesoscopic entanglement and many-body dynamics. A significant challenge in optical trap arrays is to achieve site-resolved, point-to-point tunability: adjusting the laser parameters of a single trap typically induces global cross-talk to neighboring sites, hindering independent control. Inspired by tunable couplers in superconducting circuits, we implement an ancillary nanoparticle that functions as a coupler between two target nanoparticles. Within a reconfigurable three-particle array, we demonstrate broad tunability of the direct dipole-dipole interaction by controlling the phase and position of the traps. In addition, we observe spectral signatures consistent with mediated interactions between the target particles via the ancillary one, manifested as mode participation beyond the uncoupled response. Our results establish a practical route to tailored, site-resolved control in multi-particle optical trap arrays, expanding the optical-binding toolbox and opening opportunities for programmable oscillator networks relevant to macroscopic quantum mechanics and precision sensing.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv Quantum Physics",
      "sourceUrl": "http://export.arxiv.org/rss/quant-ph",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Beyond-mean-field phases of rotating dipolar condensates",
      "link": "https://arxiv.org/abs/2503.04890",
      "description": "arXiv:2503.04890v2 Announce Type: replace-cross \nAbstract: Rotating dipolar Bose-Einstein condensates exhibit rich physics due to the interplay of long-range interactions and rotation, leading to unconventional vortex structures and strongly correlated phases. While most studies rely on mean-field approaches, these fail to capture quantum correlations that become significant at high rotation speeds and strong interactions. In this study, we go beyond the mean-field description by employing a numerically exact multiconfigurational approach to study finite-sized dipolar condensates. We reveal novel vortex structures, rotating cluster states, and strong fragmentation effects, demonstrating that beyond-mean-field correlations remain prominent even in larger systems. By quantifying deviations from mean-field theory, we provide a predictive framework for analyzing experiments and exploring emergent quantum phases, with implications for both the fundamental theory of ultracold gases and the quantum simulation of correlated superfluid systems like in neutron stars.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv Quantum Physics",
      "sourceUrl": "http://export.arxiv.org/rss/quant-ph",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Torsion-Driven Nonlinearity in Spinless Quantum Mechanics",
      "link": "https://arxiv.org/abs/2504.09698",
      "description": "arXiv:2504.09698v3 Announce Type: replace-cross \nAbstract: We investigate the previously unexplored quantum dynamics of non-relativistic, spinless particles propagating in curved spaces with torsion. Our findings demonstrate that while torsion has been predominantly associated with spin, it can also influence the quantum behavior of spinless particles by inducing a logarithmic nonlinearity in the Schroedinger equation through quantum fluctuations, even in flat space. To facilitate quantization in curved spaces, we introduce a novel stochastic variational method. Unlike canonical quantization, this approach is naturally suited to general coordinate systems, with quantum fluctuations arising from a noise term in the stochastic process that is directly influenced by torsion. By requiring consistency with quantum dynamics, we ultimately derive an upper bound on the magnitude of torsion. Our results reveal a previously unrecognized mechanism by which torsion, as predicted in certain extensions of general relativity, can influence quantum systems, with potential implications for early-universe physics and dark matter or energy models.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv Quantum Physics",
      "sourceUrl": "http://export.arxiv.org/rss/quant-ph",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Gold-Standard Chemical Database 137 (GSCDB137): A diverse set of accurate energy differences for assessing and developing density functionals",
      "link": "https://arxiv.org/abs/2508.13468",
      "description": "arXiv:2508.13468v2 Announce Type: replace-cross \nAbstract: We present GSCDB137, a rigorously curated benchmark library of 137 data sets (8377 entries) covering main-group and transition-metal reaction energies and barrier heights, (intramolecular) non-covalent interactions, dipole moments, polarizabilities, electric-field response energies, and vibrational frequencies. Legacy data from GMTKN55 and MGCDB84 have been updated to today's best reference values; redundant, spin-contaminated, or low-quality points were removed, and many new, property-focused sets were added. Testing 29 popular density functional approximations (DFAs) confirms the expected Jacob's-ladder hierarchy overall but also reveals notable exceptions: functional performance for frequencies and electric-field properties correlates poorly with that for other ground-state energetics. {\\omega}B97M-V and {\\omega}B97X-V are the most balanced hybrid meta-GGA and hybrid GGA, respectively; B97M-V and revPBE-D4 lead the meta-GGA and GGA classes. Double hybrids lower mean errors by about 25 % versus the best hybrids but demand careful frozen-core, basis-set, and multi-reference treatment. GSCDB137 offers a comprehensive, openly documented platform for stringent DFA validation and for training the next generation of non-empirical and machine-learned functionals.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv Quantum Physics",
      "sourceUrl": "http://export.arxiv.org/rss/quant-ph",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "A new characterization of the holographic entropy cone",
      "link": "https://arxiv.org/abs/2508.21823",
      "description": "arXiv:2508.21823v3 Announce Type: replace-cross \nAbstract: Entanglement entropies computed using the holographic Ryu-Takayanagi formula are known to obey an infinite set of linear inequalities, which define the so-called RT entropy cone. The general structure of this cone, or equivalently the set of all valid inequalities, is unknown. It is also unknown whether those same inequalities are also obeyed by entropies computed using the covariant Hubeny-Rangamani-Takayanagi formula, although significant evidence has accumulated that they are. Using Markov states, we develop a test of this conjecture in a heretofore unexplored regime. The test reduces to checking that a given inequality obeys a certain majorization property, which is easy to evaluate. We find that the RT inequalities pass this test and, surprisingly, only RT inequalities do so. Our results not only provide strong new evidence that the HRT and RT cones coincide, but also offer a completely new characterization of that cone.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv Quantum Physics",
      "sourceUrl": "http://export.arxiv.org/rss/quant-ph",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Exploring dark matter with quantum-enhanced haloscopes and time projection chambers",
      "link": "https://arxiv.org/abs/2509.14897",
      "description": "arXiv:2509.14897v2 Announce Type: replace-cross \nAbstract: This thesis explores experimental and theoretical approaches to dark matter detection, from gas-based detectors to quantum sensors, tackling the challenge of identifying dark matter, which makes up 27% of the Universe's energy. It reviews astrophysical and cosmological evidence, highlights the Standard Model's limitations, and motivates searches for WIMPs, axions, and dark photons through direct, indirect, and collider-based strategies.\n  The experimental work includes the Micromegas-based TREX-DM experiment for low-mass WIMPs, with studies of argon and neon-based gas mixtures, detector design, shielding, readout, and background suppression. GEM integration boosted gain by up to 45. A UV LED-based internal calibration system was developed for compact, low-background operation, while pressure-dependent gain studies optimized future low-background TPCs. The thesis also advances axion and dark photon searches via haloscopes and introduces the DarkQuantum prototype, a superconducting qubit coupled to microwave cavities for single-photon detection. This system enabled the most stringent exclusion limit on massive dark photon interactions at 5.051 GHz, demonstrating the feasibility of quantum-enhanced detectors.\n  Overall, the work bridges classical and quantum detection techniques, advancing WIMP searches and pioneering compact quantum sensors for axion and dark photon detection, laying the foundation for future high-sensitivity dark matter experiments.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv Quantum Physics",
      "sourceUrl": "http://export.arxiv.org/rss/quant-ph",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Universal decay of (conditional) mutual information in gapped pure- and mixed-state quantum matter",
      "link": "https://arxiv.org/abs/2510.22867",
      "description": "arXiv:2510.22867v2 Announce Type: replace-cross \nAbstract: For spin and fermionic systems in any spatial dimension, we establish that the superpolynomial decay behavior of mutual information and conditional mutual information is a universal property of gapped pure- and mixed-state phases, i.e., all systems in such a phase possess this property if one system in this phase possesses this property. We further demonstrate that the (conditional) mutual information indeed decays superpolynomially in a large class of phases, including chiral phases. As a byproduct, we sharpen the notion of mixed-state phases.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv Quantum Physics",
      "sourceUrl": "http://export.arxiv.org/rss/quant-ph",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Forbidden Electron Transfer in the Adiabatic Limit of the Marcus-Inverted Region",
      "link": "https://arxiv.org/abs/2511.01909",
      "description": "arXiv:2511.01909v2 Announce Type: replace-cross \nAbstract: Here it is shown that in the adiabatic limit of condensed-phase electron transfer, the onset of barrierless transition occurs at a lower driving force than predicted by the non-adiabatic Marcus formulation. Furthermore, in the adiabatic limit of the Marcus-inverted region, isoenergetic electron transfer is strictly forbidden in the absence of nuclear tunneling. This \"forbidden\" behavior arises from a topological change in the mapping between the adiabatic and diabatic electronic surfaces, emerging precisely at the onset of the Marcus-inverted region.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv Quantum Physics",
      "sourceUrl": "http://export.arxiv.org/rss/quant-ph",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "Angular momentum of rotating fermionic superfluids by Sagnac phonon interferometry",
      "link": "https://arxiv.org/abs/2511.02664",
      "description": "arXiv:2511.02664v2 Announce Type: replace-cross \nAbstract: Fermionic many-body systems provide an unrivaled arena to investigate how interactions drive the emergence of collective quantum behavior, such as macroscopic coherence and superfluidity. Central to these phenomena is the formation of Cooper pairs, correlated states of two fermions that behave as composite bosons and condense below a critical temperature. However, unlike elementary bosons, these pairs retain their internal structure set by underlying fermionic correlations, essential for understanding superfluid properties throughout the so-called Bose-Einstein condensate (BEC) to Bardeen-Cooper-Schrieffer (BCS) crossover-- a cornerstone of strongly correlated fermionic matter. Here, we harness a sonic analog of the optical Sagnac effect to disclose the composite nature of fermionic condensates across the BEC-BCS crossover. We realize an in-situ loop interferometer by coherently exciting two counter-propagating long-wavelength phonons of an annular fermionic superfluid with tuneable interparticle interactions. The frequency degeneracy between clock- and anticlock-wise sound modes is lifted upon controllably injecting a quantized supercurrent in the superfluid ring, resulting in a measurable Doppler shift that enables us to probe the elementary quantum of circulation and the angular momentum carried by each particle in the fermionic fluid. Our observations directly reveal that the superflow circulation is quantized in terms of $h/2m$, where $m$ is the mass of the constituents, in striking contrast to bosonic condensates where $h/m$ is the relevant circulation quantum. Further, by operating our interferometer at tunable temperature, we measure the thermal depletion of the superfluid in the unitary Fermi gas, demonstrating phonon interferometry as a powerful technique for probing fundamental properties of strongly-correlated quantum systems.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv Quantum Physics",
      "sourceUrl": "http://export.arxiv.org/rss/quant-ph",
      "credibility": 0.95,
      "category": "academic"
    },
    {
      "title": "The simple reason why classical gravity can entangle",
      "link": "https://arxiv.org/abs/2511.02683",
      "description": "arXiv:2511.02683v2 Announce Type: replace-cross \nAbstract: Ever since gravity-induced entanglement (GIE) experiments have been proposed as a witness of the quantum nature of gravity, more and more theories of classical gravity coupled to quantum matter have been shown to predict GIE, despite the existence of several theory-independent no-go theorems purportedly claiming that it should not be possible. This note explains why this is possible, and why this makes the GIE experiments an even more urgent matter in quantum gravity research.",
      "pubDate": "Thu, 06 Nov 2025 00:00:00 -0500",
      "source": "ArXiv Quantum Physics",
      "sourceUrl": "http://export.arxiv.org/rss/quant-ph",
      "credibility": 0.95,
      "category": "academic"
    }
  ],
  "robotics": [
    {
      "title": "iRobot revenue continues to drop with â€˜no sourcesâ€™ of additional capital",
      "link": "https://www.therobotreport.com/irobot-revenue-drops-again-with-no-sources-of-additional-capital/",
      "description": "iRobot said if it can't obtain new capital it \"may be forced to significantly curtail or cease operations and would likely seek bankruptcy protection.\"\nThe post iRobot revenue continues to drop with â€˜no sourcesâ€™ of additional capital appeared first on The Robot Report.",
      "pubDate": "Fri, 07 Nov 2025 00:40:07 +0000",
      "source": "The Robot Report",
      "sourceUrl": "https://www.therobotreport.com/feed/",
      "credibility": 0.85,
      "category": "tech_news"
    },
    {
      "title": "Wandercraft earns second FDA clearance for Atalante X exoskeleton",
      "link": "https://www.therobotreport.com/wandercraft-earns-second-fda-clearance-for-atalante-x-exoskeleton/",
      "description": "This clearance comes just a few short months after Wandercraft received expanded CE Mark certification for enhanced clinical features.\nThe post Wandercraft earns second FDA clearance for Atalante X exoskeleton appeared first on The Robot Report.",
      "pubDate": "Thu, 06 Nov 2025 21:44:21 +0000",
      "source": "The Robot Report",
      "sourceUrl": "https://www.therobotreport.com/feed/",
      "credibility": 0.85,
      "category": "tech_news"
    },
    {
      "title": "SS Innovations completes first telesurgery with new SSi Mantra console",
      "link": "https://www.therobotreport.com/ss-innovations-international-completes-first-telesurgery-ssi-mantra-surgeon-console/",
      "description": "The TSC is a compact, self-contained chair-based version of the larger SSi Mantra surgeon command center from SS Innovations.\nThe post SS Innovations completes first telesurgery with new SSi Mantra console appeared first on The Robot Report.",
      "pubDate": "Thu, 06 Nov 2025 21:31:25 +0000",
      "source": "The Robot Report",
      "sourceUrl": "https://www.therobotreport.com/feed/",
      "credibility": 0.85,
      "category": "tech_news"
    },
    {
      "title": "Sovato closes Series B funding to advance telesurgery",
      "link": "https://www.therobotreport.com/sovato-closes-series-b-funding-to-advance-telesurgery/",
      "description": "Sovato has completed fundraising to scale its telesurgery platform and extend expert robotic care globally. Distant doctors, closer care.\nThe post Sovato closes Series B funding to advance telesurgery appeared first on The Robot Report.",
      "pubDate": "Thu, 06 Nov 2025 17:18:11 +0000",
      "source": "The Robot Report",
      "sourceUrl": "https://www.therobotreport.com/feed/",
      "credibility": 0.85,
      "category": "tech_news"
    },
    {
      "title": "Teradyne Robotics lays off another 14% of workforce",
      "link": "https://www.therobotreport.com/teradyne-robotics-lays-off-another-14-of-workforce/",
      "description": "Teradyne Robotics, which includes Mobile Industrial Robots and Universal Robots, said revenue is not meeting expectations.\nThe post Teradyne Robotics lays off another 14% of workforce appeared first on The Robot Report.",
      "pubDate": "Thu, 06 Nov 2025 13:58:58 +0000",
      "source": "The Robot Report",
      "sourceUrl": "https://www.therobotreport.com/feed/",
      "credibility": 0.85,
      "category": "tech_news"
    },
    {
      "title": "PickNik expands support for Franka Research 3 robot on MoveIt Pro",
      "link": "https://www.therobotreport.com/picknik-expands-support-for-franka-research-3-robot-on-moveit-pro/",
      "description": "PickNik Robotics said this collaboration will help to address one of the central bottlenecks in AI and robotics development. \nThe post PickNik expands support for Franka Research 3 robot on MoveIt Pro appeared first on The Robot Report.",
      "pubDate": "Wed, 05 Nov 2025 20:49:39 +0000",
      "source": "The Robot Report",
      "sourceUrl": "https://www.therobotreport.com/feed/",
      "credibility": 0.85,
      "category": "tech_news"
    },
    {
      "title": "Infravision raises $91M for power line maintenance drones",
      "link": "https://www.therobotreport.com/infravision-raises-91m-for-power-line-maintenance-drones/",
      "description": "Infravision said its flexible, automated approach eliminates contingencies and hazards inherent in conventional power line stringing methods.\nThe post Infravision raises $91M for power line maintenance drones appeared first on The Robot Report.",
      "pubDate": "Wed, 05 Nov 2025 16:30:14 +0000",
      "source": "The Robot Report",
      "sourceUrl": "https://www.therobotreport.com/feed/",
      "credibility": 0.85,
      "category": "tech_news"
    },
    {
      "title": "Inside Hyundaiâ€™s Massive Metaplant",
      "link": "https://spectrum.ieee.org/hyundai-metaplant",
      "description": "When I traveled to Ellabell, Ga., in May to report on Hyundai Motor Groupâ€™s hyperefficient Metaplantâ€”a US $12.6 billion boost to U.S.-based manufacturing of EVs and batteriesâ€”the companyâ€™s timing appeared solid. At this temple of leading-edge factory tech, Ioniq 5 and Ioniq 9 SUVs marched along surgically spotless assembly lines, giving the South Korean automaker a defensible bulwark against the Trump administrationâ€™s tariffs and onshoring fervor.\nBut dark clouds were already gathering. Consumer adoption of EVs had started slowing. The U.S. federal governmentâ€™s $7,500 clean-car tax credit, which had helped hundreds of thousands of people make the leap to EVs, was being phased out.\n Held securely on a yellow jig, a three-row Ioniq 9 SUV glides from station to station in the assembly hall. A view from below shows its generous, 110.3-kilowatt-hour battery pack, which, as in most EVs, sits below the floor of the car. The pack, which is shielded to prevent or limit damage in a collision, is part of an advanced 800-volt architecture for ultrafast DC charging. Christopher Payne/Esto\nNear the Savannah-area factory, I drove a smartly designed Ioniq 9, a three-row SUV tailored to the United Statesâ€™ plus-size tastes. I also saw a battery plant taking shape: a $4.3 billion joint venture between Hyundai and LG Energy Solution, on track to produce lithium-ion cells for Hyundai, Kia, and Genesis models in 2026. That facility is one of 11 low-roofed buildings that encompass 697,000 square meters (70 hectares), their pale green walls designed to blend into the Georgia countryside.\n\n\n\n Backed by $2.1 billion in state subsidies, the Metaplant is the largest public development project in Georgiaâ€™s history. Covering 70 hectares, it is the centerpiece of Hyundaiâ€™s $12.6 billion total investment in the state, including the battery factory built with LG Energy Solution that ICE and other agents raided in September. Christopher Payne/Esto\nThat battery plant made headlines in September, when U.S. Immigration and Customs Enforcement (ICE) agents staged a workplace raid that led to more than 300 South Korean workers being detained and deported.\nThe episode highlighted the transnational cooperationâ€”and tensionsâ€”inherent in importing a leading-edge manufacturing operation, a duality that might be familiar to anyone old enough to recall Japanâ€™s game-changing entry into the U.S. automobile market in the 1970s and â€™80s. The Metaplant is the largest publicly backed project in Georgiaâ€™s history. Its creation was accelerated by the Biden administrationâ€™s pro-EV policies, and it was also the centerpiece of Republican Gov. Brian Kempâ€™s bid to make his state â€œthe electric mobility capital of the country.â€ Now, it was suddenly the latest flashpoint in an ongoing culture-and-trade war.\nAutomakers roll with the punches because they have no choice\n An automated guided vehicle (AGV) prepares to pick up a rack of windshields from an automated trailer unloader, for â€œjust in timeâ€ delivery to an assembly line where Ioniq 5 EVs are being built. There is no human intervention from the time parts arrive at the Metaplantâ€™s loading docks to their installation. Christopher Payne/Esto\n Robots perform myriad tasks, yet human hands are still best for precision work. Jerry Roach, the Metaplantâ€™s assembly manager, says, â€œI want my people doing craftsmanship. I want to pay people well for the things humans do well, and take away the stuff thatâ€™s tedious and boring.â€ Christopher Payne/Esto\nAs with other EV makers facing hurricane-force headwinds, including the U.S. rollback of pollution and fuel-economy rules, Hyundai has chosen to forge ahead with its long-laid plans. Company executives call the Metaplant North Americaâ€™s most automated car factory and the most advanced full-scale factory among Hyundai Motor Co.â€™s 12 global manufacturing facilities. It rivals or surpasses Japanâ€™s most advanced plants, such as the best operated by Toyota. Compared with the near-Dickensian Detroit auto factory that I toiled at in the 1980s, the stunning facility is a veritable MOMA: a modern museum of manufacturing art.\nTo have any chance of one-upping China, car factories elsewhere must become hyperefficient, which includes enlisting armies of AI-controlled robotsâ€”robots that can potentially work 24/7 and never ask for a raise or a lunch break.\nThe factory may eventually employ 8,500 people directly, and 7,000 satellite workers, for an annual capacity of 500,000 carsâ€”more than Teslaâ€™s Texas Gigafactory but less than Teslaâ€™s Shanghai plant. This past summer, just 1,340 humans were sufficient to send a constant stream of two Ioniq models down these gleaming assembly lines. The â€œMeta Prosâ€ working on those lines were earning on average $58,100 a year, which is 35 percent higher than the average in Bryan County, Ga.\nClearly the days of Fordâ€™s River Rouge complex, which employed more than 100,000 in the 1930s, are gone. As in many new factories, youâ€™ll see surprisingly few people beyond the assembly line itself. During my visit, I spotted less than two dozen in a cavernous welding hall, where 475 robots were piecing together car chassis in a whirling, metallic dance. A steel stamping plant was so quiet that no ear protection was required, even as robots stamped out roofs and other body panels, and then stowed them in overhead racks.\nOutside, human workers parked their cars beneath solar roofs that generate up to 5 percent of the plantâ€™s electricity. Meanwhile, a fleet of 21 hydrogen fuel-cell trucks, from the Hyundai-owned Xcient, carries parts from suppliers, emitting zero tailpipe emissions. The automakerâ€™s goal is to obtain 100 percent of the Metaplantâ€™s energy from renewables by 2030.\n An Ioniq 9 body-in-white, the basic steel skeleton of an automobile, leaves the â€œmain buckâ€ section of the body build line. This line is where the vehicleâ€™s floor and sides meet to form a recognizable car. The line adapts to changing production mixes to meet customer orders, with built-in flexibility to assemble future models.Christopher Payne/Esto\n Sparks fly as welding robots piece together the Ioniq 9â€™s â€œbody-in-white,â€ the industry term for the basic steel skeleton of a car, prior to the addition of subassemblies such as the suspension, power train, body trim, and interior. The Metaplantâ€™s welding shop houses about 500 industrial robots.Christopher Payne/Esto\n Robotic welders have revolutionized car manufacturing, joining the parts of an auto body with levels of speed, precision, and safety that humans canâ€™t match. Such advantages reduce labor costs and scrapped materials. Hyundai is also now experimenting with humanoid robots to perform welding tasks.Christopher Payne/Esto\n â€œBody-completeâ€ robots mount front doors onto Ioniq 5s, using machine vision and laser-measurement systems to ensure an exact fit of movable panels on each body. The robots also install mounting bolts to exact torque specifications, all validated to ensure their work meets safety and quality standards.Christopher Payne/Esto\nSmart, silent robots unload trucks\nWhen those trucks roll into docks at the Metaplant, some of the factoryâ€™s 850 robots promptly unload their parts. About 300 automated guided vehicles, or AGVs, glide silently across the factory floor with no tracks required, trained to smartly stop for humans. An AGV rolls beneath a finished Hyundai, squeezes the wheels in its robotic arms, then swiftly hoists and ferries the car where it needs to go. A companion AGV further down the line executes the exact same moves. Iâ€™ve never seen so many robotic sleds like these, or a tag team move with more efficiency and grace. Within an AI-based procurement-and-logistics system, the AGVs allocate and deliver parts to workstations for â€œjust in timeâ€ delivery, avoiding wasted time, space, and money as they stockpile components.\n An automated guided vehicle ferries dashboards for the Hyundai Ioniq 9 SUV, including each dashboardâ€™s pair of 30-centimeter display screens. AGVs are programmed to navigate the factory, using cameras and sensors to slow or stop to avoid collisions, and emit spoken warnings to human workers in their path.Christopher Payne/Esto\nâ€œTheyâ€™re delivering the right parts to the right station at the right time, so youâ€™re no longer relying on people to make those decisions,â€ says Jerry Roach, senior manager of general assembly at the Metaplant.\nRoach prefers that his skilled humans focus on craftsmanship, doing jobs with tactile precision that only human hands and vision can accomplish. The idea is to free people from those elements of factory work that are physically taxing, unfulfilling, and, well, robotic, so workers can use their brains and take pride in their specialized skills.\n Left: Adjustable-height carriers elevate an Ioniq 5 for easy access to the central fasteners and plugs that will position suspension components and the high-voltage battery, prior to the â€œmarriageâ€ between the upper and lower sections of the vehicle. Those carriers provide flexibility for automated functions and manual operations by the human workers at the plant (whom Hyundai calls Meta Pros). Right: On the final assembly line, an Ioniq 9â€™s â€œtop hatâ€â€”including body panelsâ€”is married to the lower â€œskateboardâ€ structure, which includes the electric motors, battery, and suspension. A finished car then undergoes various tests, including a water bath to check for leaks and a quick road test outdoors. Christopher Payne/Esto\nRobots, Roach says, are best tasked with heavy lifting and repetitive tasks, or those that demand digitized speed and accuracy. One example is a â€œcollaborativeâ€ robot, sophisticated enough to work safely in close proximity to people, despite its mammoth strength. For the first time at a Hyundai factory, such a robot is installing bulky, heavy doors on the assembly lineâ€”a notoriously tricky task to perform without scratching the glossy paint or damaging surrounding panels.\n Hyundai is proud of its collaborative robots, including one that can precisely install a heavy door, a tricky task for humans to perform without damaging the panels. Those robots require advanced control systems so that they can work alongside human workers without needing to be fenced off or otherwise isolated.Christopher Payne/Esto\nâ€œGuess what? Robots do that perfectly, always putting the door in the exact same place,â€ Roach says. â€œSo here, that technology makes sense.â€\nManâ€™s best friend, or its mechanical counterparts, stroll the factory floor: Spot, the robotic quadrupeds from Hyundai-owned Boston Dynamics, use camera vision, sensors, and what Boston Dynamics calls â€œathletic intelligenceâ€ to sniff out potential welding defects.\n Spot, the robot dog designed by Hyundai-owned Boston Dynamics, inspects body welds on an Ioniq 5 for defects. Equipped with a sensor suite, the quadruped bot can recharge autonomously, dynamically work around fixed or moving obstacles, and get back on its feet if it falls. Christopher Payne/Esto\nThose four-legged bots may soon have a biped master: Atlas, the humanoid robot, also from Boston Dynamics. The humanoidâ€™s physical dexterity is uncanny, with a 360-degree swiveling head that allows it to walk forward and backward without turning its body. One look at these Atlases crawling, cartwheeling, or breakdancing during testing and you might reasonably conclude theyâ€™re a potential Terminator of jobs. Hyundai executives insist thatâ€™s not the case, even as they plan to put Atlases to work in their global factories. Boston Dynamics is training these robots to sense their environments and manipulate and move parts in complex sequences.\n At this backup station, high-voltage battery fasteners can be installed in an Ioniq 5. The station ensures that the assembly line keeps running even if an automated production system requires servicing. Christopher Payne/Esto\nFrom nearby Interstate 16, Georgia drivers can see freshly painted Ioniq 5s and 9s moving along a conveyor on a windowed bridgeâ€”an intentional glimpse of whatâ€™s happening inside. They can also see their tax dollars at work, after $2.1 billion in state subsidies. Hyundai is already building a second battery plant in Georgia, and a steel plant in Louisiana, part of an expanded pledge of $21 billion in U.S. investment through 2028.\n After their frames are fully welded, Ioniq 5s move along a conveyor [in the background] to an environmentally friendly paint shop. From there, the cars will travel along an elevated bridge, visible from nearby Interstate 16 in Ellabell, Ga., toward final assembly.Christopher Payne/Esto\n An Ioniq 5 arrives at its final inspection station. Immediately after, a human driver gets to drive the pristine car for the first time, on a test track just outside the factory. The first Ioniq 5 rolled off the Metaplant line on 3 October 2024, with the larger Ioniq 9 kicking off production in March 2025. Christopher Payne/Esto\nIn a suddenly inhospitable climate for EVs, thereâ€™s nothing automatic about building and selling the cars. But Hyundai and other automakers will keep trying. They donâ€™t have any other choice.",
      "pubDate": "Wed, 05 Nov 2025 14:00:03 +0000",
      "source": "IEEE Spectrum Robotics",
      "sourceUrl": "https://spectrum.ieee.org/feeds/topic/robotics.rss",
      "credibility": 0.9,
      "category": "tech_news"
    }
  ]
}